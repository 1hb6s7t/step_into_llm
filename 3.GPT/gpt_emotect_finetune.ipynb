{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install mindnlp to finetune bert.\n",
    "!pip install git+https://github.com/mindspore-lab/mindnlp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mindspore'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmindspore\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmindspore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m text, GeneratorDataset, transforms\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmindspore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mindspore'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import mindspore\n",
    "from mindspore.dataset import text, GeneratorDataset, transforms\n",
    "from mindspore import nn\n",
    "\n",
    "from mindnlp.transforms import PadTransform\n",
    "from mindnlp.models import BertModel, BertConfig\n",
    "from mindnlp.transforms.tokenizers import GPTTokenizer, BertTokenizer\n",
    "\n",
    "from mindnlp.engine import Trainer, Evaluator\n",
    "from mindnlp.engine.callbacks import CheckpointCallback, BestModelCallback\n",
    "from mindnlp.metrics import Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import six\n",
    "import string\n",
    "import tarfile\n",
    "\n",
    "class SentimentDataset():\n",
    "    \"\"\"IMDB数据集加载器\n",
    "\n",
    "    加载IMDB数据集并处理为一个Python迭代对象。\n",
    "\n",
    "    \"\"\"\n",
    "    label_map = {\n",
    "        \"pos\": 1,\n",
    "        \"neg\": 0\n",
    "    }\n",
    "    def __init__(self, path, mode=\"train\"):\n",
    "        self.mode = mode\n",
    "        self.path = path\n",
    "        self.docs, self.labels = [], []\n",
    "\n",
    "        self._load(\"pos\")\n",
    "        self._load(\"neg\")\n",
    "\n",
    "    def _load(self, label):\n",
    "        pattern = re.compile(r\"aclImdb/{}/{}/.*\\.txt$\".format(self.mode, label))\n",
    "        # 将数据加载至内存\n",
    "        with tarfile.open(self.path) as tarf:\n",
    "            tf = tarf.next()\n",
    "            while tf is not None:\n",
    "                if bool(pattern.match(tf.name)):\n",
    "                    # 对文本进行分词、去除标点和特殊字符、小写处理\n",
    "                    self.docs.append(str(tarf.extractfile(tf).read().rstrip(six.b(\"\\n\\r\"))\n",
    "                                         .translate(None, six.b(string.punctuation)).lower()).split())\n",
    "                    self.labels.append([self.label_map[label]])\n",
    "                tf = tarf.next()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.labels[idx][0], self.docs[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import requests\n",
    "import tempfile\n",
    "from tqdm import tqdm\n",
    "from typing import IO\n",
    "from pathlib import Path\n",
    "\n",
    "# 指定保存路径为 `./mindspore_examples`\n",
    "cache_dir = './mindspore_examples'\n",
    "\n",
    "def http_get(url: str, temp_file: IO):\n",
    "    \"\"\"使用requests库下载数据，并使用tqdm库进行流程可视化\"\"\"\n",
    "    req = requests.get(url, stream=True)\n",
    "    content_length = req.headers.get('Content-Length')\n",
    "    total = int(content_length) if content_length is not None else None\n",
    "    progress = tqdm(unit='B', total=total)\n",
    "    for chunk in req.iter_content(chunk_size=1024):\n",
    "        if chunk:\n",
    "            progress.update(len(chunk))\n",
    "            temp_file.write(chunk)\n",
    "    progress.close()\n",
    "\n",
    "def download(file_name: str, url: str):\n",
    "    \"\"\"下载数据并存为指定名称\"\"\"\n",
    "    if not os.path.exists(cache_dir):\n",
    "        os.makedirs(cache_dir)\n",
    "    cache_path = os.path.join(cache_dir, file_name)\n",
    "    cache_exist = os.path.exists(cache_path)\n",
    "    if not cache_exist:\n",
    "        with tempfile.NamedTemporaryFile() as temp_file:\n",
    "            http_get(url, temp_file)\n",
    "            temp_file.flush()\n",
    "            temp_file.seek(0)\n",
    "            with open(cache_path, 'wb') as cache_file:\n",
    "                shutil.copyfileobj(temp_file, cache_file)\n",
    "    return cache_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./mindspore_examples/aclImdb_v1.tar.gz'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_path = download('aclImdb_v1.tar.gz', 'https://mindspore-website.obs.myhuaweicloud.com/notebook/datasets/aclImdb_v1.tar.gz')\n",
    "imdb_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label = 1\n",
      "text = [\"b'previous\", 'reviewer', 'claudio', 'carvalho', 'gave', 'a', 'much', 'better', 'recap', 'of', 'the', 'films', 'plot', 'details', 'than', 'i', 'could', 'what', 'i', 'recall', 'mostly', 'is', 'that', 'it', 'was', 'just', 'so', 'beautiful', 'in', 'every', 'sense', 'emotionally', 'visually', 'editorially', 'just', 'gorgeousbr', 'br', 'if', 'you', 'like', 'movies', 'that', 'are', 'wonderful', 'to', 'look', 'at', 'and', 'also', 'have', 'emotional', 'content', 'to', 'which', 'that', 'beauty', 'is', 'relevant', 'i', 'think', 'you', 'will', 'be', 'glad', 'to', 'have', 'seen', 'this', 'extraordinary', 'and', 'unusual', 'work', 'of', 'artbr', 'br', 'on', 'a', 'scale', 'of', '1', 'to', '10', 'id', 'give', 'it', 'about', 'an', '875', 'the', 'only', 'reason', 'i', 'shy', 'away', 'from', '9', 'is', 'that', 'it', 'is', 'a', 'mood', 'piece', 'if', 'you', 'are', 'in', 'the', 'mood', 'for', 'a', 'really', 'artistic', 'very', 'romantic', 'film', 'then', 'its', 'a', '10', 'i', 'definitely', 'think', 'its', 'a', 'mustsee', 'but', 'none', 'of', 'us', 'can', 'be', 'in', 'that', 'mood', 'all', 'the', 'time', 'so', 'overall', \"875'\"]\n"
     ]
    }
   ],
   "source": [
    "test_data = SentimentDataset(imdb_path, \"test\")\n",
    "for label, text in test_data:\n",
    "    print(f\"label = {label}\")\n",
    "    print(f\"text = {text}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\"label\", \"text_a\"]\n",
    "dataset_train = GeneratorDataset(source = SentimentDataset(imdb_path, \"train\"), column_names=column_names, shuffle=False)\n",
    "dataset_train, dataset_valid = dataset_train.split([0.7, 0.3])\n",
    "dataset_test = GeneratorDataset(source = SentimentDataset(imdb_path, \"test\"), column_names=column_names, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(dataset, tokenizer, pad_value, max_seq_len=500, shuffle=True):\n",
    "    rename_columns = [\"label\", \"input_ids\"]\n",
    "    \n",
    "    # shuffle\n",
    "    # transforms\n",
    "    pad_op = PadTransform(max_seq_len, pad_value=pad_value)\n",
    "    type_cast_op = transforms.TypeCast(mindspore.int32)\n",
    "    \n",
    "    # map dataset\n",
    "    dataset = dataset.map(operations=[tokenizer, pad_op], input_columns=\"text_a\")\n",
    "    dataset = dataset.map(operations=[type_cast_op], input_columns=\"label\")\n",
    "    # rename dataset\n",
    "    dataset = dataset.rename(input_columns=column_names, output_columns=rename_columns)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(source, tokenizer, pad_value, max_seq_len=64, batch_size=32, shuffle=True):\n",
    "    column_names = [\"label\", \"text_a\"]\n",
    "    rename_columns = [\"label\", \"input_ids\"]\n",
    "    \n",
    "    dataset = GeneratorDataset(source, column_names=column_names, shuffle=shuffle)\n",
    "    # transforms\n",
    "    pad_op = PadTransform(max_seq_len, pad_value=pad_value)\n",
    "    type_cast_op = transforms.TypeCast(mindspore.int32)\n",
    "    \n",
    "    # map dataset\n",
    "    dataset = dataset.map(operations=[tokenizer, pad_op], input_columns=\"text_a\")\n",
    "    dataset = dataset.map(operations=[type_cast_op], input_columns=\"label\")\n",
    "    # rename dataset\n",
    "    dataset = dataset.rename(input_columns=column_names, output_columns=rename_columns)\n",
    "    # batch dataset\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "gpt_tokenizer = GPTTokenizer.from_pretrained('openai-gpt')\n",
    "\n",
    "# add sepcial token: <PAD>\n",
    "special_tokens_dict = {\"pad_token\": \"<pad>\"}\n",
    "num_added_toks = gpt_tokenizer.add_special_tokens(special_tokens_dict)\n",
    "pad_value = gpt_tokenizer.token_to_id(\"<pad>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(153706:281473830918848,MainProcess):2023-05-06-06:33:31.988.617 [mindspore/dataset/engine/datasets.py:1123] Dataset is shuffled before split.\n"
     ]
    }
   ],
   "source": [
    "dataset_train = process_dataset(SentimentDataset(imdb_path, \"train\"), gpt_tokenizer, pad_value)\n",
    "# split train dataset into train and valid datasets\n",
    "dataset_train, dataset_valid = dataset_train.split([0.7, 0.3])\n",
    "dataset_test = process_dataset(SentimentDataset(imdb_path, \"test\"), gpt_tokenizer, pad_value, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch dataset\n",
    "batch_size = 32\n",
    "dataset_train = dataset_train.batch(batch_size)\n",
    "dataset_valid = dataset_valid.batch(batch_size)\n",
    "dataset_test = dataset_test.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label = [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "input ids = [[  293   244   259 ...   256   249   256]\n",
      " [  293   244   259 ...   256  1532  3414]\n",
      " [  293   244   259 ...   256   987   256]\n",
      " ...\n",
      " [  293   244   259 ...   256 23880   256]\n",
      " [  293   244   259 ...   256 16574   256]\n",
      " [  293   244   259 ...   256  9469   256]]\n"
     ]
    }
   ],
   "source": [
    "# show dataset\n",
    "for label, input_ids in dataset_test.create_tuple_iterator():\n",
    "    print(f\"label = {label}\")\n",
    "    print(f\"input ids = {input_ids}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(153706:281473830918848,MainProcess):2023-05-06-07:10:34.435.752 [mindspore/train/serialization.py:712] For 'load_param_into_net', 1 parameters in the 'net' are not loaded, because they are not in the 'parameter_dict', please check whether the network structure is consistent when training and loading checkpoint.\n",
      "[WARNING] ME(153706:281473830918848,MainProcess):2023-05-06-07:10:34.437.908 [mindspore/train/serialization.py:714] score.weight is not loaded.\n"
     ]
    }
   ],
   "source": [
    "from mindnlp.models import GPTForSequenceClassification\n",
    "from mindnlp._legacy.amp import auto_mixed_precision\n",
    "\n",
    "# set bert config and define parameters for training\n",
    "model = GPTForSequenceClassification.from_pretrained('openai-gpt', num_labels=2)\n",
    "model.pad_token_id = pad_value\n",
    "model = auto_mixed_precision(model, 'O1')\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = nn.Adam(model.trainable_params(), learning_rate=2e-5)\n",
    "\n",
    "metric = Accuracy()\n",
    "\n",
    "# define callbacks to save checkpoints\n",
    "ckpoint_cb = CheckpointCallback(save_path='checkpoint', ckpt_name='sentiment_model', epochs=1, keep_checkpoint_max=2)\n",
    "best_model_cb = BestModelCallback(save_path='checkpoint', auto_load=True)\n",
    "\n",
    "trainer = Trainer(network=model, train_dataset=dataset_train,\n",
    "                  eval_dataset=dataset_valid, metrics=metric,\n",
    "                  epochs=10, loss_fn=loss, optimizer=optimizer, callbacks=[ckpoint_cb, best_model_cb],\n",
    "                  jit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train will start from the checkpoint saved in 'checkpoint'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/547 [01:48<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Multiply values for specific argument: axis\n\n----------------------------------------------------\n- The Traceback of Net Construct Code:\n----------------------------------------------------\nThe function call stack (See file '/home/ma-user/work/rank_0/om/analyze_fail.dat' for more details. Get instructions about `analyze_fail.dat` at https://www.mindspore.cn/search?inputValue=analyze_fail.dat):\n# 0 In file /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages/mindnlp/engine/trainer.py(143)\n            if check_gradients:\n# 1 In file /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages/mindnlp/engine/trainer.py(141)\n            (loss, _), grads = grad_fn(inputs, labels)\n                               ^\n# 2 In file /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages/mindspore/ops/functional.py(454)\n        if grad_position is None:\n# 3 In file /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages/mindspore/ops/functional.py(453)\n        res = aux_fn(*args)\n              ^\n# 4 In file /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages/mindspore/ops/functional.py(435)\n        outputs = fn(*args)\n                  ^\n# 5 In file /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages/mindnlp/engine/trainer.py(112)\n            logits = network(*inputs)\n                     ^\n# 6 In file /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages/mindnlp/models/gpt/gpt.py(551)\n        if self.pad_token_id is None and batch_size != 1:\n# 7 In file /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages/mindnlp/engine/trainer.py(112)\n            logits = network(*inputs)\n                     ^\n# 8 In file /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages/mindnlp/models/gpt/gpt.py(576)\n        if loss is not None:\n# 9 In file /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages/mindnlp/models/gpt/gpt.py(566)\n        output = (pooled_logits,) + transformer_outputs[1:]\n                                    ^\n# 10 In file /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages/mindnlp/models/gpt/gpt.py(302)\n        if input_ids is not None and inputs_embeds is not None:\n# 11 In file /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages/mindnlp/models/gpt/gpt.py(304)\n        if input_ids is not None:\n# 12 In file /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages/mindnlp/models/gpt/gpt.py(533)\n        transformer_outputs = self.transformer(\n                              ^\n# 13 In file /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages/mindnlp/models/gpt/gpt.py(340)\n        for i, block in enumerate(self.h):\n# 14 In file /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages/mindnlp/models/gpt/gpt.py(533)\n        transformer_outputs = self.transformer(\n                              ^\n# 15 In file /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages/mindnlp/models/gpt/gpt.py(344)\n            outputs = block(hidden_states, attention_mask, head_mask[i])\n                      ^\n# 16 In file /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages/mindnlp/models/gpt/gpt.py(214)\n        output_attn = self.attn(\n# 17 In file /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages/mindnlp/models/gpt/gpt.py(185)\n        query, key, value = ops.split(x, self.split_size, axis=2)\n                            ^\n\n----------------------------------------------------\n- C++ Call Stack: (For framework developers)\n----------------------------------------------------\nmindspore/core/ir/func_graph_extends.cc:153 GenerateKwParams\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_153706/2278154684.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/MindSpore/lib/python3.7/site-packages/mindnlp/engine/trainer.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, tgt_columns)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mrun_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRunContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/MindSpore/lib/python3.7/site-packages/mindnlp/engine/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, run_context, tgt_columns)\u001b[0m\n\u001b[1;32m    244\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur_step_nums\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m                     \u001b[0mloss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m                     \u001b[0mrun_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_total\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur_step_nums\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/MindSpore/lib/python3.7/site-packages/mindspore/common/api.py\u001b[0m in \u001b[0;36mstaging_specialize\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    557\u001b[0m                 \u001b[0mprocess_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_MindsporeFunctionExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhash_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_signature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocess_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjit_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/MindSpore/lib/python3.7/site-packages/mindspore/common/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*arg, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_convert_python_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/MindSpore/lib/python3.7/site-packages/mindspore/common/api.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0mphase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_args_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m             \u001b[0mphase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"precompile_only\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/MindSpore/lib/python3.7/site-packages/mindspore/common/api.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, args_list, method_name)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m             \u001b[0mis_compile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Multiply values for specific argument: axis\n\n----------------------------------------------------\n- The Traceback of Net Construct Code:\n----------------------------------------------------\nThe function call stack (See file '/home/ma-user/work/rank_0/om/analyze_fail.dat' for more details. Get instructions about `analyze_fail.dat` at https://www.mindspore.cn/search?inputValue=analyze_fail.dat):\n# 0 In file /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages/mindnlp/engine/trainer.py(143)\n            if check_gradients:\n# 1 In file /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages/mindnlp/engine/trainer.py(141)\n            (loss, _), grads = grad_fn(inputs, labels)\n                               ^\n# 2 In file /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages/mindspore/ops/functional.py(454)\n        if grad_position is None:\n# 3 In file /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages/mindspore/ops/functional.py(453)\n        res = aux_fn(*args)\n              ^\n# 4 In file /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages/mindspore/ops/functional.py(435)\n        outputs = fn(*args)\n                  ^\n# 5 In file /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages/mindnlp/engine/trainer.py(112)\n            logits = network(*inputs)\n                     ^\n# 6 In file /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages/mindnlp/models/gpt/gpt.py(551)\n        if self.pad_token_id is None and batch_size != 1:\n# 7 In file /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages/mindnlp/engine/trainer.py(112)\n            logits = network(*inputs)\n                     ^\n# 8 In file /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages/mindnlp/models/gpt/gpt.py(576)\n        if loss is not None:\n# 9 In file /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages/mindnlp/models/gpt/gpt.py(566)\n        output = (pooled_logits,) + transformer_outputs[1:]\n                                    ^\n# 10 In file /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages/mindnlp/models/gpt/gpt.py(302)\n        if input_ids is not None and inputs_embeds is not None:\n# 11 In file /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages/mindnlp/models/gpt/gpt.py(304)\n        if input_ids is not None:\n# 12 In file /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages/mindnlp/models/gpt/gpt.py(533)\n        transformer_outputs = self.transformer(\n                              ^\n# 13 In file /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages/mindnlp/models/gpt/gpt.py(340)\n        for i, block in enumerate(self.h):\n# 14 In file /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages/mindnlp/models/gpt/gpt.py(533)\n        transformer_outputs = self.transformer(\n                              ^\n# 15 In file /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages/mindnlp/models/gpt/gpt.py(344)\n            outputs = block(hidden_states, attention_mask, head_mask[i])\n                      ^\n# 16 In file /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages/mindnlp/models/gpt/gpt.py(214)\n        output_attn = self.attn(\n# 17 In file /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages/mindnlp/models/gpt/gpt.py(185)\n        query, key, value = ops.split(x, self.split_size, axis=2)\n                            ^\n\n----------------------------------------------------\n- C++ Call Stack: (For framework developers)\n----------------------------------------------------\nmindspore/core/ir/func_graph_extends.cc:153 GenerateKwParams\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "trainer.run(tgt_columns=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(network=model, eval_dataset=dataset_test, metrics=metric)\n",
    "evaluator.run(tgt_columns=\"label\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
