# 大模型技术公开课

《大模型技术公开课》为昇思MindSpore与启智社区联合推出的大模型代码实践课程。在这个课程中，你将从实践的角度出发，通过复现ChatGPT的实现过程，手把手地搭建一个简易版的ChatGPT，从而深入了解大型语言模型的构建和原理。本课程将理论与代码进行融合，系统地逐步揭秘以ChatGPT、GPT-4为代表的大型语言类模型（Large-Language Model， LLM），旨在让学生深入了解大模型领域知识的同时，真正切实地参与到大模型相关的任务实践中来。

课程内容之外，我们同时开展大模型访谈活动，邀请业界专家讨论大型语言模型领域的技术趋势与行业应用。此外，我们还提供了多样的社区活动与社区实习，让你能够巩固课程所学知识，并深入拓展自己的能力。感兴趣的同学还可以参加昇腾AI创新大赛的大模型比赛，深入了解行业场景，更有丰厚的奖金在等着你！

如果你想要学习大模型背后的原理，了解前沿技术；渴望自己动手构建自己的语言模型，那就请不要错过我们大模型技术公开课！4.15起，每周六下午14:00-15:30，我们在b站等候你的到来₍ᐢ..ᐢ₎♡

## 课程安排

| 课程事项 |  |
|----------|----------|
| 开课日期 | (预计) 2023/4/15 - 2023/6/17|
| 直播时间 | 每周日下午2：00 - 3：30 |
| 直播平台 | B站 |
| 课程回放 | [B站 MindSpore官方账号](https://space.bilibili.com/526894060) |
| 算力平台 | [启智OpenI平台](https://openi.pcl.ac.cn/) |


## 课程内容

| 日期 | 课程 | 课件归档 |
|----------|----------|----------|
| 4/15 | Transformer<br> * Multi-Head Attention<br> * Transformer结构<br>&nbsp;&nbsp;&nbsp;&nbsp; * 输入编码 <br> &nbsp;&nbsp;&nbsp;&nbsp; * Encoder <br> &nbsp;&nbsp;&nbsp;&nbsp; * Decoder <br> |  |
| 4/22 | BERT<br> * BERT预训练<br> * BERT Finetune<br> * 使用MindSpore写一个数据并行的BERT<br> |  |
| 4/29 | GPT<br> * Unsupervised Language Modelling<br> * Supervised Fine-Tuning<br> * 使用GPT Finetune一个Task<br> |  |
| 5/6 | 劳动节休息，劳逸结合才能更好的学习哦~ |  |
| 5/13 | GPT2<br> * Task Conditioning<br> * Zero Shot Learning and Zero Shot Task Transfer<br> * 使用GPT2训练一个few shot任务<br> |  |
| 5/20 | MindSpore自动并行 |  |
| 5/27 | 代码预训练<br> * CodeBERT<br> * CodeX、Copilot<br> * CodeGeeX<br> * 使用CodeGeeX生成代码<br> |  |
| 6/3 | Prompt Tuning<br> * 人工定义Prompt<br> * P-tuning<br> * 使用BERT/GPT2实现Prompt Tuning<br> |  |
| 6/10 | Instruct Tuning<br> * Let's think step by step<br> * InstructGPT<br> * Chain-of-thoughts<br> |  |
| 6/17 | RLHF<br> * 强化学习与PPO算法<br> * InstructGPT/ChatGPT中的RLHF<br> * 动手训练一个Reward模型<br> * 使用GPT2实现ChatGPT全流程（基于人工反馈的评论生成模型）<br>|  |

