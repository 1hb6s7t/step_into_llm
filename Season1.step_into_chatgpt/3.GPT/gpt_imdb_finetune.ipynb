{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "安装MindSpore框架和MindNLP套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting mindspore==2.4.1\n",
      "  Downloading https://ms-release.obs.cn-north-4.myhuaweicloud.com/2.4.1/MindSpore/unified/aarch64/mindspore-2.4.1-cp39-cp39-linux_aarch64.whl (335.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m335.5/335.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.20.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindspore==2.4.1) (1.26.1)\n",
      "Requirement already satisfied: protobuf>=3.13.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindspore==2.4.1) (3.20.3)\n",
      "Requirement already satisfied: asttokens>=2.0.4 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindspore==2.4.1) (2.4.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindspore==2.4.1) (9.0.1)\n",
      "Requirement already satisfied: scipy>=1.5.4 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindspore==2.4.1) (1.11.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindspore==2.4.1) (23.2)\n",
      "Requirement already satisfied: psutil>=5.6.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindspore==2.4.1) (5.9.5)\n",
      "Requirement already satisfied: astunparse>=1.6.3 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindspore==2.4.1) (1.6.3)\n",
      "Collecting safetensors>=0.4.0 (from mindspore==2.4.1)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/08/94/7760694760f1e5001bd62c93155b8b7ccb652d1f4d0161d1e72b5bf9581a/safetensors-0.4.5-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (442 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.4/442.4 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from asttokens>=2.0.4->mindspore==2.4.1) (1.16.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from astunparse>=1.6.3->mindspore==2.4.1) (0.41.2)\n",
      "\u001b[33mDEPRECATION: moxing-framework 2.1.16.2ae09d45 has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of moxing-framework or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: safetensors, mindspore\n",
      "  Attempting uninstall: mindspore\n",
      "    Found existing installation: mindspore 2.3.0\n",
      "    Uninstalling mindspore-2.3.0:\n",
      "      Successfully uninstalled mindspore-2.3.0\n",
      "Successfully installed mindspore-2.4.1 safetensors-0.4.5\n"
     ]
    }
   ],
   "source": [
    "!pip install https://ms-release.obs.cn-north-4.myhuaweicloud.com/2.4.1/MindSpore/unified/aarch64/mindspore-2.4.1-cp39-cp39-linux_aarch64.whl --trusted-host ms-release.obs.cn-north-4.myhuaweicloud.com -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mindnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://pip.modelarts.private.com:8888/repository/pypi/simple\n",
      "Requirement already satisfied: jieba in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (0.42.1)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the '/home/ma-user/anaconda3/envs/MindSpore/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HF_ENDPOINT=https://hf-mirror.com\n"
     ]
    }
   ],
   "source": [
    "%env HF_ENDPOINT=https://hf-mirror.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] GE_ADPT(44827,ffff813130b0,python):2025-01-05-01:02:28.751.558 [mindspore/ccsrc/utils/dlopen_macro.h:163] DlsymAscend] Dynamically load symbol aclmdlBundleGetModelId failed, result = /usr/local/Ascend/ascend-toolkit/latest/lib64/libascendcl.so: undefined symbol: aclmdlBundleGetModelId\n",
      "[WARNING] GE_ADPT(44827,ffff813130b0,python):2025-01-05-01:02:28.751.609 [mindspore/ccsrc/utils/dlopen_macro.h:163] DlsymAscend] Dynamically load symbol aclmdlBundleLoadFromMem failed, result = /usr/local/Ascend/ascend-toolkit/latest/lib64/libascendcl.so: undefined symbol: aclmdlBundleLoadFromMem\n",
      "[WARNING] GE_ADPT(44827,ffff813130b0,python):2025-01-05-01:02:28.751.629 [mindspore/ccsrc/utils/dlopen_macro.h:163] DlsymAscend] Dynamically load symbol aclmdlBundleUnload failed, result = /usr/local/Ascend/ascend-toolkit/latest/lib64/libascendcl.so: undefined symbol: aclmdlBundleUnload\n",
      "[WARNING] GE_ADPT(44827,ffff813130b0,python):2025-01-05-01:02:28.751.823 [mindspore/ccsrc/utils/dlopen_macro.h:163] DlsymAscend] Dynamically load symbol aclrtGetMemUceInfo failed, result = /usr/local/Ascend/ascend-toolkit/latest/lib64/libascendcl.so: undefined symbol: aclrtGetMemUceInfo\n",
      "[WARNING] GE_ADPT(44827,ffff813130b0,python):2025-01-05-01:02:28.751.839 [mindspore/ccsrc/utils/dlopen_macro.h:163] DlsymAscend] Dynamically load symbol aclrtDeviceTaskAbort failed, result = /usr/local/Ascend/ascend-toolkit/latest/lib64/libascendcl.so: undefined symbol: aclrtDeviceTaskAbort\n",
      "[WARNING] GE_ADPT(44827,ffff813130b0,python):2025-01-05-01:02:28.751.855 [mindspore/ccsrc/utils/dlopen_macro.h:163] DlsymAscend] Dynamically load symbol aclrtMemUceRepair failed, result = /usr/local/Ascend/ascend-toolkit/latest/lib64/libascendcl.so: undefined symbol: aclrtMemUceRepair\n",
      "[WARNING] GE_ADPT(44827,ffff813130b0,python):2025-01-05-01:02:28.753.785 [mindspore/ccsrc/utils/dlopen_macro.h:163] DlsymAscend] Dynamically load symbol acltdtCleanChannel failed, result = /usr/local/Ascend/ascend-toolkit/latest/lib64/libacl_tdt_channel.so: undefined symbol: acltdtCleanChannel\n",
      "[WARNING] ME(44827:281472849227952,MainProcess):2025-01-05-01:02:28.877.761 [mindspore/run_check/_check_version.py:398] Can not find the tbe operator implementation(need by mindspore-ascend). Please check whether the Environment Variable PYTHONPATH is set. For details, refer to the installation guidelines: https://www.mindspore.cn/install\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "[WARNING] ME(44827:281472849227952,MainProcess):2025-01-05-01:02:39.387.242 [mindspore/run_check/_check_version.py:398] Can not find the tbe operator implementation(need by mindspore-ascend). Please check whether the Environment Variable PYTHONPATH is set. For details, refer to the installation guidelines: https://www.mindspore.cn/install\n",
      "[WARNING] ME(44827:281472849227952,MainProcess):2025-01-05-01:02:39.390.836 [mindspore/run_check/_check_version.py:398] Can not find the tbe operator implementation(need by mindspore-ascend). Please check whether the Environment Variable PYTHONPATH is set. For details, refer to the installation guidelines: https://www.mindspore.cn/install\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "cannot found `mindformers.experimental`, please install dev version by\n",
      "`pip install git+https://gitee.com/mindspore/mindformers` \n",
      "or remove mindformers by \n",
      "`pip uninstall mindformers`\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 1.298 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import mindspore\n",
    "from mindspore.dataset import text, GeneratorDataset, transforms\n",
    "from mindspore import nn\n",
    "mindspore.set_context(device_target='Ascend', device_id=0)\n",
    "\n",
    "from mindnlp.dataset import load_dataset\n",
    "\n",
    "from mindnlp.engine.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imdb_ds = load_dataset('imdb', split=['train', 'test'])\n",
    "imdb_train = imdb_ds['train']\n",
    "imdb_test = imdb_ds['test']\n",
    "# 为加快运行速度只选取一部分训练\n",
    "imdb_train, _ = imdb_train.split([0.1, 0.9])\n",
    "imdb_test, _ = imdb_test.split([0.1, 0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_train.get_dataset_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def process_dataset(dataset, tokenizer, max_seq_len=512, batch_size=4, shuffle=False):\n",
    "    is_ascend = mindspore.get_context('device_target') == 'Ascend'\n",
    "    def tokenize(text):\n",
    "        if is_ascend:\n",
    "            tokenized = tokenizer(text, padding='max_length', truncation=True, max_length=max_seq_len)\n",
    "        else:\n",
    "            tokenized = tokenizer(text, truncation=True, max_length=max_seq_len)\n",
    "        return tokenized['input_ids'], tokenized['attention_mask']\n",
    "\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(batch_size)\n",
    "\n",
    "    # map dataset\n",
    "    dataset = dataset.map(operations=[tokenize], input_columns=\"text\", output_columns=['input_ids', 'attention_mask'])\n",
    "    dataset = dataset.map(operations=transforms.TypeCast(mindspore.int32), input_columns=\"label\", output_columns=\"labels\")\n",
    "    # batch dataset\n",
    "    if is_ascend:\n",
    "        dataset = dataset.batch(batch_size)\n",
    "    else:\n",
    "        dataset = dataset.padded_batch(batch_size, pad_info={'input_ids': (None, tokenizer.pad_token_id),\n",
    "                                                             'attention_mask': (None, 0)})\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindnlp/transformers/tokenization_utils_base.py:1526: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted, and will be then set to `False` by default. \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from mindnlp.transformers import OpenAIGPTTokenizer\n",
    "# tokenizer\n",
    "gpt_tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')\n",
    "\n",
    "# add sepcial token: <PAD>\n",
    "special_tokens_dict = {\n",
    "    \"bos_token\": \"<bos>\",\n",
    "    \"eos_token\": \"<eos>\",\n",
    "    \"pad_token\": \"<pad>\",\n",
    "}\n",
    "num_added_toks = gpt_tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(44827:281472849227952,MainProcess):2025-01-05-01:03:04.229.025 [mindspore/dataset/engine/datasets.py:2534] Dataset is shuffled before split.\n"
     ]
    }
   ],
   "source": [
    "# split train dataset into train and valid datasets\n",
    "imdb_train, imdb_val = imdb_train.split([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = process_dataset(imdb_train, gpt_tokenizer, shuffle=True)\n",
    "dataset_val = process_dataset(imdb_val, gpt_tokenizer)\n",
    "dataset_test = process_dataset(imdb_test, gpt_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tensor(shape=[4, 512], dtype=Int64, value=\n",
       " [[  595,   881,   566 ... 40480, 40480, 40480],\n",
       "  [  246, 18351, 15369 ... 40480, 40480, 40480],\n",
       "  [  616,  4121,  4020 ... 40480, 40480, 40480],\n",
       "  [  249,  4336,   616 ... 40480, 40480, 40480]]),\n",
       " Tensor(shape=[4, 512], dtype=Int64, value=\n",
       " [[1, 1, 1 ... 0, 0, 0],\n",
       "  [1, 1, 1 ... 0, 0, 0],\n",
       "  [1, 1, 1 ... 0, 0, 0],\n",
       "  [1, 1, 1 ... 0, 0, 0]]),\n",
       " Tensor(shape=[4], dtype=Int32, value= [1, 1, 1, 1])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(dataset_train.create_tuple_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from mindnlp.engine.utils import EvalPrediction\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred: EvalPrediction):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] DEVICE(44827,ffff813130b0,python):2025-01-05-01:03:07.513.644 [mindspore/ccsrc/plugin/device/ascend/hal/device/ascend_vmm_adapter.h:188] CheckVmmDriverVersion] Driver version is less than 24.0.0, vmm is disabled by default, drvier_version: 23.0.6\n",
      "Some weights of OpenAIGPTForSequenceClassification were not initialized from the model checkpoint at openai-gpt and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from mindnlp.transformers import OpenAIGPTForSequenceClassification\n",
    "from mindnlp.engine import TrainingArguments\n",
    "\n",
    "model = OpenAIGPTForSequenceClassification.from_pretrained('openai-gpt', num_labels=2)\n",
    "model.config.pad_token_id = gpt_tokenizer.pad_token_id\n",
    "model.resize_token_embeddings(model.config.vocab_size + 3)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \"./output/gpt\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=200,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch'\n",
    ")\n",
    "\n",
    "trainer = Trainer(model=model, train_dataset=dataset_train,\n",
    "                  eval_dataset=dataset_val, compute_metrics=compute_metrics,\n",
    "                  args=training_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 注: 如果想要运行的更快一些，可以在训练时需要V100的算力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 200/1314 [01:09<04:34,  4.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5636, 'learning_rate': 1.69558599695586e-05, 'epoch': 0.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 201/1314 [01:09<05:49,  3.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 401/1314 [01:56<03:14,  4.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4809, 'learning_rate': 1.39117199391172e-05, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 438/1314 [02:04<03:12,  4.54it/s]\n",
      "  0%|          | 0/188 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|          | 2/188 [00:00<01:19,  2.35it/s]\u001b[A\n",
      "  2%|▏         | 3/188 [00:01<01:40,  1.85it/s]\u001b[A\n",
      "  2%|▏         | 4/188 [00:02<01:52,  1.63it/s]\u001b[A\n",
      "  3%|▎         | 5/188 [00:03<02:04,  1.48it/s]\u001b[A\n",
      "  3%|▎         | 6/188 [00:04<02:18,  1.31it/s]\u001b[A\n",
      "  4%|▎         | 7/188 [00:04<02:03,  1.47it/s]\u001b[A\n",
      "  5%|▍         | 9/188 [00:04<01:09,  2.59it/s]\u001b[A\n",
      "  6%|▌         | 11/188 [00:04<00:44,  3.93it/s]\u001b[A\n",
      "  6%|▋         | 12/188 [00:04<00:39,  4.44it/s]\u001b[A\n",
      "  7%|▋         | 14/188 [00:05<00:28,  6.01it/s]\u001b[A\n",
      "  9%|▊         | 16/188 [00:05<00:23,  7.46it/s]\u001b[A\n",
      " 10%|▉         | 18/188 [00:05<00:19,  8.83it/s]\u001b[A\n",
      " 11%|█         | 20/188 [00:05<00:16, 10.27it/s]\u001b[A\n",
      " 12%|█▏        | 22/188 [00:05<00:14, 11.30it/s]\u001b[A\n",
      " 13%|█▎        | 24/188 [00:05<00:12, 12.65it/s]\u001b[A\n",
      " 14%|█▍        | 26/188 [00:05<00:12, 12.91it/s]\u001b[A\n",
      " 15%|█▍        | 28/188 [00:06<00:12, 13.22it/s]\u001b[A\n",
      " 16%|█▌        | 30/188 [00:06<00:11, 13.46it/s]\u001b[A\n",
      " 17%|█▋        | 32/188 [00:06<00:10, 14.42it/s]\u001b[A\n",
      " 18%|█▊        | 34/188 [00:06<00:11, 13.82it/s]\u001b[A\n",
      " 19%|█▉        | 36/188 [00:06<00:11, 13.12it/s]\u001b[A\n",
      " 20%|██        | 38/188 [00:06<00:11, 13.40it/s]\u001b[A\n",
      " 21%|██▏       | 40/188 [00:06<00:10, 14.12it/s]\u001b[A\n",
      " 22%|██▏       | 42/188 [00:07<00:10, 13.45it/s]\u001b[A\n",
      " 23%|██▎       | 44/188 [00:07<00:10, 13.60it/s]\u001b[A\n",
      " 24%|██▍       | 46/188 [00:07<00:10, 13.61it/s]\u001b[A\n",
      " 26%|██▌       | 48/188 [00:07<00:09, 14.04it/s]\u001b[A\n",
      " 27%|██▋       | 50/188 [00:07<00:09, 14.50it/s]\u001b[A\n",
      " 28%|██▊       | 52/188 [00:07<00:09, 15.07it/s]\u001b[A\n",
      " 29%|██▊       | 54/188 [00:07<00:08, 15.92it/s]\u001b[A\n",
      " 30%|██▉       | 56/188 [00:07<00:08, 16.03it/s]\u001b[A\n",
      " 31%|███       | 58/188 [00:08<00:08, 15.44it/s]\u001b[A\n",
      " 32%|███▏      | 60/188 [00:08<00:08, 15.38it/s]\u001b[A\n",
      " 33%|███▎      | 62/188 [00:08<00:08, 15.63it/s]\u001b[A\n",
      " 34%|███▍      | 64/188 [00:08<00:07, 15.97it/s]\u001b[A\n",
      " 35%|███▌      | 66/188 [00:08<00:08, 15.06it/s]\u001b[A\n",
      " 36%|███▌      | 68/188 [00:08<00:08, 14.52it/s]\u001b[A\n",
      " 38%|███▊      | 72/188 [00:08<00:05, 19.40it/s]\u001b[A\n",
      " 40%|████      | 76/188 [00:08<00:04, 23.94it/s]\u001b[A\n",
      " 43%|████▎     | 80/188 [00:09<00:03, 27.49it/s]\u001b[A\n",
      " 45%|████▍     | 84/188 [00:09<00:03, 30.01it/s]\u001b[A\n",
      " 47%|████▋     | 88/188 [00:09<00:03, 31.97it/s]\u001b[A\n",
      " 49%|████▉     | 92/188 [00:09<00:02, 33.20it/s]\u001b[A\n",
      " 51%|█████     | 96/188 [00:09<00:02, 34.13it/s]\u001b[A\n",
      " 53%|█████▎    | 100/188 [00:09<00:02, 34.42it/s]\u001b[A\n",
      " 55%|█████▌    | 104/188 [00:09<00:02, 34.77it/s]\u001b[A\n",
      " 57%|█████▋    | 108/188 [00:09<00:02, 34.96it/s]\u001b[A\n",
      " 60%|█████▉    | 112/188 [00:09<00:02, 35.20it/s]\u001b[A\n",
      " 62%|██████▏   | 116/188 [00:10<00:02, 35.28it/s]\u001b[A\n",
      " 64%|██████▍   | 120/188 [00:10<00:01, 35.23it/s]\u001b[A\n",
      " 66%|██████▌   | 124/188 [00:10<00:01, 35.20it/s]\u001b[A\n",
      " 68%|██████▊   | 128/188 [00:10<00:01, 35.19it/s]\u001b[A\n",
      " 70%|███████   | 132/188 [00:10<00:01, 35.24it/s]\u001b[A\n",
      " 72%|███████▏  | 136/188 [00:10<00:01, 35.26it/s]\u001b[A\n",
      " 74%|███████▍  | 140/188 [00:10<00:01, 35.23it/s]\u001b[A\n",
      " 77%|███████▋  | 144/188 [00:10<00:01, 35.27it/s]\u001b[A\n",
      " 79%|███████▊  | 148/188 [00:11<00:01, 35.33it/s]\u001b[A\n",
      " 81%|████████  | 152/188 [00:11<00:01, 35.41it/s]\u001b[A\n",
      " 83%|████████▎ | 156/188 [00:11<00:00, 35.30it/s]\u001b[A\n",
      " 85%|████████▌ | 160/188 [00:11<00:00, 35.41it/s]\u001b[A\n",
      " 87%|████████▋ | 164/188 [00:11<00:00, 35.40it/s]\u001b[A\n",
      " 89%|████████▉ | 168/188 [00:11<00:00, 35.35it/s]\u001b[A\n",
      " 91%|█████████▏| 172/188 [00:11<00:00, 35.21it/s]\u001b[A\n",
      " 94%|█████████▎| 176/188 [00:11<00:00, 35.22it/s]\u001b[A\n",
      " 96%|█████████▌| 180/188 [00:11<00:00, 35.26it/s]\u001b[A\n",
      " 98%|█████████▊| 184/188 [00:12<00:00, 35.27it/s]\u001b[A\n",
      "                                                  [A\n",
      " 33%|███▎      | 438/1314 [02:17<03:12,  4.54it/s]\n",
      "100%|██████████| 188/188 [00:12<00:00, 33.31it/s]\u001b[A\n",
      "                                                 \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.37783360481262207, 'eval_accuracy': 0.8933333333333333, 'eval_runtime': 13.524, 'eval_samples_per_second': 13.901, 'eval_steps_per_second': 13.901, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 601/1314 [03:13<02:57,  4.03it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3495, 'learning_rate': 1.08675799086758e-05, 'epoch': 1.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 801/1314 [04:02<01:48,  4.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1889, 'learning_rate': 7.823439878234399e-06, 'epoch': 1.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 876/1314 [04:18<01:32,  4.72it/s]\n",
      "  0%|          | 0/188 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|          | 2/188 [00:00<00:50,  3.67it/s]\u001b[A\n",
      "  2%|▏         | 3/188 [00:01<01:05,  2.81it/s]\u001b[A\n",
      "  2%|▏         | 4/188 [00:01<01:04,  2.87it/s]\u001b[A\n",
      "  3%|▎         | 5/188 [00:01<01:05,  2.80it/s]\u001b[A\n",
      "  3%|▎         | 6/188 [00:02<01:11,  2.55it/s]\u001b[A\n",
      "  4%|▎         | 7/188 [00:02<01:07,  2.68it/s]\u001b[A\n",
      "  4%|▍         | 8/188 [00:02<01:06,  2.73it/s]\u001b[A\n",
      "  5%|▍         | 9/188 [00:03<01:08,  2.62it/s]\u001b[A\n",
      "  5%|▌         | 10/188 [00:03<01:02,  2.84it/s]\u001b[A\n",
      "  6%|▌         | 11/188 [00:04<01:03,  2.77it/s]\u001b[A\n",
      "  6%|▋         | 12/188 [00:04<01:06,  2.64it/s]\u001b[A\n",
      "  7%|▋         | 13/188 [00:04<01:05,  2.67it/s]\u001b[A\n",
      "  7%|▋         | 14/188 [00:05<01:04,  2.71it/s]\u001b[A\n",
      "  8%|▊         | 15/188 [00:05<01:04,  2.66it/s]\u001b[A\n",
      "  9%|▊         | 16/188 [00:05<00:56,  3.07it/s]\u001b[A\n",
      " 10%|▉         | 18/188 [00:05<00:36,  4.60it/s]\u001b[A\n",
      " 11%|█         | 20/188 [00:06<00:26,  6.40it/s]\u001b[A\n",
      " 12%|█▏        | 22/188 [00:06<00:20,  8.03it/s]\u001b[A\n",
      " 13%|█▎        | 24/188 [00:06<00:16,  9.80it/s]\u001b[A\n",
      " 14%|█▍        | 26/188 [00:06<00:14, 10.82it/s]\u001b[A\n",
      " 15%|█▍        | 28/188 [00:06<00:13, 11.70it/s]\u001b[A\n",
      " 16%|█▌        | 30/188 [00:06<00:12, 12.31it/s]\u001b[A\n",
      " 17%|█▋        | 32/188 [00:06<00:11, 13.52it/s]\u001b[A\n",
      " 18%|█▊        | 34/188 [00:06<00:11, 13.19it/s]\u001b[A\n",
      " 19%|█▉        | 36/188 [00:07<00:11, 12.73it/s]\u001b[A\n",
      " 20%|██        | 38/188 [00:07<00:11, 12.95it/s]\u001b[A\n",
      " 21%|██▏       | 40/188 [00:07<00:10, 13.76it/s]\u001b[A\n",
      " 22%|██▏       | 42/188 [00:07<00:11, 13.21it/s]\u001b[A\n",
      " 23%|██▎       | 44/188 [00:07<00:10, 13.36it/s]\u001b[A\n",
      " 24%|██▍       | 46/188 [00:07<00:10, 13.42it/s]\u001b[A\n",
      " 26%|██▌       | 48/188 [00:08<00:10, 13.80it/s]\u001b[A\n",
      " 27%|██▋       | 50/188 [00:08<00:09, 14.29it/s]\u001b[A\n",
      " 28%|██▊       | 52/188 [00:08<00:09, 14.77it/s]\u001b[A\n",
      " 29%|██▊       | 54/188 [00:08<00:08, 15.54it/s]\u001b[A\n",
      " 30%|██▉       | 56/188 [00:08<00:08, 15.65it/s]\u001b[A\n",
      " 31%|███       | 58/188 [00:08<00:08, 15.07it/s]\u001b[A\n",
      " 32%|███▏      | 60/188 [00:08<00:08, 15.04it/s]\u001b[A\n",
      " 33%|███▎      | 62/188 [00:08<00:08, 15.37it/s]\u001b[A\n",
      " 34%|███▍      | 64/188 [00:09<00:07, 15.77it/s]\u001b[A\n",
      " 35%|███▌      | 66/188 [00:09<00:08, 15.08it/s]\u001b[A\n",
      " 36%|███▌      | 68/188 [00:09<00:08, 14.50it/s]\u001b[A\n",
      " 38%|███▊      | 72/188 [00:09<00:05, 19.45it/s]\u001b[A\n",
      " 40%|████      | 76/188 [00:09<00:04, 24.11it/s]\u001b[A\n",
      " 43%|████▎     | 80/188 [00:09<00:03, 27.80it/s]\u001b[A\n",
      " 45%|████▍     | 84/188 [00:09<00:03, 30.60it/s]\u001b[A\n",
      " 47%|████▋     | 88/188 [00:09<00:03, 32.67it/s]\u001b[A\n",
      " 49%|████▉     | 92/188 [00:09<00:02, 34.18it/s]\u001b[A\n",
      " 51%|█████     | 96/188 [00:10<00:02, 35.32it/s]\u001b[A\n",
      " 53%|█████▎    | 100/188 [00:10<00:02, 36.24it/s]\u001b[A\n",
      " 55%|█████▌    | 104/188 [00:10<00:02, 36.84it/s]\u001b[A\n",
      " 57%|█████▋    | 108/188 [00:10<00:02, 37.28it/s]\u001b[A\n",
      " 60%|█████▉    | 112/188 [00:10<00:02, 37.67it/s]\u001b[A\n",
      " 62%|██████▏   | 116/188 [00:10<00:01, 37.65it/s]\u001b[A\n",
      " 64%|██████▍   | 120/188 [00:10<00:01, 37.79it/s]\u001b[A\n",
      " 66%|██████▌   | 124/188 [00:10<00:01, 37.88it/s]\u001b[A\n",
      " 68%|██████▊   | 128/188 [00:10<00:01, 37.93it/s]\u001b[A\n",
      " 70%|███████   | 132/188 [00:11<00:01, 38.05it/s]\u001b[A\n",
      " 72%|███████▏  | 136/188 [00:11<00:01, 38.17it/s]\u001b[A\n",
      " 74%|███████▍  | 140/188 [00:11<00:01, 38.26it/s]\u001b[A\n",
      " 77%|███████▋  | 144/188 [00:11<00:01, 38.35it/s]\u001b[A\n",
      " 79%|███████▊  | 148/188 [00:11<00:01, 38.46it/s]\u001b[A\n",
      " 81%|████████  | 152/188 [00:11<00:00, 38.51it/s]\u001b[A\n",
      " 83%|████████▎ | 156/188 [00:11<00:00, 38.66it/s]\u001b[A\n",
      " 85%|████████▌ | 160/188 [00:11<00:00, 38.74it/s]\u001b[A\n",
      " 87%|████████▋ | 164/188 [00:11<00:00, 38.80it/s]\u001b[A\n",
      " 89%|████████▉ | 168/188 [00:11<00:00, 38.82it/s]\u001b[A\n",
      " 91%|█████████▏| 172/188 [00:12<00:00, 38.85it/s]\u001b[A\n",
      " 94%|█████████▎| 176/188 [00:12<00:00, 38.86it/s]\u001b[A\n",
      " 96%|█████████▌| 180/188 [00:12<00:00, 38.24it/s]\u001b[A\n",
      " 98%|█████████▊| 184/188 [00:12<00:00, 38.53it/s]\u001b[A\n",
      "                                                  [A\n",
      " 67%|██████▋   | 876/1314 [04:31<01:32,  4.72it/s]\n",
      "100%|██████████| 188/188 [00:12<00:00, 38.70it/s]\u001b[A\n",
      "                                                 \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5083866119384766, 'eval_accuracy': 0.908, 'eval_runtime': 13.1799, 'eval_samples_per_second': 14.264, 'eval_steps_per_second': 14.264, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 1001/1314 [05:16<01:18,  3.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1181, 'learning_rate': 4.779299847792998e-06, 'epoch': 2.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 1201/1314 [06:05<00:24,  4.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0949, 'learning_rate': 1.7351598173515982e-06, 'epoch': 2.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1314/1314 [06:29<00:00,  4.80it/s]\n",
      "  0%|          | 0/188 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|          | 2/188 [00:00<01:11,  2.59it/s]\u001b[A\n",
      "  2%|▏         | 3/188 [00:01<01:35,  1.94it/s]\u001b[A\n",
      "  2%|▏         | 4/188 [00:02<01:44,  1.77it/s]\u001b[A\n",
      "  3%|▎         | 5/188 [00:02<01:45,  1.74it/s]\u001b[A\n",
      "  3%|▎         | 6/188 [00:03<01:44,  1.74it/s]\u001b[A\n",
      "  4%|▎         | 7/188 [00:03<01:44,  1.73it/s]\u001b[A\n",
      "  4%|▍         | 8/188 [00:04<01:35,  1.89it/s]\u001b[A\n",
      "  5%|▌         | 10/188 [00:04<00:55,  3.22it/s]\u001b[A\n",
      "  6%|▋         | 12/188 [00:04<00:39,  4.47it/s]\u001b[A\n",
      "  7%|▋         | 14/188 [00:04<00:29,  5.94it/s]\u001b[A\n",
      "  9%|▊         | 16/188 [00:04<00:23,  7.36it/s]\u001b[A\n",
      " 10%|▉         | 18/188 [00:05<00:19,  8.75it/s]\u001b[A\n",
      " 11%|█         | 20/188 [00:05<00:16, 10.24it/s]\u001b[A\n",
      " 12%|█▏        | 22/188 [00:05<00:14, 11.35it/s]\u001b[A\n",
      " 13%|█▎        | 24/188 [00:05<00:12, 12.68it/s]\u001b[A\n",
      " 14%|█▍        | 26/188 [00:05<00:12, 13.13it/s]\u001b[A\n",
      " 15%|█▍        | 28/188 [00:05<00:11, 13.54it/s]\u001b[A\n",
      " 16%|█▌        | 30/188 [00:05<00:11, 13.86it/s]\u001b[A\n",
      " 17%|█▋        | 32/188 [00:05<00:10, 14.84it/s]\u001b[A\n",
      " 18%|█▊        | 34/188 [00:06<00:10, 14.22it/s]\u001b[A\n",
      " 19%|█▉        | 36/188 [00:06<00:11, 13.54it/s]\u001b[A\n",
      " 20%|██        | 38/188 [00:06<00:10, 13.93it/s]\u001b[A\n",
      " 21%|██▏       | 40/188 [00:06<00:10, 14.78it/s]\u001b[A\n",
      " 22%|██▏       | 42/188 [00:06<00:10, 14.14it/s]\u001b[A\n",
      " 23%|██▎       | 44/188 [00:06<00:10, 14.28it/s]\u001b[A\n",
      " 24%|██▍       | 46/188 [00:06<00:09, 14.21it/s]\u001b[A\n",
      " 26%|██▌       | 48/188 [00:07<00:09, 14.64it/s]\u001b[A\n",
      " 27%|██▋       | 50/188 [00:07<00:09, 15.09it/s]\u001b[A\n",
      " 28%|██▊       | 52/188 [00:07<00:08, 15.50it/s]\u001b[A\n",
      " 29%|██▊       | 54/188 [00:07<00:08, 16.31it/s]\u001b[A\n",
      " 30%|██▉       | 56/188 [00:07<00:08, 16.37it/s]\u001b[A\n",
      " 31%|███       | 58/188 [00:07<00:08, 15.67it/s]\u001b[A\n",
      " 32%|███▏      | 60/188 [00:07<00:08, 15.58it/s]\u001b[A\n",
      " 33%|███▎      | 62/188 [00:07<00:07, 15.89it/s]\u001b[A\n",
      " 34%|███▍      | 64/188 [00:08<00:07, 16.34it/s]\u001b[A\n",
      " 35%|███▌      | 66/188 [00:08<00:07, 15.54it/s]\u001b[A\n",
      " 36%|███▌      | 68/188 [00:08<00:08, 14.90it/s]\u001b[A\n",
      " 38%|███▊      | 72/188 [00:08<00:05, 20.01it/s]\u001b[A\n",
      " 40%|████      | 76/188 [00:08<00:04, 24.73it/s]\u001b[A\n",
      " 43%|████▎     | 80/188 [00:08<00:03, 28.48it/s]\u001b[A\n",
      " 45%|████▍     | 84/188 [00:08<00:03, 31.44it/s]\u001b[A\n",
      " 47%|████▋     | 89/188 [00:08<00:02, 34.34it/s]\u001b[A\n",
      " 50%|█████     | 94/188 [00:09<00:02, 36.23it/s]\u001b[A\n",
      " 52%|█████▏    | 98/188 [00:09<00:02, 37.19it/s]\u001b[A\n",
      " 55%|█████▍    | 103/188 [00:09<00:02, 38.17it/s]\u001b[A\n",
      " 57%|█████▋    | 108/188 [00:09<00:02, 38.88it/s]\u001b[A\n",
      " 60%|█████▉    | 112/188 [00:09<00:01, 38.93it/s]\u001b[A\n",
      " 62%|██████▏   | 117/188 [00:09<00:01, 39.36it/s]\u001b[A\n",
      " 65%|██████▍   | 122/188 [00:09<00:01, 39.64it/s]\u001b[A\n",
      " 68%|██████▊   | 127/188 [00:09<00:01, 39.81it/s]\u001b[A\n",
      " 70%|██████▉   | 131/188 [00:09<00:01, 39.86it/s]\u001b[A\n",
      " 72%|███████▏  | 135/188 [00:10<00:01, 39.87it/s]\u001b[A\n",
      " 74%|███████▍  | 139/188 [00:10<00:01, 39.86it/s]\u001b[A\n",
      " 76%|███████▌  | 143/188 [00:10<00:01, 39.77it/s]\u001b[A\n",
      " 78%|███████▊  | 147/188 [00:10<00:01, 39.57it/s]\u001b[A\n",
      " 80%|████████  | 151/188 [00:10<00:00, 39.55it/s]\u001b[A\n",
      " 82%|████████▏ | 155/188 [00:10<00:00, 39.57it/s]\u001b[A\n",
      " 85%|████████▍ | 159/188 [00:10<00:00, 39.56it/s]\u001b[A\n",
      " 87%|████████▋ | 163/188 [00:10<00:00, 38.84it/s]\u001b[A\n",
      " 89%|████████▉ | 167/188 [00:10<00:00, 38.67it/s]\u001b[A\n",
      " 91%|█████████ | 171/188 [00:10<00:00, 38.52it/s]\u001b[A\n",
      " 93%|█████████▎| 175/188 [00:11<00:00, 38.42it/s]\u001b[A\n",
      " 95%|█████████▌| 179/188 [00:11<00:00, 38.36it/s]\u001b[A\n",
      " 97%|█████████▋| 183/188 [00:11<00:00, 38.32it/s]\u001b[A\n",
      "                                                   A\n",
      "100%|██████████| 1314/1314 [06:41<00:00,  4.80it/s]\n",
      "100%|██████████| 188/188 [00:11<00:00, 38.32it/s]\u001b[A\n",
      "                                                 \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5479908585548401, 'eval_accuracy': 0.9106666666666666, 'eval_runtime': 12.6013, 'eval_samples_per_second': 14.919, 'eval_steps_per_second': 14.919, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1314/1314 [06:51<00:00,  3.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 411.6353, 'train_samples_per_second': 12.769, 'train_steps_per_second': 3.192, 'train_loss': 0.280830503780185, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1314, training_loss=0.280830503780185, metrics={'train_runtime': 411.6353, 'train_samples_per_second': 12.769, 'train_steps_per_second': 3.192, 'train_loss': 0.280830503780185, 'epoch': 3.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trainer.run(tgt_columns=\"labels\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# evaluator = Evaluator(network=model, eval_dataset=dataset_test, metrics=metric)\n",
    "# evaluator.run(tgt_columns=\"labels\")\n",
    "from tqdm import tqdm\n",
    "\n",
    "def compute_accuracy(logits, labels):\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "def evaluate_fn(model, test_dataset):\n",
    "    total = test_dataset.get_dataset_size()\n",
    "    epoch_acc = 0\n",
    "    step_total = 0\n",
    "    model.set_train(False)\n",
    "\n",
    "    with tqdm(total=total) as progress_bar:\n",
    "        for batch in test_dataset.create_dict_iterator():\n",
    "            label = batch.pop('labels')\n",
    "            logits = model(**batch).logits\n",
    "\n",
    "            acc = compute_accuracy(logits, label)['accuracy']\n",
    "            epoch_acc += acc\n",
    "            \n",
    "            step_total += 1\n",
    "            acc=epoch_acc/step_total\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/188 [00:14<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9095744680851063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "acc = evaluate_fn(model, dataset_val)\n",
    "print(f\"Accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "AIGalleryInfo": {
   "item_id": "1325b719-fc78-46c4-8f47-9f3623e9b0f4"
  },
  "flavorInfo": {
   "architecture": "X86_64",
   "category": "GPU"
  },
  "imageInfo": {
   "id": "e1a07296-22a8-4f05-8bc8-e936c8e54202",
   "name": "mindspore1.7.0-cuda10.1-py3.7-ubuntu18.04"
  },
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
