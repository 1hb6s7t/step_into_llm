{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![下载Notebook](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.4.0/resource/_static/logo_notebook.svg)](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/notebook/r2.4.0/tutorials/zh_cn/cv/mindspore_shufflenet.ipynb)&emsp;[![下载样例代码](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.4.0/resource/_static/logo_download_code.svg)](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/notebook/r2.4.0/tutorials/zh_cn/cv/mindspore_shufflenet.py)&emsp;[![查看源文件](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.4.0/resource/_static/logo_source.svg)](https://gitee.com/mindspore/docs/blob/r2.4.0/tutorials/source_zh_cn/cv/shufflenet.ipynb)\n",
    "\n",
    "# ShuffleNet图像分类\n",
    "\n",
    "> 当前案例不支持在GPU设备上静态图模式运行，其他模式运行皆支持。\n",
    "\n",
    "## ShuffleNet网络介绍\n",
    "\n",
    "ShuffleNetV1是旷视科技提出的一种计算高效的CNN模型，和MobileNet, SqueezeNet等一样主要应用在移动端，所以模型的设计目标就是利用有限的计算资源来达到最好的模型精度。ShuffleNetV1的设计核心是引入了两种操作：Pointwise Group Convolution和Channel Shuffle，这在保持精度的同时大大降低了模型的计算量。因此，ShuffleNetV1和MobileNet类似，都是通过设计更高效的网络结构来实现模型的压缩和加速。\n",
    "\n",
    "> 了解ShuffleNet更多详细内容，详见论文[ShuffleNet](https://arxiv.org/abs/1707.01083)。\n",
    "\n",
    "如下图所示，ShuffleNet在保持不低的准确率的前提下，将参数量几乎降低到了最小，因此其运算速度较快，单位参数量对模型准确率的贡献非常高。\n",
    "\n",
    "![shufflenet1](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.4.0/tutorials/source_zh_cn/cv/images/shufflenet_1.png)\n",
    "\n",
    "> 图片来源：Bianco S, Cadene R, Celona L, et al. Benchmark analysis of representative deep neural network architectures[J]. IEEE access, 2018, 6: 64270-64277.\n",
    "\n",
    "## 模型架构\n",
    "\n",
    "ShuffleNet最显著的特点在于对不同通道进行重排来解决Group Convolution带来的弊端。通过对ResNet的Bottleneck单元进行改进，在较小的计算量的情况下达到了较高的准确率。\n",
    "\n",
    "### Pointwise Group Convolution\n",
    "\n",
    "Group Convolution（分组卷积）原理如下图所示，相比于普通的卷积操作，分组卷积的情况下，每一组的卷积核大小为in_channels/g\\*k\\*k，一共有g组，所有组共有(in_channels/g\\*k\\*k)\\*out_channels个参数，是正常卷积参数的1/g。分组卷积中，每个卷积核只处理输入特征图的一部分通道，**其优点在于参数量会有所降低，但输出通道数仍等于卷积核的数量**。\n",
    "\n",
    "![shufflenet2](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.4.0/tutorials/source_zh_cn/cv/images/shufflenet_2.png)\n",
    "\n",
    "> 图片来源：Huang G, Liu S, Van der Maaten L, et al. Condensenet: An efficient densenet using learned group convolutions[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 2752-2761.\n",
    "\n",
    "Depthwise Convolution（深度可分离卷积）将组数g分为和输入通道相等的`in_channels`，然后对每一个`in_channels`做卷积操作，每个卷积核只处理一个通道，记卷积核大小为1\\*k\\*k，则卷积核参数量为：in_channels\\*k\\*k，**得到的feature maps通道数与输入通道数相等**；\n",
    "\n",
    "Pointwise Group Convolution（逐点分组卷积）在分组卷积的基础上，令**每一组的卷积核大小为** $1\\times 1$，卷积核参数量为(in_channels/g\\*1\\*1)\\*out_channels。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] GE_ADPT(11825,ffffb01e3010,python):2024-11-22-21:09:49.085.529 [mindspore/ccsrc/utils/dlopen_macro.h:163] DlsymAscend] Dynamically load symbol aclmdlBundleGetModelId failed, result = /usr/local/Ascend/ascend-toolkit/latest/lib64/libascendcl.so: undefined symbol: aclmdlBundleGetModelId\n",
      "[WARNING] GE_ADPT(11825,ffffb01e3010,python):2024-11-22-21:09:49.085.590 [mindspore/ccsrc/utils/dlopen_macro.h:163] DlsymAscend] Dynamically load symbol aclmdlBundleLoadFromMem failed, result = /usr/local/Ascend/ascend-toolkit/latest/lib64/libascendcl.so: undefined symbol: aclmdlBundleLoadFromMem\n",
      "[WARNING] GE_ADPT(11825,ffffb01e3010,python):2024-11-22-21:09:49.085.608 [mindspore/ccsrc/utils/dlopen_macro.h:163] DlsymAscend] Dynamically load symbol aclmdlBundleUnload failed, result = /usr/local/Ascend/ascend-toolkit/latest/lib64/libascendcl.so: undefined symbol: aclmdlBundleUnload\n",
      "[WARNING] ME(11825:281473636511760,MainProcess):2024-11-22-21:09:49.237.691 [mindspore/run_check/_check_version.py:396] Can not find the tbe operator implementation(need by mindspore-ascend). Please check whether the Environment Variable PYTHONPATH is set. For details, refer to the installation guidelines: https://www.mindspore.cn/install\n"
     ]
    }
   ],
   "source": [
    "from mindspore import nn\n",
    "import mindspore.ops as ops\n",
    "from mindspore import Tensor\n",
    "\n",
    "class GroupConv(nn.Cell):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size,\n",
    "                 stride, pad_mode=\"pad\", pad=0, groups=1, has_bias=False):\n",
    "        super(GroupConv, self).__init__()\n",
    "        self.groups = groups\n",
    "        self.convs = nn.CellList()\n",
    "        for _ in range(groups):\n",
    "            self.convs.append(nn.Conv2d(in_channels // groups, out_channels // groups,\n",
    "                                        kernel_size=kernel_size, stride=stride, has_bias=has_bias,\n",
    "                                        padding=pad, pad_mode=pad_mode, group=1, weight_init='xavier_uniform'))\n",
    "\n",
    "    def construct(self, x):\n",
    "        features = ops.split(x, split_size_or_sections=int(len(x[0]) // self.groups), axis=1)\n",
    "        outputs = ()\n",
    "        for i in range(self.groups):\n",
    "            outputs = outputs + (self.convs[i](features[i].astype(\"float32\")),)\n",
    "        out = ops.cat(outputs, axis=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Channel Shuffle\n",
    "\n",
    "Group Convolution的弊端在于不同组别的通道无法进行信息交流，堆积GConv层后一个问题是不同组之间的特征图是不通信的，这就好像分成了g个互不相干的道路，每一个人各走各的，**这可能会降低网络的特征提取能力**。这也是Xception，MobileNet等网络采用密集的1x1卷积（Dense Pointwise Convolution）的原因。\n",
    "\n",
    "为了解决不同组别通道“近亲繁殖”的问题，ShuffleNet优化了大量密集的1x1卷积（在使用的情况下计算量占用率达到了惊人的93.4%），引入Channel Shuffle机制（通道重排）。这项操作直观上表现为将不同分组通道**均匀分散重组**，使网络在下一层能处理不同组别通道的信息。\n",
    "\n",
    "![shufflenet3](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.4.0/tutorials/source_zh_cn/cv/images/shufflenet_3.png)\n",
    "\n",
    "如下图所示，对于g组，每组有n个通道的特征图，首先reshape成g行n列的矩阵，再将矩阵转置成n行g列，最后进行flatten操作，得到新的排列。这些操作都是可微分可导的且计算简单，在解决了信息交互的同时符合了ShuffleNet轻量级网络设计的轻量特征。\n",
    "\n",
    "![shufflenet4](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.4.0/tutorials/source_zh_cn/cv/images/shufflenet_4.png)\n",
    "\n",
    "为了阅读方便，将Channel Shuffle的代码实现放在下方ShuffleNet模块的代码中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ShuffleNet模块\n",
    "\n",
    "如下图所示，ShuffleNet对ResNet中的Bottleneck结构进行由(a)到(b), (c)的更改：\n",
    "\n",
    "1. 将开始和最后的$1\\times 1$卷积模块（降维、升维）改成Point Wise Group Convolution；\n",
    "\n",
    "2. 为了进行不同通道的信息交流，再降维之后进行Channel Shuffle；\n",
    "\n",
    "3. 降采样模块中，$3 \\times 3$ Depth Wise Convolution的步长设置为2，长宽降为原来的一半，因此shortcut中采用步长为2的$3\\times 3$平均池化，并把相加改成拼接。\n",
    "\n",
    "![shufflenet5](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.4.0/tutorials/source_zh_cn/cv/images/shufflenet_5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShuffleV1Block(nn.Cell):\n",
    "    def __init__(self, inp, oup, group, first_group, mid_channels, ksize, stride):\n",
    "        super(ShuffleV1Block, self).__init__()\n",
    "        self.stride = stride\n",
    "        pad = ksize // 2\n",
    "        self.group = group\n",
    "        if stride == 2:\n",
    "            outputs = oup - inp\n",
    "        else:\n",
    "            outputs = oup\n",
    "        self.relu = nn.ReLU()\n",
    "        branch_main_1 = [\n",
    "            GroupConv(in_channels=inp, out_channels=mid_channels,\n",
    "                      kernel_size=1, stride=1, pad_mode=\"pad\", pad=0,\n",
    "                      groups=1 if first_group else group),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(),\n",
    "        ]\n",
    "        branch_main_2 = [\n",
    "            nn.Conv2d(mid_channels, mid_channels, kernel_size=ksize, stride=stride,\n",
    "                      pad_mode='pad', padding=pad, group=mid_channels,\n",
    "                      weight_init='xavier_uniform', has_bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            GroupConv(in_channels=mid_channels, out_channels=outputs,\n",
    "                      kernel_size=1, stride=1, pad_mode=\"pad\", pad=0,\n",
    "                      groups=group),\n",
    "            nn.BatchNorm2d(outputs),\n",
    "        ]\n",
    "        self.branch_main_1 = nn.SequentialCell(branch_main_1)\n",
    "        self.branch_main_2 = nn.SequentialCell(branch_main_2)\n",
    "        if stride == 2:\n",
    "            self.branch_proj = nn.AvgPool2d(kernel_size=3, stride=2, pad_mode='same')\n",
    "\n",
    "    def construct(self, old_x):\n",
    "        left = old_x\n",
    "        right = old_x\n",
    "        out = old_x\n",
    "        right = self.branch_main_1(right)\n",
    "        if self.group > 1:\n",
    "            right = self.channel_shuffle(right)\n",
    "        right = self.branch_main_2(right)\n",
    "        if self.stride == 1:\n",
    "            out = self.relu(left + right)\n",
    "        elif self.stride == 2:\n",
    "            left = self.branch_proj(left)\n",
    "            out = ops.cat((left, right), 1)\n",
    "            out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "    def channel_shuffle(self, x):\n",
    "        batchsize, num_channels, height, width = ops.shape(x)\n",
    "        group_channels = num_channels // self.group\n",
    "        x = ops.reshape(x, (batchsize, group_channels, self.group, height, width))\n",
    "        x = ops.transpose(x, (0, 2, 1, 3, 4))\n",
    "        x = ops.reshape(x, (batchsize, num_channels, height, width))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建ShuffleNet网络\n",
    "\n",
    "ShuffleNet网络结构如下图所示，以输入图像$224 \\times 224$，组数3（g = 3）为例，首先通过数量24，卷积核大小为$3 \\times 3$，stride为2的卷积层，输出特征图大小为$112 \\times 112$，channel为24；然后通过stride为2的最大池化层，输出特征图大小为$56 \\times 56$，channel数不变；再堆叠3个ShuffleNet模块（Stage2, Stage3, Stage4），三个模块分别重复4次、8次、4次，其中每个模块开始先经过一次下采样模块（上图(c)），使特征图长宽减半，channel翻倍（Stage2的下采样模块除外，将channel数从24变为240）；随后经过全局平均池化，输出大小为$1 \\times 1 \\times 960$，再经过全连接层和softmax，得到分类概率。\n",
    "\n",
    "![shufflenet6](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.4.0/tutorials/source_zh_cn/cv/images/shufflenet_6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShuffleNetV1(nn.Cell):\n",
    "    def __init__(self, n_class=1000, model_size='2.0x', group=3):\n",
    "        super(ShuffleNetV1, self).__init__()\n",
    "        print('model size is ', model_size)\n",
    "        self.stage_repeats = [4, 8, 4]\n",
    "        self.model_size = model_size\n",
    "        if group == 3:\n",
    "            if model_size == '0.5x':\n",
    "                self.stage_out_channels = [-1, 12, 120, 240, 480]\n",
    "            elif model_size == '1.0x':\n",
    "                self.stage_out_channels = [-1, 24, 240, 480, 960]\n",
    "            elif model_size == '1.5x':\n",
    "                self.stage_out_channels = [-1, 24, 360, 720, 1440]\n",
    "            elif model_size == '2.0x':\n",
    "                self.stage_out_channels = [-1, 48, 480, 960, 1920]\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "        elif group == 8:\n",
    "            if model_size == '0.5x':\n",
    "                self.stage_out_channels = [-1, 16, 192, 384, 768]\n",
    "            elif model_size == '1.0x':\n",
    "                self.stage_out_channels = [-1, 24, 384, 768, 1536]\n",
    "            elif model_size == '1.5x':\n",
    "                self.stage_out_channels = [-1, 24, 576, 1152, 2304]\n",
    "            elif model_size == '2.0x':\n",
    "                self.stage_out_channels = [-1, 48, 768, 1536, 3072]\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "        input_channel = self.stage_out_channels[1]\n",
    "        self.first_conv = nn.SequentialCell(\n",
    "            nn.Conv2d(3, input_channel, 3, 2, 'pad', 1, weight_init='xavier_uniform', has_bias=False),\n",
    "            nn.BatchNorm2d(input_channel),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, pad_mode='same')\n",
    "        features = []\n",
    "        for idxstage in range(len(self.stage_repeats)):\n",
    "            numrepeat = self.stage_repeats[idxstage]\n",
    "            output_channel = self.stage_out_channels[idxstage + 2]\n",
    "            for i in range(numrepeat):\n",
    "                stride = 2 if i == 0 else 1\n",
    "                first_group = idxstage == 0 and i == 0\n",
    "                features.append(ShuffleV1Block(input_channel, output_channel,\n",
    "                                               group=group, first_group=first_group,\n",
    "                                               mid_channels=output_channel // 4, ksize=3, stride=stride))\n",
    "                input_channel = output_channel\n",
    "        self.features = nn.SequentialCell(features)\n",
    "        self.globalpool = nn.AvgPool2d(7)\n",
    "        self.classifier = nn.Dense(self.stage_out_channels[-1], n_class)\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.first_conv(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.features(x)\n",
    "        x = self.globalpool(x)\n",
    "        x = ops.reshape(x, (-1, self.stage_out_channels[-1]))\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练和评估\n",
    "\n",
    "采用CIFAR-10数据集对ShuffleNet进行预训练。\n",
    "\n",
    "### 训练集准备与加载\n",
    "\n",
    "采用CIFAR-10数据集对ShuffleNet进行预训练。CIFAR-10共有60000张32*32的彩色图像，均匀地分为10个类别，其中50000张图片作为训练集，10000图片作为测试集。如下示例使用`mindspore.dataset.Cifar10Dataset`接口下载并加载CIFAR-10的训练集。目前仅支持二进制版本（CIFAR-10 binary version）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/notebook/datasets/cifar-10-binary.tar.gz (162.2 MB)\n",
      "\n",
      "file_sizes: 100%|████████████████████████████| 170M/170M [00:12<00:00, 13.9MB/s]\n",
      "Extracting tar.gz file...\n",
      "Successfully downloaded / unzipped to ./dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./dataset'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from download import download\n",
    "\n",
    "url = \"https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/notebook/datasets/cifar-10-binary.tar.gz\"\n",
    "\n",
    "download(url, \"./dataset\", kind=\"tar.gz\", replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore as ms\n",
    "from mindspore.dataset import Cifar10Dataset\n",
    "from mindspore.dataset import vision, transforms\n",
    "\n",
    "def get_dataset(train_dataset_path, batch_size, usage):\n",
    "    image_trans = []\n",
    "    if usage == \"train\":\n",
    "        image_trans = [\n",
    "            vision.RandomCrop((32, 32), (4, 4, 4, 4)),\n",
    "            vision.RandomHorizontalFlip(prob=0.5),\n",
    "            vision.Resize((224, 224)),\n",
    "            vision.Rescale(1.0 / 255.0, 0.0),\n",
    "            vision.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010]),\n",
    "            vision.HWC2CHW()\n",
    "        ]\n",
    "    elif usage == \"test\":\n",
    "        image_trans = [\n",
    "            vision.Resize((224, 224)),\n",
    "            vision.Rescale(1.0 / 255.0, 0.0),\n",
    "            vision.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010]),\n",
    "            vision.HWC2CHW()\n",
    "        ]\n",
    "    label_trans = transforms.TypeCast(ms.int32)\n",
    "    dataset = Cifar10Dataset(train_dataset_path, usage=usage, shuffle=True)\n",
    "    dataset = dataset.map(image_trans, 'image')\n",
    "    dataset = dataset.map(label_trans, 'label')\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    return dataset\n",
    "\n",
    "dataset = get_dataset(\"./dataset/cifar-10-batches-bin\", 128, \"train\")\n",
    "batches_per_epoch = dataset.get_dataset_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练\n",
    "\n",
    "本节用随机初始化的参数做预训练。首先调用`ShuffleNetV1`定义网络，参数量选择`\"2.0x\"`，并定义损失函数为交叉熵损失，学习率经过4轮的`warmup`后采用余弦退火，优化器采用`Momentum`。最后用`train.model`中的`Model`接口将模型、损失函数、优化器封装在`model`中，并用`model.train()`对网络进行训练。将`ModelCheckpoint`、`CheckpointConfig`、`TimeMonitor`和`LossMonitor`传入回调函数中，将会打印训练的轮数、损失和时间，并将ckpt文件保存在当前目录下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(11825:281473636511760,MainProcess):2024-11-22-21:10:23.866.116 [mindspore/run_check/_check_version.py:396] Can not find the tbe operator implementation(need by mindspore-ascend). Please check whether the Environment Variable PYTHONPATH is set. For details, refer to the installation guidelines: https://www.mindspore.cn/install\n",
      "[WARNING] ME(11825:281473636511760,MainProcess):2024-11-22-21:10:23.868.497 [mindspore/run_check/_check_version.py:396] Can not find the tbe operator implementation(need by mindspore-ascend). Please check whether the Environment Variable PYTHONPATH is set. For details, refer to the installation guidelines: https://www.mindspore.cn/install\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-21:10:23.873.285 [mindspore/core/utils/ms_context.cc:530] GetJitLevel] Set jit level to O2 for rank table startup method.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size is  2.0x\n",
      "============== Starting Training ==============\n",
      "-\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Ascend/ascend-toolkit/8.0.RC3.alpha001/opp/built-in/op_impl/ai_core/tbe/impl/util/util_conv2d_dynamic.py:130: UserWarning: conv2d fmap ori_range changed from [[32, 2147483647], [48, 48], [16, 63], [16, 63]] to [[32, 2147483647], [48, 48], [16, 63], (16, 63)].\n",
      "  warnings.warn(to_print)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Ascend/ascend-toolkit/8.0.RC3.alpha001/opp/built-in/op_impl/ai_core/tbe/impl/util/util_conv2d_dynamic.py:130: UserWarning: conv2d fmap ori_range changed from [[32, 2147483647], [480, 480], [16, 63], [16, 63]] to [[32, 2147483647], [480, 480], [16, 63], (16, 63)].\n",
      "  warnings.warn(to_print)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Ascend/ascend-toolkit/8.0.RC3.alpha001/opp/built-in/op_impl/ai_core/tbe/impl/util/util_conv2d_dynamic.py:130: UserWarning: conv2d fmap ori_range changed from [[32, 2147483647], [960, 960], [4, 15], [4, 15]] to [[32, 2147483647], [960, 960], [4, 15], (4, 15)].\n",
      "  warnings.warn(to_print)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Ascend/ascend-toolkit/8.0.RC3.alpha001/opp/built-in/op_impl/ai_core/tbe/impl/util/util_conv2d_dynamic.py:130: UserWarning: conv2d fmap ori_range changed from [[32, 2147483647], [1920, 1920], [7, 15], [7, 15]] to [[32, 2147483647], [1920, 1920], [7, 15], (7, 15)].\n",
      "  warnings.warn(to_print)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 1, loss is 2.7045326232910156\n",
      "epoch: 1 step: 2, loss is 2.4297170639038086\n",
      "epoch: 1 step: 3, loss is 2.3773369789123535\n",
      "epoch: 1 step: 4, loss is 2.3594677448272705\n",
      "epoch: 1 step: 5, loss is 2.428555488586426\n",
      "epoch: 1 step: 6, loss is 2.4701077938079834\n",
      "epoch: 1 step: 7, loss is 2.4211666584014893\n",
      "epoch: 1 step: 8, loss is 2.3893022537231445\n",
      "epoch: 1 step: 9, loss is 2.3408799171447754\n",
      "epoch: 1 step: 10, loss is 2.312217950820923\n",
      "epoch: 1 step: 11, loss is 2.3299927711486816\n",
      "epoch: 1 step: 12, loss is 2.3413565158843994\n",
      "epoch: 1 step: 13, loss is 2.3667845726013184\n",
      "epoch: 1 step: 14, loss is 2.337486743927002\n",
      "epoch: 1 step: 15, loss is 2.2735753059387207\n",
      "epoch: 1 step: 16, loss is 2.3029141426086426\n",
      "epoch: 1 step: 17, loss is 2.3207013607025146\n",
      "epoch: 1 step: 18, loss is 2.2434487342834473\n",
      "epoch: 1 step: 19, loss is 2.37697696685791\n",
      "epoch: 1 step: 20, loss is 2.2701454162597656\n",
      "epoch: 1 step: 21, loss is 2.1911630630493164\n",
      "epoch: 1 step: 22, loss is 2.188706874847412\n",
      "epoch: 1 step: 23, loss is 2.269958257675171\n",
      "epoch: 1 step: 24, loss is 2.200922966003418\n",
      "epoch: 1 step: 25, loss is 2.2487034797668457\n",
      "epoch: 1 step: 26, loss is 2.210291624069214\n",
      "epoch: 1 step: 27, loss is 2.1788363456726074\n",
      "epoch: 1 step: 28, loss is 2.2182300090789795\n",
      "epoch: 1 step: 29, loss is 2.190652847290039\n",
      "epoch: 1 step: 30, loss is 2.200214147567749\n",
      "epoch: 1 step: 31, loss is 2.187483787536621\n",
      "epoch: 1 step: 32, loss is 2.2492220401763916\n",
      "epoch: 1 step: 33, loss is 2.202796697616577\n",
      "epoch: 1 step: 34, loss is 2.1295721530914307\n",
      "epoch: 1 step: 35, loss is 2.2072367668151855\n",
      "epoch: 1 step: 36, loss is 2.1801059246063232\n",
      "epoch: 1 step: 37, loss is 2.2200796604156494\n",
      "epoch: 1 step: 38, loss is 2.1639204025268555\n",
      "epoch: 1 step: 39, loss is 2.0903351306915283\n",
      "epoch: 1 step: 40, loss is 2.11934232711792\n",
      "epoch: 1 step: 41, loss is 2.1767454147338867\n",
      "epoch: 1 step: 42, loss is 2.1329727172851562\n",
      "epoch: 1 step: 43, loss is 2.1529533863067627\n",
      "epoch: 1 step: 44, loss is 2.1483163833618164\n",
      "epoch: 1 step: 45, loss is 2.140941619873047\n",
      "epoch: 1 step: 46, loss is 2.095781087875366\n",
      "epoch: 1 step: 47, loss is 2.0752248764038086\n",
      "epoch: 1 step: 48, loss is 2.1237993240356445\n",
      "epoch: 1 step: 49, loss is 2.1654536724090576\n",
      "epoch: 1 step: 50, loss is 2.1082704067230225\n",
      "epoch: 1 step: 51, loss is 2.1507012844085693\n",
      "epoch: 1 step: 52, loss is 2.075218915939331\n",
      "epoch: 1 step: 53, loss is 2.1514933109283447\n",
      "epoch: 1 step: 54, loss is 2.0827343463897705\n",
      "epoch: 1 step: 55, loss is 2.050493001937866\n",
      "epoch: 1 step: 56, loss is 2.1714367866516113\n",
      "epoch: 1 step: 57, loss is 2.1808927059173584\n",
      "epoch: 1 step: 58, loss is 2.053356170654297\n",
      "epoch: 1 step: 59, loss is 2.1227200031280518\n",
      "epoch: 1 step: 60, loss is 2.0421667098999023\n",
      "epoch: 1 step: 61, loss is 2.0837676525115967\n",
      "epoch: 1 step: 62, loss is 2.1855711936950684\n",
      "epoch: 1 step: 63, loss is 2.100964307785034\n",
      "epoch: 1 step: 64, loss is 2.056723117828369\n",
      "epoch: 1 step: 65, loss is 2.0960402488708496\n",
      "epoch: 1 step: 66, loss is 2.087071418762207\n",
      "epoch: 1 step: 67, loss is 2.077333688735962\n",
      "epoch: 1 step: 68, loss is 2.103856086730957\n",
      "epoch: 1 step: 69, loss is 2.134664535522461\n",
      "epoch: 1 step: 70, loss is 2.102674961090088\n",
      "epoch: 1 step: 71, loss is 2.100358486175537\n",
      "epoch: 1 step: 72, loss is 2.0841100215911865\n",
      "epoch: 1 step: 73, loss is 2.205552339553833\n",
      "epoch: 1 step: 74, loss is 1.9542889595031738\n",
      "epoch: 1 step: 75, loss is 2.0906519889831543\n",
      "epoch: 1 step: 76, loss is 2.020677089691162\n",
      "epoch: 1 step: 77, loss is 2.0763204097747803\n",
      "epoch: 1 step: 78, loss is 2.058709144592285\n",
      "epoch: 1 step: 79, loss is 2.037187337875366\n",
      "epoch: 1 step: 80, loss is 1.9652031660079956\n",
      "epoch: 1 step: 81, loss is 2.068389654159546\n",
      "epoch: 1 step: 82, loss is 2.190096139907837\n",
      "epoch: 1 step: 83, loss is 2.141512632369995\n",
      "epoch: 1 step: 84, loss is 2.036909580230713\n",
      "epoch: 1 step: 85, loss is 2.1601529121398926\n",
      "epoch: 1 step: 86, loss is 2.0518386363983154\n",
      "epoch: 1 step: 87, loss is 2.0754456520080566\n",
      "epoch: 1 step: 88, loss is 2.1133108139038086\n",
      "epoch: 1 step: 89, loss is 2.0085442066192627\n",
      "epoch: 1 step: 90, loss is 2.0681047439575195\n",
      "epoch: 1 step: 91, loss is 2.074331283569336\n",
      "epoch: 1 step: 92, loss is 1.9655704498291016\n",
      "epoch: 1 step: 93, loss is 2.0204453468322754\n",
      "epoch: 1 step: 94, loss is 1.9979422092437744\n",
      "epoch: 1 step: 95, loss is 2.053238868713379\n",
      "epoch: 1 step: 96, loss is 2.0628960132598877\n",
      "epoch: 1 step: 97, loss is 2.0323750972747803\n",
      "epoch: 1 step: 98, loss is 1.9990330934524536\n",
      "epoch: 1 step: 99, loss is 2.030705690383911\n",
      "epoch: 1 step: 100, loss is 2.0278983116149902\n",
      "epoch: 1 step: 101, loss is 2.0419633388519287\n",
      "epoch: 1 step: 102, loss is 1.9993759393692017\n",
      "epoch: 1 step: 103, loss is 2.080458164215088\n",
      "epoch: 1 step: 104, loss is 2.0549211502075195\n",
      "epoch: 1 step: 105, loss is 2.003319263458252\n",
      "epoch: 1 step: 106, loss is 2.051405191421509\n",
      "epoch: 1 step: 107, loss is 2.046905755996704\n",
      "epoch: 1 step: 108, loss is 2.0174081325531006\n",
      "epoch: 1 step: 109, loss is 1.9650417566299438\n",
      "epoch: 1 step: 110, loss is 1.9839047193527222\n",
      "epoch: 1 step: 111, loss is 2.0257532596588135\n",
      "epoch: 1 step: 112, loss is 1.9783114194869995\n",
      "epoch: 1 step: 113, loss is 2.0271997451782227\n",
      "epoch: 1 step: 114, loss is 1.9964654445648193\n",
      "epoch: 1 step: 115, loss is 2.0149619579315186\n",
      "epoch: 1 step: 116, loss is 2.0488290786743164\n",
      "epoch: 1 step: 117, loss is 1.9951342344284058\n",
      "epoch: 1 step: 118, loss is 1.9837828874588013\n",
      "epoch: 1 step: 119, loss is 2.0601680278778076\n",
      "epoch: 1 step: 120, loss is 2.133901834487915\n",
      "epoch: 1 step: 121, loss is 2.020021438598633\n",
      "epoch: 1 step: 122, loss is 1.9648263454437256\n",
      "epoch: 1 step: 123, loss is 1.977852463722229\n",
      "epoch: 1 step: 124, loss is 1.9866948127746582\n",
      "epoch: 1 step: 125, loss is 2.0547115802764893\n",
      "epoch: 1 step: 126, loss is 2.00142502784729\n",
      "epoch: 1 step: 127, loss is 2.0715084075927734\n",
      "epoch: 1 step: 128, loss is 2.0677413940429688\n",
      "epoch: 1 step: 129, loss is 1.9718257188796997\n",
      "epoch: 1 step: 130, loss is 2.029982089996338\n",
      "epoch: 1 step: 131, loss is 2.055715322494507\n",
      "epoch: 1 step: 132, loss is 2.0754635334014893\n",
      "epoch: 1 step: 133, loss is 1.9996469020843506\n",
      "epoch: 1 step: 134, loss is 1.996026873588562\n",
      "epoch: 1 step: 135, loss is 1.9414938688278198\n",
      "epoch: 1 step: 136, loss is 1.979081630706787\n",
      "epoch: 1 step: 137, loss is 1.883628249168396\n",
      "epoch: 1 step: 138, loss is 1.9936153888702393\n",
      "epoch: 1 step: 139, loss is 1.9726465940475464\n",
      "epoch: 1 step: 140, loss is 2.0227761268615723\n",
      "epoch: 1 step: 141, loss is 1.9922586679458618\n",
      "epoch: 1 step: 142, loss is 2.0001163482666016\n",
      "epoch: 1 step: 143, loss is 2.0124733448028564\n",
      "epoch: 1 step: 144, loss is 2.0239012241363525\n",
      "epoch: 1 step: 145, loss is 1.967778205871582\n",
      "epoch: 1 step: 146, loss is 1.9856994152069092\n",
      "epoch: 1 step: 147, loss is 1.9283901453018188\n",
      "epoch: 1 step: 148, loss is 1.9791945219039917\n",
      "epoch: 1 step: 149, loss is 1.980053186416626\n",
      "epoch: 1 step: 150, loss is 1.9799649715423584\n",
      "epoch: 1 step: 151, loss is 1.9393376111984253\n",
      "epoch: 1 step: 152, loss is 1.9895823001861572\n",
      "epoch: 1 step: 153, loss is 1.8479770421981812\n",
      "epoch: 1 step: 154, loss is 1.9862157106399536\n",
      "epoch: 1 step: 155, loss is 1.9474316835403442\n",
      "epoch: 1 step: 156, loss is 2.0114948749542236\n",
      "epoch: 1 step: 157, loss is 2.00577974319458\n",
      "epoch: 1 step: 158, loss is 1.980332612991333\n",
      "epoch: 1 step: 159, loss is 1.9804142713546753\n",
      "epoch: 1 step: 160, loss is 2.0386574268341064\n",
      "epoch: 1 step: 161, loss is 2.077336549758911\n",
      "epoch: 1 step: 162, loss is 1.8915318250656128\n",
      "epoch: 1 step: 163, loss is 1.962664246559143\n",
      "epoch: 1 step: 164, loss is 2.038909912109375\n",
      "epoch: 1 step: 165, loss is 2.0415947437286377\n",
      "epoch: 1 step: 166, loss is 1.9614626169204712\n",
      "epoch: 1 step: 167, loss is 2.0117523670196533\n",
      "epoch: 1 step: 168, loss is 2.0224101543426514\n",
      "epoch: 1 step: 169, loss is 1.947633147239685\n",
      "epoch: 1 step: 170, loss is 1.9922688007354736\n",
      "epoch: 1 step: 171, loss is 1.9154891967773438\n",
      "epoch: 1 step: 172, loss is 1.8837952613830566\n",
      "epoch: 1 step: 173, loss is 1.970343828201294\n",
      "epoch: 1 step: 174, loss is 1.9448961019515991\n",
      "epoch: 1 step: 175, loss is 2.03334903717041\n",
      "epoch: 1 step: 176, loss is 1.9650554656982422\n",
      "epoch: 1 step: 177, loss is 1.9494843482971191\n",
      "epoch: 1 step: 178, loss is 1.9962995052337646\n",
      "epoch: 1 step: 179, loss is 1.9986743927001953\n",
      "epoch: 1 step: 180, loss is 1.9384503364562988\n",
      "epoch: 1 step: 181, loss is 2.115180730819702\n",
      "epoch: 1 step: 182, loss is 1.9880766868591309\n",
      "epoch: 1 step: 183, loss is 1.9708445072174072\n",
      "epoch: 1 step: 184, loss is 1.967747688293457\n",
      "epoch: 1 step: 185, loss is 1.992913842201233\n",
      "epoch: 1 step: 186, loss is 1.8817598819732666\n",
      "epoch: 1 step: 187, loss is 2.0446577072143555\n",
      "epoch: 1 step: 188, loss is 1.9904955625534058\n",
      "epoch: 1 step: 189, loss is 1.992323398590088\n",
      "epoch: 1 step: 190, loss is 2.021892786026001\n",
      "epoch: 1 step: 191, loss is 1.9671850204467773\n",
      "epoch: 1 step: 192, loss is 2.01259446144104\n",
      "epoch: 1 step: 193, loss is 1.9324965476989746\n",
      "epoch: 1 step: 194, loss is 1.9554868936538696\n",
      "epoch: 1 step: 195, loss is 1.9154616594314575\n",
      "epoch: 1 step: 196, loss is 1.936394453048706\n",
      "epoch: 1 step: 197, loss is 1.9169068336486816\n",
      "epoch: 1 step: 198, loss is 2.0222647190093994\n",
      "epoch: 1 step: 199, loss is 1.9322456121444702\n",
      "epoch: 1 step: 200, loss is 1.9493210315704346\n",
      "epoch: 1 step: 201, loss is 1.8992329835891724\n",
      "epoch: 1 step: 202, loss is 1.9097914695739746\n",
      "epoch: 1 step: 203, loss is 1.8490853309631348\n",
      "epoch: 1 step: 204, loss is 1.9249014854431152\n",
      "epoch: 1 step: 205, loss is 1.781200885772705\n",
      "epoch: 1 step: 206, loss is 2.0260848999023438\n",
      "epoch: 1 step: 207, loss is 1.9688332080841064\n",
      "epoch: 1 step: 208, loss is 1.914362907409668\n",
      "epoch: 1 step: 209, loss is 1.9563026428222656\n",
      "epoch: 1 step: 210, loss is 1.8516042232513428\n",
      "epoch: 1 step: 211, loss is 1.94370698928833\n",
      "epoch: 1 step: 212, loss is 1.9105091094970703\n",
      "epoch: 1 step: 213, loss is 1.9276528358459473\n",
      "epoch: 1 step: 214, loss is 1.9104366302490234\n",
      "epoch: 1 step: 215, loss is 1.9084393978118896\n",
      "epoch: 1 step: 216, loss is 1.925203800201416\n",
      "epoch: 1 step: 217, loss is 1.899909496307373\n",
      "epoch: 1 step: 218, loss is 2.0236380100250244\n",
      "epoch: 1 step: 219, loss is 1.9182403087615967\n",
      "epoch: 1 step: 220, loss is 1.9504972696304321\n",
      "epoch: 1 step: 221, loss is 1.8400804996490479\n",
      "epoch: 1 step: 222, loss is 1.883099913597107\n",
      "epoch: 1 step: 223, loss is 1.92307448387146\n",
      "epoch: 1 step: 224, loss is 1.8402960300445557\n",
      "epoch: 1 step: 225, loss is 1.9774012565612793\n",
      "epoch: 1 step: 226, loss is 1.9876115322113037\n",
      "epoch: 1 step: 227, loss is 2.0012478828430176\n",
      "epoch: 1 step: 228, loss is 1.943044662475586\n",
      "epoch: 1 step: 229, loss is 1.876057505607605\n",
      "epoch: 1 step: 230, loss is 1.9552240371704102\n",
      "epoch: 1 step: 231, loss is 1.864089846611023\n",
      "epoch: 1 step: 232, loss is 1.9491647481918335\n",
      "epoch: 1 step: 233, loss is 1.9949067831039429\n",
      "epoch: 1 step: 234, loss is 1.8376539945602417\n",
      "epoch: 1 step: 235, loss is 1.9775210618972778\n",
      "epoch: 1 step: 236, loss is 1.913941740989685\n",
      "epoch: 1 step: 237, loss is 1.9534885883331299\n",
      "epoch: 1 step: 238, loss is 1.9404211044311523\n",
      "epoch: 1 step: 239, loss is 1.929660677909851\n",
      "epoch: 1 step: 240, loss is 1.9186817407608032\n",
      "epoch: 1 step: 241, loss is 1.9124469757080078\n",
      "epoch: 1 step: 242, loss is 2.034776449203491\n",
      "epoch: 1 step: 243, loss is 1.9305436611175537\n",
      "epoch: 1 step: 244, loss is 1.887369990348816\n",
      "epoch: 1 step: 245, loss is 1.951008915901184\n",
      "epoch: 1 step: 246, loss is 1.941895604133606\n",
      "epoch: 1 step: 247, loss is 1.9071109294891357\n",
      "epoch: 1 step: 248, loss is 2.0668718814849854\n",
      "epoch: 1 step: 249, loss is 2.000673770904541\n",
      "epoch: 1 step: 250, loss is 1.9765684604644775\n",
      "epoch: 1 step: 251, loss is 1.9949495792388916\n",
      "epoch: 1 step: 252, loss is 1.950356364250183\n",
      "epoch: 1 step: 253, loss is 1.906728744506836\n",
      "epoch: 1 step: 254, loss is 1.889809250831604\n",
      "epoch: 1 step: 255, loss is 1.929376244544983\n",
      "epoch: 1 step: 256, loss is 1.9321508407592773\n",
      "epoch: 1 step: 257, loss is 1.8858799934387207\n",
      "epoch: 1 step: 258, loss is 1.9469196796417236\n",
      "epoch: 1 step: 259, loss is 1.8795547485351562\n",
      "epoch: 1 step: 260, loss is 1.8760193586349487\n",
      "epoch: 1 step: 261, loss is 1.8270217180252075\n",
      "epoch: 1 step: 262, loss is 1.9137431383132935\n",
      "epoch: 1 step: 263, loss is 1.838327407836914\n",
      "epoch: 1 step: 264, loss is 1.93601393699646\n",
      "epoch: 1 step: 265, loss is 1.9867279529571533\n",
      "epoch: 1 step: 266, loss is 1.926814317703247\n",
      "epoch: 1 step: 267, loss is 1.840099811553955\n",
      "epoch: 1 step: 268, loss is 1.876579999923706\n",
      "epoch: 1 step: 269, loss is 1.9250459671020508\n",
      "epoch: 1 step: 270, loss is 1.788459062576294\n",
      "epoch: 1 step: 271, loss is 1.860358715057373\n",
      "epoch: 1 step: 272, loss is 1.803744912147522\n",
      "epoch: 1 step: 273, loss is 1.9307785034179688\n",
      "epoch: 1 step: 274, loss is 1.9009946584701538\n",
      "epoch: 1 step: 275, loss is 1.95005202293396\n",
      "epoch: 1 step: 276, loss is 1.8743079900741577\n",
      "epoch: 1 step: 277, loss is 1.8509714603424072\n",
      "epoch: 1 step: 278, loss is 1.9201388359069824\n",
      "epoch: 1 step: 279, loss is 1.9963434934616089\n",
      "epoch: 1 step: 280, loss is 1.8501687049865723\n",
      "epoch: 1 step: 281, loss is 1.9621950387954712\n",
      "epoch: 1 step: 282, loss is 1.8723030090332031\n",
      "epoch: 1 step: 283, loss is 1.9040448665618896\n",
      "epoch: 1 step: 284, loss is 2.0268852710723877\n",
      "epoch: 1 step: 285, loss is 2.0206267833709717\n",
      "epoch: 1 step: 286, loss is 1.9647547006607056\n",
      "epoch: 1 step: 287, loss is 1.8911473751068115\n",
      "epoch: 1 step: 288, loss is 2.011986494064331\n",
      "epoch: 1 step: 289, loss is 1.953148603439331\n",
      "epoch: 1 step: 290, loss is 1.918166995048523\n",
      "epoch: 1 step: 291, loss is 1.9224646091461182\n",
      "epoch: 1 step: 292, loss is 1.8459067344665527\n",
      "epoch: 1 step: 293, loss is 1.8538532257080078\n",
      "epoch: 1 step: 294, loss is 1.867178201675415\n",
      "epoch: 1 step: 295, loss is 1.9155274629592896\n",
      "epoch: 1 step: 296, loss is 1.8865580558776855\n",
      "epoch: 1 step: 297, loss is 2.0104308128356934\n",
      "epoch: 1 step: 298, loss is 1.8734208345413208\n",
      "epoch: 1 step: 299, loss is 1.9480352401733398\n",
      "epoch: 1 step: 300, loss is 1.8880243301391602\n",
      "epoch: 1 step: 301, loss is 1.9902675151824951\n",
      "epoch: 1 step: 302, loss is 1.8093417882919312\n",
      "epoch: 1 step: 303, loss is 1.907879114151001\n",
      "epoch: 1 step: 304, loss is 1.79038667678833\n",
      "epoch: 1 step: 305, loss is 1.8786585330963135\n",
      "epoch: 1 step: 306, loss is 1.7830373048782349\n",
      "epoch: 1 step: 307, loss is 1.8315597772598267\n",
      "epoch: 1 step: 308, loss is 1.8300867080688477\n",
      "epoch: 1 step: 309, loss is 1.9650505781173706\n",
      "epoch: 1 step: 310, loss is 1.9045860767364502\n",
      "epoch: 1 step: 311, loss is 1.8913958072662354\n",
      "epoch: 1 step: 312, loss is 1.856520414352417\n",
      "epoch: 1 step: 313, loss is 1.9955934286117554\n",
      "epoch: 1 step: 314, loss is 1.971932291984558\n",
      "epoch: 1 step: 315, loss is 1.828593134880066\n",
      "epoch: 1 step: 316, loss is 1.80549955368042\n",
      "epoch: 1 step: 317, loss is 1.9746370315551758\n",
      "epoch: 1 step: 318, loss is 1.885345697402954\n",
      "epoch: 1 step: 319, loss is 1.9872039556503296\n",
      "epoch: 1 step: 320, loss is 1.8523399829864502\n",
      "epoch: 1 step: 321, loss is 1.8725484609603882\n",
      "epoch: 1 step: 322, loss is 1.8426036834716797\n",
      "epoch: 1 step: 323, loss is 1.8610047101974487\n",
      "epoch: 1 step: 324, loss is 1.9297184944152832\n",
      "epoch: 1 step: 325, loss is 1.9655100107192993\n",
      "epoch: 1 step: 326, loss is 1.8626599311828613\n",
      "epoch: 1 step: 327, loss is 1.8399511575698853\n",
      "epoch: 1 step: 328, loss is 1.7793594598770142\n",
      "epoch: 1 step: 329, loss is 1.9723944664001465\n",
      "epoch: 1 step: 330, loss is 1.8662266731262207\n",
      "epoch: 1 step: 331, loss is 1.9474475383758545\n",
      "epoch: 1 step: 332, loss is 1.8542823791503906\n",
      "epoch: 1 step: 333, loss is 1.8589725494384766\n",
      "epoch: 1 step: 334, loss is 1.8106977939605713\n",
      "epoch: 1 step: 335, loss is 1.818113088607788\n",
      "epoch: 1 step: 336, loss is 1.8041951656341553\n",
      "epoch: 1 step: 337, loss is 1.87131929397583\n",
      "epoch: 1 step: 338, loss is 1.9232635498046875\n",
      "epoch: 1 step: 339, loss is 1.8267995119094849\n",
      "epoch: 1 step: 340, loss is 1.8992289304733276\n",
      "epoch: 1 step: 341, loss is 1.8485268354415894\n",
      "epoch: 1 step: 342, loss is 1.907149314880371\n",
      "epoch: 1 step: 343, loss is 1.8712842464447021\n",
      "epoch: 1 step: 344, loss is 1.8505468368530273\n",
      "epoch: 1 step: 345, loss is 1.6883039474487305\n",
      "epoch: 1 step: 346, loss is 1.8862459659576416\n",
      "epoch: 1 step: 347, loss is 1.876391887664795\n",
      "epoch: 1 step: 348, loss is 1.9384708404541016\n",
      "epoch: 1 step: 349, loss is 1.8445193767547607\n",
      "epoch: 1 step: 350, loss is 1.9239284992218018\n",
      "epoch: 1 step: 351, loss is 1.8106094598770142\n",
      "epoch: 1 step: 352, loss is 1.9707798957824707\n",
      "epoch: 1 step: 353, loss is 1.9329595565795898\n",
      "epoch: 1 step: 354, loss is 1.818453073501587\n",
      "epoch: 1 step: 355, loss is 1.8059395551681519\n",
      "epoch: 1 step: 356, loss is 1.7735846042633057\n",
      "epoch: 1 step: 357, loss is 2.0155866146087646\n",
      "epoch: 1 step: 358, loss is 1.7715871334075928\n",
      "epoch: 1 step: 359, loss is 1.8406931161880493\n",
      "epoch: 1 step: 360, loss is 1.8743536472320557\n",
      "epoch: 1 step: 361, loss is 1.873831033706665\n",
      "epoch: 1 step: 362, loss is 1.9301378726959229\n",
      "epoch: 1 step: 363, loss is 1.8849780559539795\n",
      "epoch: 1 step: 364, loss is 1.863820195198059\n",
      "epoch: 1 step: 365, loss is 1.8349175453186035\n",
      "epoch: 1 step: 366, loss is 1.9146064519882202\n",
      "epoch: 1 step: 367, loss is 1.8553221225738525\n",
      "epoch: 1 step: 368, loss is 1.8339868783950806\n",
      "epoch: 1 step: 369, loss is 1.9271371364593506\n",
      "epoch: 1 step: 370, loss is 1.811110258102417\n",
      "epoch: 1 step: 371, loss is 1.8655471801757812\n",
      "epoch: 1 step: 372, loss is 1.8916500806808472\n",
      "epoch: 1 step: 373, loss is 1.7277153730392456\n",
      "epoch: 1 step: 374, loss is 1.7608921527862549\n",
      "epoch: 1 step: 375, loss is 1.937756896018982\n",
      "epoch: 1 step: 376, loss is 1.78263258934021\n",
      "epoch: 1 step: 377, loss is 1.773841142654419\n",
      "epoch: 1 step: 378, loss is 1.8060996532440186\n",
      "epoch: 1 step: 379, loss is 1.9382216930389404\n",
      "epoch: 1 step: 380, loss is 1.8700287342071533\n",
      "epoch: 1 step: 381, loss is 1.8229063749313354\n",
      "epoch: 1 step: 382, loss is 1.828370451927185\n",
      "epoch: 1 step: 383, loss is 1.8144601583480835\n",
      "epoch: 1 step: 384, loss is 1.9103831052780151\n",
      "epoch: 1 step: 385, loss is 1.952892541885376\n",
      "epoch: 1 step: 386, loss is 1.821767807006836\n",
      "epoch: 1 step: 387, loss is 1.794567584991455\n",
      "epoch: 1 step: 388, loss is 1.8847589492797852\n",
      "epoch: 1 step: 389, loss is 1.9963340759277344\n",
      "epoch: 1 step: 390, loss is 1.8046464920043945\n",
      "Train epoch time: 344520.913 ms, per step time: 883.387 ms\n",
      "epoch: 2 step: 1, loss is 1.8945525884628296\n",
      "epoch: 2 step: 2, loss is 1.8569152355194092\n",
      "epoch: 2 step: 3, loss is 1.9038139581680298\n",
      "epoch: 2 step: 4, loss is 1.795443058013916\n",
      "epoch: 2 step: 5, loss is 1.8207106590270996\n",
      "epoch: 2 step: 6, loss is 1.775176763534546\n",
      "epoch: 2 step: 7, loss is 1.8117501735687256\n",
      "epoch: 2 step: 8, loss is 1.889425277709961\n",
      "epoch: 2 step: 9, loss is 1.807533860206604\n",
      "epoch: 2 step: 10, loss is 1.841181993484497\n",
      "epoch: 2 step: 11, loss is 1.9062650203704834\n",
      "epoch: 2 step: 12, loss is 1.8253180980682373\n",
      "epoch: 2 step: 13, loss is 1.771891713142395\n",
      "epoch: 2 step: 14, loss is 1.9231603145599365\n",
      "epoch: 2 step: 15, loss is 1.8823935985565186\n",
      "epoch: 2 step: 16, loss is 1.7682228088378906\n",
      "epoch: 2 step: 17, loss is 1.882201910018921\n",
      "epoch: 2 step: 18, loss is 1.9153859615325928\n",
      "epoch: 2 step: 19, loss is 1.8411047458648682\n",
      "epoch: 2 step: 20, loss is 1.8377629518508911\n",
      "epoch: 2 step: 21, loss is 1.865814208984375\n",
      "epoch: 2 step: 22, loss is 1.8010458946228027\n",
      "epoch: 2 step: 23, loss is 1.7961945533752441\n",
      "epoch: 2 step: 24, loss is 1.8622868061065674\n",
      "epoch: 2 step: 25, loss is 1.8333609104156494\n",
      "epoch: 2 step: 26, loss is 1.7787933349609375\n",
      "epoch: 2 step: 27, loss is 1.8181865215301514\n",
      "epoch: 2 step: 28, loss is 1.6805185079574585\n",
      "epoch: 2 step: 29, loss is 1.794920563697815\n",
      "epoch: 2 step: 30, loss is 1.8566044569015503\n",
      "epoch: 2 step: 31, loss is 1.965673804283142\n",
      "epoch: 2 step: 32, loss is 1.8685450553894043\n",
      "epoch: 2 step: 33, loss is 1.8142887353897095\n",
      "epoch: 2 step: 34, loss is 1.821938395500183\n",
      "epoch: 2 step: 35, loss is 1.7734969854354858\n",
      "epoch: 2 step: 36, loss is 1.8036385774612427\n",
      "epoch: 2 step: 37, loss is 1.9588627815246582\n",
      "epoch: 2 step: 38, loss is 1.8880013227462769\n",
      "epoch: 2 step: 39, loss is 1.7455761432647705\n",
      "epoch: 2 step: 40, loss is 1.815040946006775\n",
      "epoch: 2 step: 41, loss is 1.7327849864959717\n",
      "epoch: 2 step: 42, loss is 1.8139694929122925\n",
      "epoch: 2 step: 43, loss is 1.8630287647247314\n",
      "epoch: 2 step: 44, loss is 1.7655448913574219\n",
      "epoch: 2 step: 45, loss is 1.8529824018478394\n",
      "epoch: 2 step: 46, loss is 1.884705662727356\n",
      "epoch: 2 step: 47, loss is 1.8723533153533936\n",
      "epoch: 2 step: 48, loss is 1.881901502609253\n",
      "epoch: 2 step: 49, loss is 1.711344599723816\n",
      "epoch: 2 step: 50, loss is 1.8293408155441284\n",
      "epoch: 2 step: 51, loss is 1.8674213886260986\n",
      "epoch: 2 step: 52, loss is 1.9391252994537354\n",
      "epoch: 2 step: 53, loss is 1.7820360660552979\n",
      "epoch: 2 step: 54, loss is 1.8862563371658325\n",
      "epoch: 2 step: 55, loss is 1.9094069004058838\n",
      "epoch: 2 step: 56, loss is 1.793907880783081\n",
      "epoch: 2 step: 57, loss is 1.817842721939087\n",
      "epoch: 2 step: 58, loss is 2.0142107009887695\n",
      "epoch: 2 step: 59, loss is 1.8427670001983643\n",
      "epoch: 2 step: 60, loss is 1.8146620988845825\n",
      "epoch: 2 step: 61, loss is 1.831465482711792\n",
      "epoch: 2 step: 62, loss is 1.8468817472457886\n",
      "epoch: 2 step: 63, loss is 1.8212990760803223\n",
      "epoch: 2 step: 64, loss is 1.7451083660125732\n",
      "epoch: 2 step: 65, loss is 1.794258952140808\n",
      "epoch: 2 step: 66, loss is 1.820499300956726\n",
      "epoch: 2 step: 67, loss is 1.787029504776001\n",
      "epoch: 2 step: 68, loss is 1.8097143173217773\n",
      "epoch: 2 step: 69, loss is 1.8079169988632202\n",
      "epoch: 2 step: 70, loss is 1.8676941394805908\n",
      "epoch: 2 step: 71, loss is 1.8570928573608398\n",
      "epoch: 2 step: 72, loss is 1.6973531246185303\n",
      "epoch: 2 step: 73, loss is 1.773540735244751\n",
      "epoch: 2 step: 74, loss is 1.77775239944458\n",
      "epoch: 2 step: 75, loss is 1.8068735599517822\n",
      "epoch: 2 step: 76, loss is 1.856125831604004\n",
      "epoch: 2 step: 77, loss is 1.827237844467163\n",
      "epoch: 2 step: 78, loss is 1.8081825971603394\n",
      "epoch: 2 step: 79, loss is 1.7823776006698608\n",
      "epoch: 2 step: 80, loss is 1.7548497915267944\n",
      "epoch: 2 step: 81, loss is 1.8086273670196533\n",
      "epoch: 2 step: 82, loss is 1.7785089015960693\n",
      "epoch: 2 step: 83, loss is 1.6934409141540527\n",
      "epoch: 2 step: 84, loss is 1.8938673734664917\n",
      "epoch: 2 step: 85, loss is 1.8019201755523682\n",
      "epoch: 2 step: 86, loss is 1.8621107339859009\n",
      "epoch: 2 step: 87, loss is 1.8565860986709595\n",
      "epoch: 2 step: 88, loss is 1.8272336721420288\n",
      "epoch: 2 step: 89, loss is 1.6841096878051758\n",
      "epoch: 2 step: 90, loss is 1.8775591850280762\n",
      "epoch: 2 step: 91, loss is 1.8601481914520264\n",
      "epoch: 2 step: 92, loss is 1.8978790044784546\n",
      "epoch: 2 step: 93, loss is 1.8485523462295532\n",
      "epoch: 2 step: 94, loss is 1.827807903289795\n",
      "epoch: 2 step: 95, loss is 1.9146654605865479\n",
      "epoch: 2 step: 96, loss is 1.8136837482452393\n",
      "epoch: 2 step: 97, loss is 1.8710707426071167\n",
      "epoch: 2 step: 98, loss is 1.7521955966949463\n",
      "epoch: 2 step: 99, loss is 1.7648217678070068\n",
      "epoch: 2 step: 100, loss is 1.8753511905670166\n",
      "epoch: 2 step: 101, loss is 1.8809317350387573\n",
      "epoch: 2 step: 102, loss is 1.7396796941757202\n",
      "epoch: 2 step: 103, loss is 1.794123888015747\n",
      "epoch: 2 step: 104, loss is 1.9893277883529663\n",
      "epoch: 2 step: 105, loss is 1.8815124034881592\n",
      "epoch: 2 step: 106, loss is 1.8023463487625122\n",
      "epoch: 2 step: 107, loss is 1.7521333694458008\n",
      "epoch: 2 step: 108, loss is 1.723016381263733\n",
      "epoch: 2 step: 109, loss is 1.8150088787078857\n",
      "epoch: 2 step: 110, loss is 1.8765511512756348\n",
      "epoch: 2 step: 111, loss is 1.7538777589797974\n",
      "epoch: 2 step: 112, loss is 1.735034704208374\n",
      "epoch: 2 step: 113, loss is 1.7693133354187012\n",
      "epoch: 2 step: 114, loss is 1.7853963375091553\n",
      "epoch: 2 step: 115, loss is 1.751426100730896\n",
      "epoch: 2 step: 116, loss is 1.7537691593170166\n",
      "epoch: 2 step: 117, loss is 1.802445888519287\n",
      "epoch: 2 step: 118, loss is 1.8695919513702393\n",
      "epoch: 2 step: 119, loss is 1.9646646976470947\n",
      "epoch: 2 step: 120, loss is 1.865411400794983\n",
      "epoch: 2 step: 121, loss is 1.770955204963684\n",
      "epoch: 2 step: 122, loss is 1.8813735246658325\n",
      "epoch: 2 step: 123, loss is 1.76405668258667\n",
      "epoch: 2 step: 124, loss is 1.9372148513793945\n",
      "epoch: 2 step: 125, loss is 1.9699043035507202\n",
      "epoch: 2 step: 126, loss is 1.8165686130523682\n",
      "epoch: 2 step: 127, loss is 1.7924730777740479\n",
      "epoch: 2 step: 128, loss is 1.8043967485427856\n",
      "epoch: 2 step: 129, loss is 1.823354721069336\n",
      "epoch: 2 step: 130, loss is 1.8309557437896729\n",
      "epoch: 2 step: 131, loss is 1.863976001739502\n",
      "epoch: 2 step: 132, loss is 1.905734658241272\n",
      "epoch: 2 step: 133, loss is 1.8512355089187622\n",
      "epoch: 2 step: 134, loss is 1.8119724988937378\n",
      "epoch: 2 step: 135, loss is 1.7968237400054932\n",
      "epoch: 2 step: 136, loss is 1.7292697429656982\n",
      "epoch: 2 step: 137, loss is 1.8510806560516357\n",
      "epoch: 2 step: 138, loss is 1.7424910068511963\n",
      "epoch: 2 step: 139, loss is 1.7733596563339233\n",
      "epoch: 2 step: 140, loss is 1.8843027353286743\n",
      "epoch: 2 step: 141, loss is 1.8148223161697388\n",
      "epoch: 2 step: 142, loss is 1.7864460945129395\n",
      "epoch: 2 step: 143, loss is 1.8195383548736572\n",
      "epoch: 2 step: 144, loss is 1.8777785301208496\n",
      "epoch: 2 step: 145, loss is 1.8183729648590088\n",
      "epoch: 2 step: 146, loss is 1.6976211071014404\n",
      "epoch: 2 step: 147, loss is 1.876814603805542\n",
      "epoch: 2 step: 148, loss is 1.7496285438537598\n",
      "epoch: 2 step: 149, loss is 1.8535996675491333\n",
      "epoch: 2 step: 150, loss is 1.796952724456787\n",
      "epoch: 2 step: 151, loss is 1.806018590927124\n",
      "epoch: 2 step: 152, loss is 1.7491767406463623\n",
      "epoch: 2 step: 153, loss is 1.8746309280395508\n",
      "epoch: 2 step: 154, loss is 1.9306716918945312\n",
      "epoch: 2 step: 155, loss is 1.8103342056274414\n",
      "epoch: 2 step: 156, loss is 1.7101269960403442\n",
      "epoch: 2 step: 157, loss is 1.7705525159835815\n",
      "epoch: 2 step: 158, loss is 1.8352112770080566\n",
      "epoch: 2 step: 159, loss is 1.8156527280807495\n",
      "epoch: 2 step: 160, loss is 1.717010259628296\n",
      "epoch: 2 step: 161, loss is 1.8012093305587769\n",
      "epoch: 2 step: 162, loss is 1.785902976989746\n",
      "epoch: 2 step: 163, loss is 1.789047360420227\n",
      "epoch: 2 step: 164, loss is 1.8363763093948364\n",
      "epoch: 2 step: 165, loss is 1.7139137983322144\n",
      "epoch: 2 step: 166, loss is 1.756276249885559\n",
      "epoch: 2 step: 167, loss is 1.6996160745620728\n",
      "epoch: 2 step: 168, loss is 1.6734352111816406\n",
      "epoch: 2 step: 169, loss is 1.7432632446289062\n",
      "epoch: 2 step: 170, loss is 1.8326778411865234\n",
      "epoch: 2 step: 171, loss is 1.8653473854064941\n",
      "epoch: 2 step: 172, loss is 1.7676925659179688\n",
      "epoch: 2 step: 173, loss is 1.8686515092849731\n",
      "epoch: 2 step: 174, loss is 1.8718883991241455\n",
      "epoch: 2 step: 175, loss is 1.8808921575546265\n",
      "epoch: 2 step: 176, loss is 1.7595124244689941\n",
      "epoch: 2 step: 177, loss is 1.7545740604400635\n",
      "epoch: 2 step: 178, loss is 1.8411805629730225\n",
      "epoch: 2 step: 179, loss is 1.929649829864502\n",
      "epoch: 2 step: 180, loss is 1.802598237991333\n",
      "epoch: 2 step: 181, loss is 1.7505182027816772\n",
      "epoch: 2 step: 182, loss is 1.7618069648742676\n",
      "epoch: 2 step: 183, loss is 1.837759256362915\n",
      "epoch: 2 step: 184, loss is 1.765385627746582\n",
      "epoch: 2 step: 185, loss is 1.9235482215881348\n",
      "epoch: 2 step: 186, loss is 1.806941032409668\n",
      "epoch: 2 step: 187, loss is 1.7384614944458008\n",
      "epoch: 2 step: 188, loss is 1.9349522590637207\n",
      "epoch: 2 step: 189, loss is 1.7089070081710815\n",
      "epoch: 2 step: 190, loss is 1.7684979438781738\n",
      "epoch: 2 step: 191, loss is 1.8961999416351318\n",
      "epoch: 2 step: 192, loss is 1.7195935249328613\n",
      "epoch: 2 step: 193, loss is 1.7508193254470825\n",
      "epoch: 2 step: 194, loss is 1.9236730337142944\n",
      "epoch: 2 step: 195, loss is 1.981024980545044\n",
      "epoch: 2 step: 196, loss is 1.7796822786331177\n",
      "epoch: 2 step: 197, loss is 1.7724997997283936\n",
      "epoch: 2 step: 198, loss is 1.812699556350708\n",
      "epoch: 2 step: 199, loss is 1.9002104997634888\n",
      "epoch: 2 step: 200, loss is 1.8755866289138794\n",
      "epoch: 2 step: 201, loss is 1.7489025592803955\n",
      "epoch: 2 step: 202, loss is 1.6987552642822266\n",
      "epoch: 2 step: 203, loss is 1.8786289691925049\n",
      "epoch: 2 step: 204, loss is 1.6599220037460327\n",
      "epoch: 2 step: 205, loss is 1.7036033868789673\n",
      "epoch: 2 step: 206, loss is 1.8975181579589844\n",
      "epoch: 2 step: 207, loss is 1.8075889348983765\n",
      "epoch: 2 step: 208, loss is 1.8151626586914062\n",
      "epoch: 2 step: 209, loss is 1.7465567588806152\n",
      "epoch: 2 step: 210, loss is 1.7294089794158936\n",
      "epoch: 2 step: 211, loss is 1.772495985031128\n",
      "epoch: 2 step: 212, loss is 1.8171839714050293\n",
      "epoch: 2 step: 213, loss is 1.7786492109298706\n",
      "epoch: 2 step: 214, loss is 1.8117351531982422\n",
      "epoch: 2 step: 215, loss is 1.7659989595413208\n",
      "epoch: 2 step: 216, loss is 1.7702867984771729\n",
      "epoch: 2 step: 217, loss is 1.8774490356445312\n",
      "epoch: 2 step: 218, loss is 1.7160958051681519\n",
      "epoch: 2 step: 219, loss is 1.7751215696334839\n",
      "epoch: 2 step: 220, loss is 1.6456378698349\n",
      "epoch: 2 step: 221, loss is 1.6813347339630127\n",
      "epoch: 2 step: 222, loss is 1.8778666257858276\n",
      "epoch: 2 step: 223, loss is 1.8798909187316895\n",
      "epoch: 2 step: 224, loss is 1.6057026386260986\n",
      "epoch: 2 step: 225, loss is 1.8499743938446045\n",
      "epoch: 2 step: 226, loss is 1.737620234489441\n",
      "epoch: 2 step: 227, loss is 1.8067655563354492\n",
      "epoch: 2 step: 228, loss is 1.8640872240066528\n",
      "epoch: 2 step: 229, loss is 1.7220911979675293\n",
      "epoch: 2 step: 230, loss is 1.8099430799484253\n",
      "epoch: 2 step: 231, loss is 1.8614847660064697\n",
      "epoch: 2 step: 232, loss is 1.7109806537628174\n",
      "epoch: 2 step: 233, loss is 1.8100706338882446\n",
      "epoch: 2 step: 234, loss is 1.734800100326538\n",
      "epoch: 2 step: 235, loss is 1.7895331382751465\n",
      "epoch: 2 step: 236, loss is 1.8122661113739014\n",
      "epoch: 2 step: 237, loss is 2.013300895690918\n",
      "epoch: 2 step: 238, loss is 1.8756444454193115\n",
      "epoch: 2 step: 239, loss is 1.884601354598999\n",
      "epoch: 2 step: 240, loss is 1.786630392074585\n",
      "epoch: 2 step: 241, loss is 1.7151936292648315\n",
      "epoch: 2 step: 242, loss is 1.845477819442749\n",
      "epoch: 2 step: 243, loss is 1.7450082302093506\n",
      "epoch: 2 step: 244, loss is 1.832369327545166\n",
      "epoch: 2 step: 245, loss is 1.6118192672729492\n",
      "epoch: 2 step: 246, loss is 1.7784909009933472\n",
      "epoch: 2 step: 247, loss is 1.7796354293823242\n",
      "epoch: 2 step: 248, loss is 1.7941548824310303\n",
      "epoch: 2 step: 249, loss is 1.7007685899734497\n",
      "epoch: 2 step: 250, loss is 1.709944725036621\n",
      "epoch: 2 step: 251, loss is 1.7577710151672363\n",
      "epoch: 2 step: 252, loss is 1.8962986469268799\n",
      "epoch: 2 step: 253, loss is 1.8715739250183105\n",
      "epoch: 2 step: 254, loss is 1.7273553609848022\n",
      "epoch: 2 step: 255, loss is 1.771601915359497\n",
      "epoch: 2 step: 256, loss is 1.809171199798584\n",
      "epoch: 2 step: 257, loss is 1.7766156196594238\n",
      "epoch: 2 step: 258, loss is 1.7243129014968872\n",
      "epoch: 2 step: 259, loss is 1.8365719318389893\n",
      "epoch: 2 step: 260, loss is 1.8225613832473755\n",
      "epoch: 2 step: 261, loss is 1.7973504066467285\n",
      "epoch: 2 step: 262, loss is 1.8488688468933105\n",
      "epoch: 2 step: 263, loss is 1.840958595275879\n",
      "epoch: 2 step: 264, loss is 1.8383232355117798\n",
      "epoch: 2 step: 265, loss is 1.7713403701782227\n",
      "epoch: 2 step: 266, loss is 1.7488727569580078\n",
      "epoch: 2 step: 267, loss is 1.8938403129577637\n",
      "epoch: 2 step: 268, loss is 1.7161457538604736\n",
      "epoch: 2 step: 269, loss is 1.8334311246871948\n",
      "epoch: 2 step: 270, loss is 1.8253642320632935\n",
      "epoch: 2 step: 271, loss is 1.8019657135009766\n",
      "epoch: 2 step: 272, loss is 1.8969593048095703\n",
      "epoch: 2 step: 273, loss is 1.7905489206314087\n",
      "epoch: 2 step: 274, loss is 1.7243704795837402\n",
      "epoch: 2 step: 275, loss is 1.7332351207733154\n",
      "epoch: 2 step: 276, loss is 1.7963664531707764\n",
      "epoch: 2 step: 277, loss is 1.7327513694763184\n",
      "epoch: 2 step: 278, loss is 1.8921434879302979\n",
      "epoch: 2 step: 279, loss is 1.8590633869171143\n",
      "epoch: 2 step: 280, loss is 1.726840853691101\n",
      "epoch: 2 step: 281, loss is 1.8088352680206299\n",
      "epoch: 2 step: 282, loss is 1.723556399345398\n",
      "epoch: 2 step: 283, loss is 1.702614426612854\n",
      "epoch: 2 step: 284, loss is 1.7671856880187988\n",
      "epoch: 2 step: 285, loss is 1.786613941192627\n",
      "epoch: 2 step: 286, loss is 1.7143785953521729\n",
      "epoch: 2 step: 287, loss is 1.8997410535812378\n",
      "epoch: 2 step: 288, loss is 1.8376439809799194\n",
      "epoch: 2 step: 289, loss is 1.7041194438934326\n",
      "epoch: 2 step: 290, loss is 1.7114979028701782\n",
      "epoch: 2 step: 291, loss is 1.8502823114395142\n",
      "epoch: 2 step: 292, loss is 1.8213032484054565\n",
      "epoch: 2 step: 293, loss is 1.7849597930908203\n",
      "epoch: 2 step: 294, loss is 1.794844150543213\n",
      "epoch: 2 step: 295, loss is 1.7728464603424072\n",
      "epoch: 2 step: 296, loss is 1.8073326349258423\n",
      "epoch: 2 step: 297, loss is 1.7847306728363037\n",
      "epoch: 2 step: 298, loss is 1.8260455131530762\n",
      "epoch: 2 step: 299, loss is 1.687066912651062\n",
      "epoch: 2 step: 300, loss is 1.845051646232605\n",
      "epoch: 2 step: 301, loss is 1.7059284448623657\n",
      "epoch: 2 step: 302, loss is 1.8422201871871948\n",
      "epoch: 2 step: 303, loss is 1.738629937171936\n",
      "epoch: 2 step: 304, loss is 1.738729476928711\n",
      "epoch: 2 step: 305, loss is 1.7295763492584229\n",
      "epoch: 2 step: 306, loss is 1.7651633024215698\n",
      "epoch: 2 step: 307, loss is 1.7719955444335938\n",
      "epoch: 2 step: 308, loss is 1.6563937664031982\n",
      "epoch: 2 step: 309, loss is 1.8022732734680176\n",
      "epoch: 2 step: 310, loss is 1.6498041152954102\n",
      "epoch: 2 step: 311, loss is 1.8574634790420532\n",
      "epoch: 2 step: 312, loss is 1.941843032836914\n",
      "epoch: 2 step: 313, loss is 1.744573712348938\n",
      "epoch: 2 step: 314, loss is 1.7406206130981445\n",
      "epoch: 2 step: 315, loss is 1.7198848724365234\n",
      "epoch: 2 step: 316, loss is 1.6953433752059937\n",
      "epoch: 2 step: 317, loss is 1.6808401346206665\n",
      "epoch: 2 step: 318, loss is 1.772491455078125\n",
      "epoch: 2 step: 319, loss is 1.798353672027588\n",
      "epoch: 2 step: 320, loss is 1.8839746713638306\n",
      "epoch: 2 step: 321, loss is 1.764935851097107\n",
      "epoch: 2 step: 322, loss is 1.7735204696655273\n",
      "epoch: 2 step: 323, loss is 1.8297133445739746\n",
      "epoch: 2 step: 324, loss is 1.7353029251098633\n",
      "epoch: 2 step: 325, loss is 1.7961034774780273\n",
      "epoch: 2 step: 326, loss is 1.7544457912445068\n",
      "epoch: 2 step: 327, loss is 1.8094202280044556\n",
      "epoch: 2 step: 328, loss is 1.7533464431762695\n",
      "epoch: 2 step: 329, loss is 1.7003331184387207\n",
      "epoch: 2 step: 330, loss is 1.7196769714355469\n",
      "epoch: 2 step: 331, loss is 1.879317283630371\n",
      "epoch: 2 step: 332, loss is 1.7524715662002563\n",
      "epoch: 2 step: 333, loss is 1.8638522624969482\n",
      "epoch: 2 step: 334, loss is 1.8962103128433228\n",
      "epoch: 2 step: 335, loss is 1.861535906791687\n",
      "epoch: 2 step: 336, loss is 1.8170044422149658\n",
      "epoch: 2 step: 337, loss is 1.7068583965301514\n",
      "epoch: 2 step: 338, loss is 1.8157427310943604\n",
      "epoch: 2 step: 339, loss is 1.8754661083221436\n",
      "epoch: 2 step: 340, loss is 1.8371095657348633\n",
      "epoch: 2 step: 341, loss is 1.726752519607544\n",
      "epoch: 2 step: 342, loss is 1.6313923597335815\n",
      "epoch: 2 step: 343, loss is 1.8192588090896606\n",
      "epoch: 2 step: 344, loss is 1.8241643905639648\n",
      "epoch: 2 step: 345, loss is 1.8795254230499268\n",
      "epoch: 2 step: 346, loss is 1.7098431587219238\n",
      "epoch: 2 step: 347, loss is 1.7351210117340088\n",
      "epoch: 2 step: 348, loss is 1.8333569765090942\n",
      "epoch: 2 step: 349, loss is 1.8830465078353882\n",
      "epoch: 2 step: 350, loss is 1.6975725889205933\n",
      "epoch: 2 step: 351, loss is 1.7478020191192627\n",
      "epoch: 2 step: 352, loss is 1.6892606019973755\n",
      "epoch: 2 step: 353, loss is 1.8383458852767944\n",
      "epoch: 2 step: 354, loss is 1.712646722793579\n",
      "epoch: 2 step: 355, loss is 1.7658872604370117\n",
      "epoch: 2 step: 356, loss is 1.676194190979004\n",
      "epoch: 2 step: 357, loss is 1.7460318803787231\n",
      "epoch: 2 step: 358, loss is 1.6801637411117554\n",
      "epoch: 2 step: 359, loss is 1.7887636423110962\n",
      "epoch: 2 step: 360, loss is 1.8446871042251587\n",
      "epoch: 2 step: 361, loss is 1.8518426418304443\n",
      "epoch: 2 step: 362, loss is 1.7850022315979004\n",
      "epoch: 2 step: 363, loss is 1.7013213634490967\n",
      "epoch: 2 step: 364, loss is 1.819025993347168\n",
      "epoch: 2 step: 365, loss is 1.722090721130371\n",
      "epoch: 2 step: 366, loss is 1.7995755672454834\n",
      "epoch: 2 step: 367, loss is 1.7689719200134277\n",
      "epoch: 2 step: 368, loss is 1.708316683769226\n",
      "epoch: 2 step: 369, loss is 1.829291820526123\n",
      "epoch: 2 step: 370, loss is 1.718804955482483\n",
      "epoch: 2 step: 371, loss is 1.689685344696045\n",
      "epoch: 2 step: 372, loss is 1.6916160583496094\n",
      "epoch: 2 step: 373, loss is 1.76424241065979\n",
      "epoch: 2 step: 374, loss is 1.782180905342102\n",
      "epoch: 2 step: 375, loss is 1.6751296520233154\n",
      "epoch: 2 step: 376, loss is 1.6689401865005493\n",
      "epoch: 2 step: 377, loss is 1.8441873788833618\n",
      "epoch: 2 step: 378, loss is 1.7211354970932007\n",
      "epoch: 2 step: 379, loss is 1.791574239730835\n",
      "epoch: 2 step: 380, loss is 1.694443702697754\n",
      "epoch: 2 step: 381, loss is 1.7762110233306885\n",
      "epoch: 2 step: 382, loss is 1.8468267917633057\n",
      "epoch: 2 step: 383, loss is 1.7518916130065918\n",
      "epoch: 2 step: 384, loss is 1.8045787811279297\n",
      "epoch: 2 step: 385, loss is 1.788115382194519\n",
      "epoch: 2 step: 386, loss is 1.6811193227767944\n",
      "epoch: 2 step: 387, loss is 1.9048511981964111\n",
      "epoch: 2 step: 388, loss is 1.7419694662094116\n",
      "epoch: 2 step: 389, loss is 1.7608060836791992\n",
      "epoch: 2 step: 390, loss is 1.9117628335952759\n",
      "Train epoch time: 155091.745 ms, per step time: 397.671 ms\n",
      "epoch: 3 step: 1, loss is 1.8041187524795532\n",
      "epoch: 3 step: 2, loss is 1.6821457147598267\n",
      "epoch: 3 step: 3, loss is 1.8518505096435547\n",
      "epoch: 3 step: 4, loss is 1.6637108325958252\n",
      "epoch: 3 step: 5, loss is 1.8331453800201416\n",
      "epoch: 3 step: 6, loss is 1.795602560043335\n",
      "epoch: 3 step: 7, loss is 1.6499547958374023\n",
      "epoch: 3 step: 8, loss is 1.7542872428894043\n",
      "epoch: 3 step: 9, loss is 1.8084278106689453\n",
      "epoch: 3 step: 10, loss is 1.8102600574493408\n",
      "epoch: 3 step: 11, loss is 1.8899677991867065\n",
      "epoch: 3 step: 12, loss is 1.7119872570037842\n",
      "epoch: 3 step: 13, loss is 1.7242140769958496\n",
      "epoch: 3 step: 14, loss is 1.7088037729263306\n",
      "epoch: 3 step: 15, loss is 1.7943447828292847\n",
      "epoch: 3 step: 16, loss is 1.8693287372589111\n",
      "epoch: 3 step: 17, loss is 1.8311870098114014\n",
      "epoch: 3 step: 18, loss is 1.6961530447006226\n",
      "epoch: 3 step: 19, loss is 1.7468407154083252\n",
      "epoch: 3 step: 20, loss is 1.7191359996795654\n",
      "epoch: 3 step: 21, loss is 1.909653902053833\n",
      "epoch: 3 step: 22, loss is 1.6911823749542236\n",
      "epoch: 3 step: 23, loss is 1.7507864236831665\n",
      "epoch: 3 step: 24, loss is 1.781396508216858\n",
      "epoch: 3 step: 25, loss is 1.857836127281189\n",
      "epoch: 3 step: 26, loss is 1.8132880926132202\n",
      "epoch: 3 step: 27, loss is 1.7361056804656982\n",
      "epoch: 3 step: 28, loss is 1.719710350036621\n",
      "epoch: 3 step: 29, loss is 1.5791881084442139\n",
      "epoch: 3 step: 30, loss is 1.775634527206421\n",
      "epoch: 3 step: 31, loss is 1.7295968532562256\n",
      "epoch: 3 step: 32, loss is 1.6920909881591797\n",
      "epoch: 3 step: 33, loss is 1.7254393100738525\n",
      "epoch: 3 step: 34, loss is 1.765946865081787\n",
      "epoch: 3 step: 35, loss is 1.7695269584655762\n",
      "epoch: 3 step: 36, loss is 1.6318049430847168\n",
      "epoch: 3 step: 37, loss is 1.6407890319824219\n",
      "epoch: 3 step: 38, loss is 1.732858657836914\n",
      "epoch: 3 step: 39, loss is 1.6839852333068848\n",
      "epoch: 3 step: 40, loss is 1.6842708587646484\n",
      "epoch: 3 step: 41, loss is 1.8513081073760986\n",
      "epoch: 3 step: 42, loss is 1.7279647588729858\n",
      "epoch: 3 step: 43, loss is 1.6437108516693115\n",
      "epoch: 3 step: 44, loss is 1.6929361820220947\n",
      "epoch: 3 step: 45, loss is 1.89357328414917\n",
      "epoch: 3 step: 46, loss is 1.6639975309371948\n",
      "epoch: 3 step: 47, loss is 1.8474705219268799\n",
      "epoch: 3 step: 48, loss is 1.6972320079803467\n",
      "epoch: 3 step: 49, loss is 1.6734051704406738\n",
      "epoch: 3 step: 50, loss is 1.6439549922943115\n",
      "epoch: 3 step: 51, loss is 1.7483515739440918\n",
      "epoch: 3 step: 52, loss is 1.791205883026123\n",
      "epoch: 3 step: 53, loss is 1.8148789405822754\n",
      "epoch: 3 step: 54, loss is 1.7839992046356201\n",
      "epoch: 3 step: 55, loss is 1.7768974304199219\n",
      "epoch: 3 step: 56, loss is 1.6432812213897705\n",
      "epoch: 3 step: 57, loss is 1.7069675922393799\n",
      "epoch: 3 step: 58, loss is 1.6965492963790894\n",
      "epoch: 3 step: 59, loss is 1.7591698169708252\n",
      "epoch: 3 step: 60, loss is 1.7662901878356934\n",
      "epoch: 3 step: 61, loss is 1.8772209882736206\n",
      "epoch: 3 step: 62, loss is 1.7739598751068115\n",
      "epoch: 3 step: 63, loss is 1.8451894521713257\n",
      "epoch: 3 step: 64, loss is 1.7157039642333984\n",
      "epoch: 3 step: 65, loss is 1.7897367477416992\n",
      "epoch: 3 step: 66, loss is 1.7822284698486328\n",
      "epoch: 3 step: 67, loss is 1.7247874736785889\n",
      "epoch: 3 step: 68, loss is 1.7945055961608887\n",
      "epoch: 3 step: 69, loss is 1.7046828269958496\n",
      "epoch: 3 step: 70, loss is 1.7168217897415161\n",
      "epoch: 3 step: 71, loss is 1.6723493337631226\n",
      "epoch: 3 step: 72, loss is 1.7603579759597778\n",
      "epoch: 3 step: 73, loss is 1.673022985458374\n",
      "epoch: 3 step: 74, loss is 1.7513794898986816\n",
      "epoch: 3 step: 75, loss is 1.748082160949707\n",
      "epoch: 3 step: 76, loss is 1.6093227863311768\n",
      "epoch: 3 step: 77, loss is 1.5542151927947998\n",
      "epoch: 3 step: 78, loss is 1.6499236822128296\n",
      "epoch: 3 step: 79, loss is 1.7284990549087524\n",
      "epoch: 3 step: 80, loss is 1.801774501800537\n",
      "epoch: 3 step: 81, loss is 1.7758820056915283\n",
      "epoch: 3 step: 82, loss is 1.6527438163757324\n",
      "epoch: 3 step: 83, loss is 1.6773872375488281\n",
      "epoch: 3 step: 84, loss is 1.7841553688049316\n",
      "epoch: 3 step: 85, loss is 1.5956951379776\n",
      "epoch: 3 step: 86, loss is 1.7226147651672363\n",
      "epoch: 3 step: 87, loss is 1.769914150238037\n",
      "epoch: 3 step: 88, loss is 1.7776628732681274\n",
      "epoch: 3 step: 89, loss is 1.6649631261825562\n",
      "epoch: 3 step: 90, loss is 1.6352028846740723\n",
      "epoch: 3 step: 91, loss is 1.849703311920166\n",
      "epoch: 3 step: 92, loss is 1.7590945959091187\n",
      "epoch: 3 step: 93, loss is 1.8629975318908691\n",
      "epoch: 3 step: 94, loss is 1.6942453384399414\n",
      "epoch: 3 step: 95, loss is 1.7084589004516602\n",
      "epoch: 3 step: 96, loss is 1.719478726387024\n",
      "epoch: 3 step: 97, loss is 1.6647242307662964\n",
      "epoch: 3 step: 98, loss is 1.7109454870224\n",
      "epoch: 3 step: 99, loss is 1.846712589263916\n",
      "epoch: 3 step: 100, loss is 1.8539937734603882\n",
      "epoch: 3 step: 101, loss is 1.6484992504119873\n",
      "epoch: 3 step: 102, loss is 1.7145891189575195\n",
      "epoch: 3 step: 103, loss is 1.7289820909500122\n",
      "epoch: 3 step: 104, loss is 1.8431551456451416\n",
      "epoch: 3 step: 105, loss is 1.7214335203170776\n",
      "epoch: 3 step: 106, loss is 1.7246097326278687\n",
      "epoch: 3 step: 107, loss is 1.7756640911102295\n",
      "epoch: 3 step: 108, loss is 1.6865336894989014\n",
      "epoch: 3 step: 109, loss is 1.873664140701294\n",
      "epoch: 3 step: 110, loss is 1.661585807800293\n",
      "epoch: 3 step: 111, loss is 1.693084716796875\n",
      "epoch: 3 step: 112, loss is 1.7606048583984375\n",
      "epoch: 3 step: 113, loss is 1.7829015254974365\n",
      "epoch: 3 step: 114, loss is 1.600665807723999\n",
      "epoch: 3 step: 115, loss is 1.7568230628967285\n",
      "epoch: 3 step: 116, loss is 1.6375051736831665\n",
      "epoch: 3 step: 117, loss is 1.7204151153564453\n",
      "epoch: 3 step: 118, loss is 1.7628724575042725\n",
      "epoch: 3 step: 119, loss is 1.6842751502990723\n",
      "epoch: 3 step: 120, loss is 1.6538523435592651\n",
      "epoch: 3 step: 121, loss is 1.7021969556808472\n",
      "epoch: 3 step: 122, loss is 1.7649751901626587\n",
      "epoch: 3 step: 123, loss is 1.6648082733154297\n",
      "epoch: 3 step: 124, loss is 1.6320574283599854\n",
      "epoch: 3 step: 125, loss is 1.6374001502990723\n",
      "epoch: 3 step: 126, loss is 1.8154065608978271\n",
      "epoch: 3 step: 127, loss is 1.6139285564422607\n",
      "epoch: 3 step: 128, loss is 1.7115719318389893\n",
      "epoch: 3 step: 129, loss is 1.679856300354004\n",
      "epoch: 3 step: 130, loss is 1.768909215927124\n",
      "epoch: 3 step: 131, loss is 1.6869661808013916\n",
      "epoch: 3 step: 132, loss is 1.8088666200637817\n",
      "epoch: 3 step: 133, loss is 1.6870317459106445\n",
      "epoch: 3 step: 134, loss is 1.7048790454864502\n",
      "epoch: 3 step: 135, loss is 1.6838421821594238\n",
      "epoch: 3 step: 136, loss is 1.6491299867630005\n",
      "epoch: 3 step: 137, loss is 1.6815462112426758\n",
      "epoch: 3 step: 138, loss is 1.6483099460601807\n",
      "epoch: 3 step: 139, loss is 1.8282232284545898\n",
      "epoch: 3 step: 140, loss is 1.7713768482208252\n",
      "epoch: 3 step: 141, loss is 1.6989705562591553\n",
      "epoch: 3 step: 142, loss is 1.7974036931991577\n",
      "epoch: 3 step: 143, loss is 1.7138376235961914\n",
      "epoch: 3 step: 144, loss is 1.6901443004608154\n",
      "epoch: 3 step: 145, loss is 1.9093880653381348\n",
      "epoch: 3 step: 146, loss is 1.658223032951355\n",
      "epoch: 3 step: 147, loss is 1.7649500370025635\n",
      "epoch: 3 step: 148, loss is 1.7925554513931274\n",
      "epoch: 3 step: 149, loss is 1.5729247331619263\n",
      "epoch: 3 step: 150, loss is 1.7889046669006348\n",
      "epoch: 3 step: 151, loss is 1.7209911346435547\n",
      "epoch: 3 step: 152, loss is 1.8672127723693848\n",
      "epoch: 3 step: 153, loss is 1.8382447957992554\n",
      "epoch: 3 step: 154, loss is 1.7192919254302979\n",
      "epoch: 3 step: 155, loss is 1.7559906244277954\n",
      "epoch: 3 step: 156, loss is 1.5874978303909302\n",
      "epoch: 3 step: 157, loss is 1.7879860401153564\n",
      "epoch: 3 step: 158, loss is 1.6673849821090698\n",
      "epoch: 3 step: 159, loss is 1.7046711444854736\n",
      "epoch: 3 step: 160, loss is 1.7646697759628296\n",
      "epoch: 3 step: 161, loss is 1.6352314949035645\n",
      "epoch: 3 step: 162, loss is 1.7171915769577026\n",
      "epoch: 3 step: 163, loss is 1.7495574951171875\n",
      "epoch: 3 step: 164, loss is 1.6446654796600342\n",
      "epoch: 3 step: 165, loss is 1.6533331871032715\n",
      "epoch: 3 step: 166, loss is 1.6620293855667114\n",
      "epoch: 3 step: 167, loss is 1.6554538011550903\n",
      "epoch: 3 step: 168, loss is 1.6005606651306152\n",
      "epoch: 3 step: 169, loss is 1.7313356399536133\n",
      "epoch: 3 step: 170, loss is 1.764371395111084\n",
      "epoch: 3 step: 171, loss is 1.7782981395721436\n",
      "epoch: 3 step: 172, loss is 1.6503123044967651\n",
      "epoch: 3 step: 173, loss is 1.791553020477295\n",
      "epoch: 3 step: 174, loss is 1.633089542388916\n",
      "epoch: 3 step: 175, loss is 1.621227502822876\n",
      "epoch: 3 step: 176, loss is 1.6715974807739258\n",
      "epoch: 3 step: 177, loss is 1.6420929431915283\n",
      "epoch: 3 step: 178, loss is 1.6717944145202637\n",
      "epoch: 3 step: 179, loss is 1.7855398654937744\n",
      "epoch: 3 step: 180, loss is 1.8057538270950317\n",
      "epoch: 3 step: 181, loss is 1.7236912250518799\n",
      "epoch: 3 step: 182, loss is 1.6936862468719482\n",
      "epoch: 3 step: 183, loss is 1.7778359651565552\n",
      "epoch: 3 step: 184, loss is 1.7933632135391235\n",
      "epoch: 3 step: 185, loss is 1.687056541442871\n",
      "epoch: 3 step: 186, loss is 1.77288818359375\n",
      "epoch: 3 step: 187, loss is 1.7109907865524292\n",
      "epoch: 3 step: 188, loss is 1.6759185791015625\n",
      "epoch: 3 step: 189, loss is 1.640801191329956\n",
      "epoch: 3 step: 190, loss is 1.6485729217529297\n",
      "epoch: 3 step: 191, loss is 1.643891453742981\n",
      "epoch: 3 step: 192, loss is 1.7004891633987427\n",
      "epoch: 3 step: 193, loss is 1.6263055801391602\n",
      "epoch: 3 step: 194, loss is 1.5829501152038574\n",
      "epoch: 3 step: 195, loss is 1.7955998182296753\n",
      "epoch: 3 step: 196, loss is 1.7944493293762207\n",
      "epoch: 3 step: 197, loss is 1.560317039489746\n",
      "epoch: 3 step: 198, loss is 1.7445170879364014\n",
      "epoch: 3 step: 199, loss is 1.7665503025054932\n",
      "epoch: 3 step: 200, loss is 1.7129604816436768\n",
      "epoch: 3 step: 201, loss is 1.611220359802246\n",
      "epoch: 3 step: 202, loss is 1.7701786756515503\n",
      "epoch: 3 step: 203, loss is 1.6320850849151611\n",
      "epoch: 3 step: 204, loss is 1.7465507984161377\n",
      "epoch: 3 step: 205, loss is 1.717793345451355\n",
      "epoch: 3 step: 206, loss is 1.6731313467025757\n",
      "epoch: 3 step: 207, loss is 1.6704896688461304\n",
      "epoch: 3 step: 208, loss is 1.7345404624938965\n",
      "epoch: 3 step: 209, loss is 1.718611478805542\n",
      "epoch: 3 step: 210, loss is 1.8436481952667236\n",
      "epoch: 3 step: 211, loss is 1.8119925260543823\n",
      "epoch: 3 step: 212, loss is 1.7504353523254395\n",
      "epoch: 3 step: 213, loss is 1.7289063930511475\n",
      "epoch: 3 step: 214, loss is 1.6494653224945068\n",
      "epoch: 3 step: 215, loss is 1.6812269687652588\n",
      "epoch: 3 step: 216, loss is 1.7302427291870117\n",
      "epoch: 3 step: 217, loss is 1.685023546218872\n",
      "epoch: 3 step: 218, loss is 1.684441328048706\n",
      "epoch: 3 step: 219, loss is 1.736788034439087\n",
      "epoch: 3 step: 220, loss is 1.6329361200332642\n",
      "epoch: 3 step: 221, loss is 1.6915364265441895\n",
      "epoch: 3 step: 222, loss is 1.6514320373535156\n",
      "epoch: 3 step: 223, loss is 1.6674014329910278\n",
      "epoch: 3 step: 224, loss is 1.7268428802490234\n",
      "epoch: 3 step: 225, loss is 1.7014063596725464\n",
      "epoch: 3 step: 226, loss is 1.9366819858551025\n",
      "epoch: 3 step: 227, loss is 1.7203974723815918\n",
      "epoch: 3 step: 228, loss is 1.6429052352905273\n",
      "epoch: 3 step: 229, loss is 1.6453189849853516\n",
      "epoch: 3 step: 230, loss is 1.6570723056793213\n",
      "epoch: 3 step: 231, loss is 1.7360262870788574\n",
      "epoch: 3 step: 232, loss is 1.6237499713897705\n",
      "epoch: 3 step: 233, loss is 1.6786444187164307\n",
      "epoch: 3 step: 234, loss is 1.619027853012085\n",
      "epoch: 3 step: 235, loss is 1.646023154258728\n",
      "epoch: 3 step: 236, loss is 1.684103012084961\n",
      "epoch: 3 step: 237, loss is 1.6904548406600952\n",
      "epoch: 3 step: 238, loss is 1.7243669033050537\n",
      "epoch: 3 step: 239, loss is 1.5772721767425537\n",
      "epoch: 3 step: 240, loss is 1.6544058322906494\n",
      "epoch: 3 step: 241, loss is 1.6129417419433594\n",
      "epoch: 3 step: 242, loss is 1.7080676555633545\n",
      "epoch: 3 step: 243, loss is 1.6127352714538574\n",
      "epoch: 3 step: 244, loss is 1.7869985103607178\n",
      "epoch: 3 step: 245, loss is 1.705806016921997\n",
      "epoch: 3 step: 246, loss is 1.5874465703964233\n",
      "epoch: 3 step: 247, loss is 1.652915358543396\n",
      "epoch: 3 step: 248, loss is 1.7367191314697266\n",
      "epoch: 3 step: 249, loss is 1.717552900314331\n",
      "epoch: 3 step: 250, loss is 1.7771363258361816\n",
      "epoch: 3 step: 251, loss is 1.8194817304611206\n",
      "epoch: 3 step: 252, loss is 1.637866735458374\n",
      "epoch: 3 step: 253, loss is 1.7053582668304443\n",
      "epoch: 3 step: 254, loss is 1.6731737852096558\n",
      "epoch: 3 step: 255, loss is 1.5302538871765137\n",
      "epoch: 3 step: 256, loss is 1.82688307762146\n",
      "epoch: 3 step: 257, loss is 1.6499943733215332\n",
      "epoch: 3 step: 258, loss is 1.883705735206604\n",
      "epoch: 3 step: 259, loss is 1.7993624210357666\n",
      "epoch: 3 step: 260, loss is 1.7085955142974854\n",
      "epoch: 3 step: 261, loss is 1.7244999408721924\n",
      "epoch: 3 step: 262, loss is 1.7225713729858398\n",
      "epoch: 3 step: 263, loss is 1.6364023685455322\n",
      "epoch: 3 step: 264, loss is 1.64637291431427\n",
      "epoch: 3 step: 265, loss is 1.7270725965499878\n",
      "epoch: 3 step: 266, loss is 1.793522834777832\n",
      "epoch: 3 step: 267, loss is 1.6057219505310059\n",
      "epoch: 3 step: 268, loss is 1.7006888389587402\n",
      "epoch: 3 step: 269, loss is 1.547559142112732\n",
      "epoch: 3 step: 270, loss is 1.746313214302063\n",
      "epoch: 3 step: 271, loss is 1.7609832286834717\n",
      "epoch: 3 step: 272, loss is 1.7137649059295654\n",
      "epoch: 3 step: 273, loss is 1.7359225749969482\n",
      "epoch: 3 step: 274, loss is 1.709384799003601\n",
      "epoch: 3 step: 275, loss is 1.710477590560913\n",
      "epoch: 3 step: 276, loss is 1.7855277061462402\n",
      "epoch: 3 step: 277, loss is 1.6583470106124878\n",
      "epoch: 3 step: 278, loss is 1.7514548301696777\n",
      "epoch: 3 step: 279, loss is 1.6175321340560913\n",
      "epoch: 3 step: 280, loss is 1.6960948705673218\n",
      "epoch: 3 step: 281, loss is 1.747652530670166\n",
      "epoch: 3 step: 282, loss is 1.7307085990905762\n",
      "epoch: 3 step: 283, loss is 1.7372740507125854\n",
      "epoch: 3 step: 284, loss is 1.6898164749145508\n",
      "epoch: 3 step: 285, loss is 1.7398252487182617\n",
      "epoch: 3 step: 286, loss is 1.5939226150512695\n",
      "epoch: 3 step: 287, loss is 1.7752735614776611\n",
      "epoch: 3 step: 288, loss is 1.7012168169021606\n",
      "epoch: 3 step: 289, loss is 1.6471842527389526\n",
      "epoch: 3 step: 290, loss is 1.752162218093872\n",
      "epoch: 3 step: 291, loss is 1.714247226715088\n",
      "epoch: 3 step: 292, loss is 1.6521100997924805\n",
      "epoch: 3 step: 293, loss is 1.615269660949707\n",
      "epoch: 3 step: 294, loss is 1.7525135278701782\n",
      "epoch: 3 step: 295, loss is 1.8584809303283691\n",
      "epoch: 3 step: 296, loss is 1.7114235162734985\n",
      "epoch: 3 step: 297, loss is 1.7033754587173462\n",
      "epoch: 3 step: 298, loss is 1.725196361541748\n",
      "epoch: 3 step: 299, loss is 1.5943577289581299\n",
      "epoch: 3 step: 300, loss is 1.886621356010437\n",
      "epoch: 3 step: 301, loss is 1.7195109128952026\n",
      "epoch: 3 step: 302, loss is 1.6882150173187256\n",
      "epoch: 3 step: 303, loss is 1.7555503845214844\n",
      "epoch: 3 step: 304, loss is 1.7295451164245605\n",
      "epoch: 3 step: 305, loss is 1.6903622150421143\n",
      "epoch: 3 step: 306, loss is 1.6945877075195312\n",
      "epoch: 3 step: 307, loss is 1.623642086982727\n",
      "epoch: 3 step: 308, loss is 1.7068103551864624\n",
      "epoch: 3 step: 309, loss is 1.6941462755203247\n",
      "epoch: 3 step: 310, loss is 1.7612473964691162\n",
      "epoch: 3 step: 311, loss is 1.6686365604400635\n",
      "epoch: 3 step: 312, loss is 1.7309536933898926\n",
      "epoch: 3 step: 313, loss is 1.6361316442489624\n",
      "epoch: 3 step: 314, loss is 1.6902602910995483\n",
      "epoch: 3 step: 315, loss is 1.6480906009674072\n",
      "epoch: 3 step: 316, loss is 1.5541130304336548\n",
      "epoch: 3 step: 317, loss is 1.8171864748001099\n",
      "epoch: 3 step: 318, loss is 1.714763879776001\n",
      "epoch: 3 step: 319, loss is 1.7930618524551392\n",
      "epoch: 3 step: 320, loss is 1.66696298122406\n",
      "epoch: 3 step: 321, loss is 1.5879921913146973\n",
      "epoch: 3 step: 322, loss is 1.6165305376052856\n",
      "epoch: 3 step: 323, loss is 1.6062180995941162\n",
      "epoch: 3 step: 324, loss is 1.5308279991149902\n",
      "epoch: 3 step: 325, loss is 1.6424267292022705\n",
      "epoch: 3 step: 326, loss is 1.6934940814971924\n",
      "epoch: 3 step: 327, loss is 1.5939593315124512\n",
      "epoch: 3 step: 328, loss is 1.6814756393432617\n",
      "epoch: 3 step: 329, loss is 1.7503517866134644\n",
      "epoch: 3 step: 330, loss is 1.7303972244262695\n",
      "epoch: 3 step: 331, loss is 1.7075780630111694\n",
      "epoch: 3 step: 332, loss is 1.653115153312683\n",
      "epoch: 3 step: 333, loss is 1.6558903455734253\n",
      "epoch: 3 step: 334, loss is 1.5989410877227783\n",
      "epoch: 3 step: 335, loss is 1.7108469009399414\n",
      "epoch: 3 step: 336, loss is 1.6155492067337036\n",
      "epoch: 3 step: 337, loss is 1.674238920211792\n",
      "epoch: 3 step: 338, loss is 1.6500860452651978\n",
      "epoch: 3 step: 339, loss is 1.6412904262542725\n",
      "epoch: 3 step: 340, loss is 1.722642421722412\n",
      "epoch: 3 step: 341, loss is 1.6958956718444824\n",
      "epoch: 3 step: 342, loss is 1.6764521598815918\n",
      "epoch: 3 step: 343, loss is 1.8474749326705933\n",
      "epoch: 3 step: 344, loss is 1.621074914932251\n",
      "epoch: 3 step: 345, loss is 1.721930742263794\n",
      "epoch: 3 step: 346, loss is 1.7099248170852661\n",
      "epoch: 3 step: 347, loss is 1.675371527671814\n",
      "epoch: 3 step: 348, loss is 1.5884050130844116\n",
      "epoch: 3 step: 349, loss is 1.760193109512329\n",
      "epoch: 3 step: 350, loss is 1.7285938262939453\n",
      "epoch: 3 step: 351, loss is 1.613969326019287\n",
      "epoch: 3 step: 352, loss is 1.7852697372436523\n",
      "epoch: 3 step: 353, loss is 1.720529317855835\n",
      "epoch: 3 step: 354, loss is 1.637528419494629\n",
      "epoch: 3 step: 355, loss is 1.628883719444275\n",
      "epoch: 3 step: 356, loss is 1.6745045185089111\n",
      "epoch: 3 step: 357, loss is 1.5551666021347046\n",
      "epoch: 3 step: 358, loss is 1.6739614009857178\n",
      "epoch: 3 step: 359, loss is 1.7937932014465332\n",
      "epoch: 3 step: 360, loss is 1.800926923751831\n",
      "epoch: 3 step: 361, loss is 1.691750168800354\n",
      "epoch: 3 step: 362, loss is 1.558704137802124\n",
      "epoch: 3 step: 363, loss is 1.5886015892028809\n",
      "epoch: 3 step: 364, loss is 1.7133221626281738\n",
      "epoch: 3 step: 365, loss is 1.6716694831848145\n",
      "epoch: 3 step: 366, loss is 1.71336030960083\n",
      "epoch: 3 step: 367, loss is 1.6455049514770508\n",
      "epoch: 3 step: 368, loss is 1.6687395572662354\n",
      "epoch: 3 step: 369, loss is 1.657429575920105\n",
      "epoch: 3 step: 370, loss is 1.6223580837249756\n",
      "epoch: 3 step: 371, loss is 1.6125116348266602\n",
      "epoch: 3 step: 372, loss is 1.6898837089538574\n",
      "epoch: 3 step: 373, loss is 1.7080491781234741\n",
      "epoch: 3 step: 374, loss is 1.8188735246658325\n",
      "epoch: 3 step: 375, loss is 1.7022194862365723\n",
      "epoch: 3 step: 376, loss is 1.6072618961334229\n",
      "epoch: 3 step: 377, loss is 1.7579196691513062\n",
      "epoch: 3 step: 378, loss is 1.532618522644043\n",
      "epoch: 3 step: 379, loss is 1.5876238346099854\n",
      "epoch: 3 step: 380, loss is 1.5572417974472046\n",
      "epoch: 3 step: 381, loss is 1.6533613204956055\n",
      "epoch: 3 step: 382, loss is 1.68630051612854\n",
      "epoch: 3 step: 383, loss is 1.5749298334121704\n",
      "epoch: 3 step: 384, loss is 1.6608189344406128\n",
      "epoch: 3 step: 385, loss is 1.660757064819336\n",
      "epoch: 3 step: 386, loss is 1.6061325073242188\n",
      "epoch: 3 step: 387, loss is 1.566266417503357\n",
      "epoch: 3 step: 388, loss is 1.6366106271743774\n",
      "epoch: 3 step: 389, loss is 1.6270592212677002\n",
      "epoch: 3 step: 390, loss is 1.7262952327728271\n",
      "Train epoch time: 167911.971 ms, per step time: 430.544 ms\n",
      "epoch: 4 step: 1, loss is 1.5829299688339233\n",
      "epoch: 4 step: 2, loss is 1.5693445205688477\n",
      "epoch: 4 step: 3, loss is 1.5824251174926758\n",
      "epoch: 4 step: 4, loss is 1.5963923931121826\n",
      "epoch: 4 step: 5, loss is 1.6957604885101318\n",
      "epoch: 4 step: 6, loss is 1.7766586542129517\n",
      "epoch: 4 step: 7, loss is 1.6637433767318726\n",
      "epoch: 4 step: 8, loss is 1.6702439785003662\n",
      "epoch: 4 step: 9, loss is 1.6253892183303833\n",
      "epoch: 4 step: 10, loss is 1.6290547847747803\n",
      "epoch: 4 step: 11, loss is 1.672435998916626\n",
      "epoch: 4 step: 12, loss is 1.6319053173065186\n",
      "epoch: 4 step: 13, loss is 1.6681797504425049\n",
      "epoch: 4 step: 14, loss is 1.6832587718963623\n",
      "epoch: 4 step: 15, loss is 1.6377614736557007\n",
      "epoch: 4 step: 16, loss is 1.5284627676010132\n",
      "epoch: 4 step: 17, loss is 1.602254033088684\n",
      "epoch: 4 step: 18, loss is 1.6477086544036865\n",
      "epoch: 4 step: 19, loss is 1.7934701442718506\n",
      "epoch: 4 step: 20, loss is 1.7218852043151855\n",
      "epoch: 4 step: 21, loss is 1.5026096105575562\n",
      "epoch: 4 step: 22, loss is 1.6615244150161743\n",
      "epoch: 4 step: 23, loss is 1.6737785339355469\n",
      "epoch: 4 step: 24, loss is 1.636248230934143\n",
      "epoch: 4 step: 25, loss is 1.6532087326049805\n",
      "epoch: 4 step: 26, loss is 1.6721432209014893\n",
      "epoch: 4 step: 27, loss is 1.651584506034851\n",
      "epoch: 4 step: 28, loss is 1.5126296281814575\n",
      "epoch: 4 step: 29, loss is 1.5947800874710083\n",
      "epoch: 4 step: 30, loss is 1.681195616722107\n",
      "epoch: 4 step: 31, loss is 1.6675529479980469\n",
      "epoch: 4 step: 32, loss is 1.6401782035827637\n",
      "epoch: 4 step: 33, loss is 1.6378740072250366\n",
      "epoch: 4 step: 34, loss is 1.6793100833892822\n",
      "epoch: 4 step: 35, loss is 1.596339464187622\n",
      "epoch: 4 step: 36, loss is 1.6026438474655151\n",
      "epoch: 4 step: 37, loss is 1.874906301498413\n",
      "epoch: 4 step: 38, loss is 1.667198896408081\n",
      "epoch: 4 step: 39, loss is 1.781720757484436\n",
      "epoch: 4 step: 40, loss is 1.5554993152618408\n",
      "epoch: 4 step: 41, loss is 1.5294808149337769\n",
      "epoch: 4 step: 42, loss is 1.6304394006729126\n",
      "epoch: 4 step: 43, loss is 1.5418341159820557\n",
      "epoch: 4 step: 44, loss is 1.7607017755508423\n",
      "epoch: 4 step: 45, loss is 1.6173977851867676\n",
      "epoch: 4 step: 46, loss is 1.7534754276275635\n",
      "epoch: 4 step: 47, loss is 1.5370150804519653\n",
      "epoch: 4 step: 48, loss is 1.5983922481536865\n",
      "epoch: 4 step: 49, loss is 1.6118555068969727\n",
      "epoch: 4 step: 50, loss is 1.7713351249694824\n",
      "epoch: 4 step: 51, loss is 1.6969902515411377\n",
      "epoch: 4 step: 52, loss is 1.583895206451416\n",
      "epoch: 4 step: 53, loss is 1.6778128147125244\n",
      "epoch: 4 step: 54, loss is 1.6911044120788574\n",
      "epoch: 4 step: 55, loss is 1.6254521608352661\n",
      "epoch: 4 step: 56, loss is 1.6175671815872192\n",
      "epoch: 4 step: 57, loss is 1.5637515783309937\n",
      "epoch: 4 step: 58, loss is 1.6386781930923462\n",
      "epoch: 4 step: 59, loss is 1.5619299411773682\n",
      "epoch: 4 step: 60, loss is 1.6050524711608887\n",
      "epoch: 4 step: 61, loss is 1.6135334968566895\n",
      "epoch: 4 step: 62, loss is 1.755538821220398\n",
      "epoch: 4 step: 63, loss is 1.7609714269638062\n",
      "epoch: 4 step: 64, loss is 1.6690949201583862\n",
      "epoch: 4 step: 65, loss is 1.6691219806671143\n",
      "epoch: 4 step: 66, loss is 1.622606635093689\n",
      "epoch: 4 step: 67, loss is 1.6514678001403809\n",
      "epoch: 4 step: 68, loss is 1.72477126121521\n",
      "epoch: 4 step: 69, loss is 1.858774185180664\n",
      "epoch: 4 step: 70, loss is 1.6931428909301758\n",
      "epoch: 4 step: 71, loss is 1.6269776821136475\n",
      "epoch: 4 step: 72, loss is 1.6212255954742432\n",
      "epoch: 4 step: 73, loss is 1.7461669445037842\n",
      "epoch: 4 step: 74, loss is 1.7665090560913086\n",
      "epoch: 4 step: 75, loss is 1.725243330001831\n",
      "epoch: 4 step: 76, loss is 1.6242246627807617\n",
      "epoch: 4 step: 77, loss is 1.602867841720581\n",
      "epoch: 4 step: 78, loss is 1.5933910608291626\n",
      "epoch: 4 step: 79, loss is 1.7560391426086426\n",
      "epoch: 4 step: 80, loss is 1.7514057159423828\n",
      "epoch: 4 step: 81, loss is 1.6101763248443604\n",
      "epoch: 4 step: 82, loss is 1.7222893238067627\n",
      "epoch: 4 step: 83, loss is 1.5314515829086304\n",
      "epoch: 4 step: 84, loss is 1.7064518928527832\n",
      "epoch: 4 step: 85, loss is 1.6648098230361938\n",
      "epoch: 4 step: 86, loss is 1.5837016105651855\n",
      "epoch: 4 step: 87, loss is 1.6222425699234009\n",
      "epoch: 4 step: 88, loss is 1.541229009628296\n",
      "epoch: 4 step: 89, loss is 1.7052843570709229\n",
      "epoch: 4 step: 90, loss is 1.6660864353179932\n",
      "epoch: 4 step: 91, loss is 1.8080928325653076\n",
      "epoch: 4 step: 92, loss is 1.6630170345306396\n",
      "epoch: 4 step: 93, loss is 1.6155906915664673\n",
      "epoch: 4 step: 94, loss is 1.6780176162719727\n",
      "epoch: 4 step: 95, loss is 1.6186597347259521\n",
      "epoch: 4 step: 96, loss is 1.6310924291610718\n",
      "epoch: 4 step: 97, loss is 1.653515100479126\n",
      "epoch: 4 step: 98, loss is 1.6759705543518066\n",
      "epoch: 4 step: 99, loss is 1.6577727794647217\n",
      "epoch: 4 step: 100, loss is 1.6819411516189575\n",
      "epoch: 4 step: 101, loss is 1.6068147420883179\n",
      "epoch: 4 step: 102, loss is 1.5567617416381836\n",
      "epoch: 4 step: 103, loss is 1.6313304901123047\n",
      "epoch: 4 step: 104, loss is 1.695307731628418\n",
      "epoch: 4 step: 105, loss is 1.7208259105682373\n",
      "epoch: 4 step: 106, loss is 1.5389158725738525\n",
      "epoch: 4 step: 107, loss is 1.7145525217056274\n",
      "epoch: 4 step: 108, loss is 1.7132201194763184\n",
      "epoch: 4 step: 109, loss is 1.6299629211425781\n",
      "epoch: 4 step: 110, loss is 1.7154746055603027\n",
      "epoch: 4 step: 111, loss is 1.684906005859375\n",
      "epoch: 4 step: 112, loss is 1.8617663383483887\n",
      "epoch: 4 step: 113, loss is 1.61043119430542\n",
      "epoch: 4 step: 114, loss is 1.6349791288375854\n",
      "epoch: 4 step: 115, loss is 1.6132259368896484\n",
      "epoch: 4 step: 116, loss is 1.5955924987792969\n",
      "epoch: 4 step: 117, loss is 1.6977200508117676\n",
      "epoch: 4 step: 118, loss is 1.6962404251098633\n",
      "epoch: 4 step: 119, loss is 1.6229161024093628\n",
      "epoch: 4 step: 120, loss is 1.578674077987671\n",
      "epoch: 4 step: 121, loss is 1.762335181236267\n",
      "epoch: 4 step: 122, loss is 1.6472244262695312\n",
      "epoch: 4 step: 123, loss is 1.6369438171386719\n",
      "epoch: 4 step: 124, loss is 1.6291934251785278\n",
      "epoch: 4 step: 125, loss is 1.684539794921875\n",
      "epoch: 4 step: 126, loss is 1.7689433097839355\n",
      "epoch: 4 step: 127, loss is 1.7075495719909668\n",
      "epoch: 4 step: 128, loss is 1.7113661766052246\n",
      "epoch: 4 step: 129, loss is 1.806403398513794\n",
      "epoch: 4 step: 130, loss is 1.7136273384094238\n",
      "epoch: 4 step: 131, loss is 1.7909307479858398\n",
      "epoch: 4 step: 132, loss is 1.5844465494155884\n",
      "epoch: 4 step: 133, loss is 1.7024388313293457\n",
      "epoch: 4 step: 134, loss is 1.6056947708129883\n",
      "epoch: 4 step: 135, loss is 1.5530234575271606\n",
      "epoch: 4 step: 136, loss is 1.6717385053634644\n",
      "epoch: 4 step: 137, loss is 1.5605062246322632\n",
      "epoch: 4 step: 138, loss is 1.535179615020752\n",
      "epoch: 4 step: 139, loss is 1.4817845821380615\n",
      "epoch: 4 step: 140, loss is 1.6447482109069824\n",
      "epoch: 4 step: 141, loss is 1.6872224807739258\n",
      "epoch: 4 step: 142, loss is 1.634595274925232\n",
      "epoch: 4 step: 143, loss is 1.698324203491211\n",
      "epoch: 4 step: 144, loss is 1.6966055631637573\n",
      "epoch: 4 step: 145, loss is 1.544918417930603\n",
      "epoch: 4 step: 146, loss is 1.6719825267791748\n",
      "epoch: 4 step: 147, loss is 1.8534560203552246\n",
      "epoch: 4 step: 148, loss is 1.5323069095611572\n",
      "epoch: 4 step: 149, loss is 1.6596399545669556\n",
      "epoch: 4 step: 150, loss is 1.5409574508666992\n",
      "epoch: 4 step: 151, loss is 1.5733013153076172\n",
      "epoch: 4 step: 152, loss is 1.7184078693389893\n",
      "epoch: 4 step: 153, loss is 1.7309341430664062\n",
      "epoch: 4 step: 154, loss is 1.7049087285995483\n",
      "epoch: 4 step: 155, loss is 1.7532085180282593\n",
      "epoch: 4 step: 156, loss is 1.6354608535766602\n",
      "epoch: 4 step: 157, loss is 1.5885239839553833\n",
      "epoch: 4 step: 158, loss is 1.648087501525879\n",
      "epoch: 4 step: 159, loss is 1.4401443004608154\n",
      "epoch: 4 step: 160, loss is 1.68305242061615\n",
      "epoch: 4 step: 161, loss is 1.6925188302993774\n",
      "epoch: 4 step: 162, loss is 1.5900064706802368\n",
      "epoch: 4 step: 163, loss is 1.5573451519012451\n",
      "epoch: 4 step: 164, loss is 1.6827977895736694\n",
      "epoch: 4 step: 165, loss is 1.6730437278747559\n",
      "epoch: 4 step: 166, loss is 1.586284875869751\n",
      "epoch: 4 step: 167, loss is 1.6574667692184448\n",
      "epoch: 4 step: 168, loss is 1.5647363662719727\n",
      "epoch: 4 step: 169, loss is 1.6438543796539307\n",
      "epoch: 4 step: 170, loss is 1.547151803970337\n",
      "epoch: 4 step: 171, loss is 1.6340081691741943\n",
      "epoch: 4 step: 172, loss is 1.7272722721099854\n",
      "epoch: 4 step: 173, loss is 1.7903401851654053\n",
      "epoch: 4 step: 174, loss is 1.6705818176269531\n",
      "epoch: 4 step: 175, loss is 1.6279442310333252\n",
      "epoch: 4 step: 176, loss is 1.5501224994659424\n",
      "epoch: 4 step: 177, loss is 1.5633615255355835\n",
      "epoch: 4 step: 178, loss is 1.6059284210205078\n",
      "epoch: 4 step: 179, loss is 1.601068139076233\n",
      "epoch: 4 step: 180, loss is 1.616374135017395\n",
      "epoch: 4 step: 181, loss is 1.7364256381988525\n",
      "epoch: 4 step: 182, loss is 1.536895513534546\n",
      "epoch: 4 step: 183, loss is 1.593032956123352\n",
      "epoch: 4 step: 184, loss is 1.698512315750122\n",
      "epoch: 4 step: 185, loss is 1.6330678462982178\n",
      "epoch: 4 step: 186, loss is 1.5912775993347168\n",
      "epoch: 4 step: 187, loss is 1.6639299392700195\n",
      "epoch: 4 step: 188, loss is 1.7077727317810059\n",
      "epoch: 4 step: 189, loss is 1.6887027025222778\n",
      "epoch: 4 step: 190, loss is 1.5392605066299438\n",
      "epoch: 4 step: 191, loss is 1.5678379535675049\n",
      "epoch: 4 step: 192, loss is 1.660683035850525\n",
      "epoch: 4 step: 193, loss is 1.6775976419448853\n",
      "epoch: 4 step: 194, loss is 1.6945269107818604\n",
      "epoch: 4 step: 195, loss is 1.7047927379608154\n",
      "epoch: 4 step: 196, loss is 1.681919813156128\n",
      "epoch: 4 step: 197, loss is 1.6582547426223755\n",
      "epoch: 4 step: 198, loss is 1.587843418121338\n",
      "epoch: 4 step: 199, loss is 1.5965137481689453\n",
      "epoch: 4 step: 200, loss is 1.6342859268188477\n",
      "epoch: 4 step: 201, loss is 1.659938931465149\n",
      "epoch: 4 step: 202, loss is 1.5812181234359741\n",
      "epoch: 4 step: 203, loss is 1.6331396102905273\n",
      "epoch: 4 step: 204, loss is 1.5935415029525757\n",
      "epoch: 4 step: 205, loss is 1.6486618518829346\n",
      "epoch: 4 step: 206, loss is 1.5842865705490112\n",
      "epoch: 4 step: 207, loss is 1.6260342597961426\n",
      "epoch: 4 step: 208, loss is 1.624285101890564\n",
      "epoch: 4 step: 209, loss is 1.7654907703399658\n",
      "epoch: 4 step: 210, loss is 1.5233187675476074\n",
      "epoch: 4 step: 211, loss is 1.5586742162704468\n",
      "epoch: 4 step: 212, loss is 1.7189607620239258\n",
      "epoch: 4 step: 213, loss is 1.5936640501022339\n",
      "epoch: 4 step: 214, loss is 1.5922008752822876\n",
      "epoch: 4 step: 215, loss is 1.6850839853286743\n",
      "epoch: 4 step: 216, loss is 1.603017807006836\n",
      "epoch: 4 step: 217, loss is 1.6860337257385254\n",
      "epoch: 4 step: 218, loss is 1.5936999320983887\n",
      "epoch: 4 step: 219, loss is 1.6589868068695068\n",
      "epoch: 4 step: 220, loss is 1.7813372611999512\n",
      "epoch: 4 step: 221, loss is 1.6586273908615112\n",
      "epoch: 4 step: 222, loss is 1.649903416633606\n",
      "epoch: 4 step: 223, loss is 1.5465424060821533\n",
      "epoch: 4 step: 224, loss is 1.6156469583511353\n",
      "epoch: 4 step: 225, loss is 1.7655547857284546\n",
      "epoch: 4 step: 226, loss is 1.5664501190185547\n",
      "epoch: 4 step: 227, loss is 1.6318681240081787\n",
      "epoch: 4 step: 228, loss is 1.8914825916290283\n",
      "epoch: 4 step: 229, loss is 1.6528351306915283\n",
      "epoch: 4 step: 230, loss is 1.732174277305603\n",
      "epoch: 4 step: 231, loss is 1.5698684453964233\n",
      "epoch: 4 step: 232, loss is 1.7182011604309082\n",
      "epoch: 4 step: 233, loss is 1.6266276836395264\n",
      "epoch: 4 step: 234, loss is 1.6507911682128906\n",
      "epoch: 4 step: 235, loss is 1.5411432981491089\n",
      "epoch: 4 step: 236, loss is 1.6983280181884766\n",
      "epoch: 4 step: 237, loss is 1.7054725885391235\n",
      "epoch: 4 step: 238, loss is 1.602392315864563\n",
      "epoch: 4 step: 239, loss is 1.5485563278198242\n",
      "epoch: 4 step: 240, loss is 1.7465356588363647\n",
      "epoch: 4 step: 241, loss is 1.7299978733062744\n",
      "epoch: 4 step: 242, loss is 1.5871987342834473\n",
      "epoch: 4 step: 243, loss is 1.6115849018096924\n",
      "epoch: 4 step: 244, loss is 1.7095119953155518\n",
      "epoch: 4 step: 245, loss is 1.639748215675354\n",
      "epoch: 4 step: 246, loss is 1.6491291522979736\n",
      "epoch: 4 step: 247, loss is 1.6639573574066162\n",
      "epoch: 4 step: 248, loss is 1.5341964960098267\n",
      "epoch: 4 step: 249, loss is 1.4680808782577515\n",
      "epoch: 4 step: 250, loss is 1.6667271852493286\n",
      "epoch: 4 step: 251, loss is 1.5828607082366943\n",
      "epoch: 4 step: 252, loss is 1.823240041732788\n",
      "epoch: 4 step: 253, loss is 1.5923690795898438\n",
      "epoch: 4 step: 254, loss is 1.6090154647827148\n",
      "epoch: 4 step: 255, loss is 1.6296203136444092\n",
      "epoch: 4 step: 256, loss is 1.5912137031555176\n",
      "epoch: 4 step: 257, loss is 1.7031619548797607\n",
      "epoch: 4 step: 258, loss is 1.7012250423431396\n",
      "epoch: 4 step: 259, loss is 1.6589365005493164\n",
      "epoch: 4 step: 260, loss is 1.582275390625\n",
      "epoch: 4 step: 261, loss is 1.593373417854309\n",
      "epoch: 4 step: 262, loss is 1.6797672510147095\n",
      "epoch: 4 step: 263, loss is 1.6216516494750977\n",
      "epoch: 4 step: 264, loss is 1.6320663690567017\n",
      "epoch: 4 step: 265, loss is 1.6905829906463623\n",
      "epoch: 4 step: 266, loss is 1.6113979816436768\n",
      "epoch: 4 step: 267, loss is 1.6020233631134033\n",
      "epoch: 4 step: 268, loss is 1.5987467765808105\n",
      "epoch: 4 step: 269, loss is 1.66866934299469\n",
      "epoch: 4 step: 270, loss is 1.6889519691467285\n",
      "epoch: 4 step: 271, loss is 1.6561753749847412\n",
      "epoch: 4 step: 272, loss is 1.6207703351974487\n",
      "epoch: 4 step: 273, loss is 1.6806178092956543\n",
      "epoch: 4 step: 274, loss is 1.7421997785568237\n",
      "epoch: 4 step: 275, loss is 1.7381541728973389\n",
      "epoch: 4 step: 276, loss is 1.5207467079162598\n",
      "epoch: 4 step: 277, loss is 1.609339714050293\n",
      "epoch: 4 step: 278, loss is 1.6510047912597656\n",
      "epoch: 4 step: 279, loss is 1.6102848052978516\n",
      "epoch: 4 step: 280, loss is 1.6236134767532349\n",
      "epoch: 4 step: 281, loss is 1.6147637367248535\n",
      "epoch: 4 step: 282, loss is 1.703237771987915\n",
      "epoch: 4 step: 283, loss is 1.5800964832305908\n",
      "epoch: 4 step: 284, loss is 1.654229998588562\n",
      "epoch: 4 step: 285, loss is 1.5974348783493042\n",
      "epoch: 4 step: 286, loss is 1.5227885246276855\n",
      "epoch: 4 step: 287, loss is 1.6657015085220337\n",
      "epoch: 4 step: 288, loss is 1.6064530611038208\n",
      "epoch: 4 step: 289, loss is 1.631739616394043\n",
      "epoch: 4 step: 290, loss is 1.70586359500885\n",
      "epoch: 4 step: 291, loss is 1.5884249210357666\n",
      "epoch: 4 step: 292, loss is 1.6504004001617432\n",
      "epoch: 4 step: 293, loss is 1.6181716918945312\n",
      "epoch: 4 step: 294, loss is 1.4467582702636719\n",
      "epoch: 4 step: 295, loss is 1.6283750534057617\n",
      "epoch: 4 step: 296, loss is 1.5621960163116455\n",
      "epoch: 4 step: 297, loss is 1.622222900390625\n",
      "epoch: 4 step: 298, loss is 1.61374831199646\n",
      "epoch: 4 step: 299, loss is 1.764058232307434\n",
      "epoch: 4 step: 300, loss is 1.6054450273513794\n",
      "epoch: 4 step: 301, loss is 1.6863371133804321\n",
      "epoch: 4 step: 302, loss is 1.7176074981689453\n",
      "epoch: 4 step: 303, loss is 1.6106423139572144\n",
      "epoch: 4 step: 304, loss is 1.6369190216064453\n",
      "epoch: 4 step: 305, loss is 1.6072428226470947\n",
      "epoch: 4 step: 306, loss is 1.5659527778625488\n",
      "epoch: 4 step: 307, loss is 1.6721653938293457\n",
      "epoch: 4 step: 308, loss is 1.5650262832641602\n",
      "epoch: 4 step: 309, loss is 1.7235126495361328\n",
      "epoch: 4 step: 310, loss is 1.5545871257781982\n",
      "epoch: 4 step: 311, loss is 1.7141504287719727\n",
      "epoch: 4 step: 312, loss is 1.5261311531066895\n",
      "epoch: 4 step: 313, loss is 1.6585447788238525\n",
      "epoch: 4 step: 314, loss is 1.7442691326141357\n",
      "epoch: 4 step: 315, loss is 1.7445752620697021\n",
      "epoch: 4 step: 316, loss is 1.7204279899597168\n",
      "epoch: 4 step: 317, loss is 1.609500527381897\n",
      "epoch: 4 step: 318, loss is 1.6916319131851196\n",
      "epoch: 4 step: 319, loss is 1.6140573024749756\n",
      "epoch: 4 step: 320, loss is 1.6281723976135254\n",
      "epoch: 4 step: 321, loss is 1.5357807874679565\n",
      "epoch: 4 step: 322, loss is 1.7447744607925415\n",
      "epoch: 4 step: 323, loss is 1.509748101234436\n",
      "epoch: 4 step: 324, loss is 1.5093777179718018\n",
      "epoch: 4 step: 325, loss is 1.5936386585235596\n",
      "epoch: 4 step: 326, loss is 1.536180853843689\n",
      "epoch: 4 step: 327, loss is 1.4817479848861694\n",
      "epoch: 4 step: 328, loss is 1.5301902294158936\n",
      "epoch: 4 step: 329, loss is 1.63462495803833\n",
      "epoch: 4 step: 330, loss is 1.5918354988098145\n",
      "epoch: 4 step: 331, loss is 1.7010464668273926\n",
      "epoch: 4 step: 332, loss is 1.5102592706680298\n",
      "epoch: 4 step: 333, loss is 1.6884500980377197\n",
      "epoch: 4 step: 334, loss is 1.5468496084213257\n",
      "epoch: 4 step: 335, loss is 1.6176331043243408\n",
      "epoch: 4 step: 336, loss is 1.7623285055160522\n",
      "epoch: 4 step: 337, loss is 1.6311419010162354\n",
      "epoch: 4 step: 338, loss is 1.563688039779663\n",
      "epoch: 4 step: 339, loss is 1.6383483409881592\n",
      "epoch: 4 step: 340, loss is 1.7046725749969482\n",
      "epoch: 4 step: 341, loss is 1.5588977336883545\n",
      "epoch: 4 step: 342, loss is 1.5621461868286133\n",
      "epoch: 4 step: 343, loss is 1.5687371492385864\n",
      "epoch: 4 step: 344, loss is 1.5618845224380493\n",
      "epoch: 4 step: 345, loss is 1.7920244932174683\n",
      "epoch: 4 step: 346, loss is 1.6862246990203857\n",
      "epoch: 4 step: 347, loss is 1.5601305961608887\n",
      "epoch: 4 step: 348, loss is 1.6314886808395386\n",
      "epoch: 4 step: 349, loss is 1.6498364210128784\n",
      "epoch: 4 step: 350, loss is 1.766697883605957\n",
      "epoch: 4 step: 351, loss is 1.5979117155075073\n",
      "epoch: 4 step: 352, loss is 1.6079094409942627\n",
      "epoch: 4 step: 353, loss is 1.671204686164856\n",
      "epoch: 4 step: 354, loss is 1.552411675453186\n",
      "epoch: 4 step: 355, loss is 1.787611722946167\n",
      "epoch: 4 step: 356, loss is 1.652950406074524\n",
      "epoch: 4 step: 357, loss is 1.6155451536178589\n",
      "epoch: 4 step: 358, loss is 1.6446219682693481\n",
      "epoch: 4 step: 359, loss is 1.688068151473999\n",
      "epoch: 4 step: 360, loss is 1.58878493309021\n",
      "epoch: 4 step: 361, loss is 1.7054420709609985\n",
      "epoch: 4 step: 362, loss is 1.6636598110198975\n",
      "epoch: 4 step: 363, loss is 1.7006630897521973\n",
      "epoch: 4 step: 364, loss is 1.6021153926849365\n",
      "epoch: 4 step: 365, loss is 1.6692979335784912\n",
      "epoch: 4 step: 366, loss is 1.629596471786499\n",
      "epoch: 4 step: 367, loss is 1.7189619541168213\n",
      "epoch: 4 step: 368, loss is 1.6148046255111694\n",
      "epoch: 4 step: 369, loss is 1.6493756771087646\n",
      "epoch: 4 step: 370, loss is 1.5772145986557007\n",
      "epoch: 4 step: 371, loss is 1.6973352432250977\n",
      "epoch: 4 step: 372, loss is 1.6571273803710938\n",
      "epoch: 4 step: 373, loss is 1.4109066724777222\n",
      "epoch: 4 step: 374, loss is 1.6115119457244873\n",
      "epoch: 4 step: 375, loss is 1.5268816947937012\n",
      "epoch: 4 step: 376, loss is 1.576293706893921\n",
      "epoch: 4 step: 377, loss is 1.5580863952636719\n",
      "epoch: 4 step: 378, loss is 1.5493950843811035\n",
      "epoch: 4 step: 379, loss is 1.5041099786758423\n",
      "epoch: 4 step: 380, loss is 1.579092025756836\n",
      "epoch: 4 step: 381, loss is 1.5620362758636475\n",
      "epoch: 4 step: 382, loss is 1.537705898284912\n",
      "epoch: 4 step: 383, loss is 1.7589390277862549\n",
      "epoch: 4 step: 384, loss is 1.6191767454147339\n",
      "epoch: 4 step: 385, loss is 1.5224347114562988\n",
      "epoch: 4 step: 386, loss is 1.6983803510665894\n",
      "epoch: 4 step: 387, loss is 1.5634644031524658\n",
      "epoch: 4 step: 388, loss is 1.5402841567993164\n",
      "epoch: 4 step: 389, loss is 1.5573701858520508\n",
      "epoch: 4 step: 390, loss is 1.4771547317504883\n",
      "Train epoch time: 154120.190 ms, per step time: 395.180 ms\n",
      "epoch: 5 step: 1, loss is 1.55489981174469\n",
      "epoch: 5 step: 2, loss is 1.6186591386795044\n",
      "epoch: 5 step: 3, loss is 1.4524123668670654\n",
      "epoch: 5 step: 4, loss is 1.530942678451538\n",
      "epoch: 5 step: 5, loss is 1.5542352199554443\n",
      "epoch: 5 step: 6, loss is 1.5420260429382324\n",
      "epoch: 5 step: 7, loss is 1.6115238666534424\n",
      "epoch: 5 step: 8, loss is 1.5270391702651978\n",
      "epoch: 5 step: 9, loss is 1.6089870929718018\n",
      "epoch: 5 step: 10, loss is 1.5543837547302246\n",
      "epoch: 5 step: 11, loss is 1.5891510248184204\n",
      "epoch: 5 step: 12, loss is 1.6037161350250244\n",
      "epoch: 5 step: 13, loss is 1.5928730964660645\n",
      "epoch: 5 step: 14, loss is 1.5584030151367188\n",
      "epoch: 5 step: 15, loss is 1.5587607622146606\n",
      "epoch: 5 step: 16, loss is 1.5861611366271973\n",
      "epoch: 5 step: 17, loss is 1.6396548748016357\n",
      "epoch: 5 step: 18, loss is 1.5108718872070312\n",
      "epoch: 5 step: 19, loss is 1.696102261543274\n",
      "epoch: 5 step: 20, loss is 1.6431739330291748\n",
      "epoch: 5 step: 21, loss is 1.6952271461486816\n",
      "epoch: 5 step: 22, loss is 1.4943596124649048\n",
      "epoch: 5 step: 23, loss is 1.4515728950500488\n",
      "epoch: 5 step: 24, loss is 1.5814507007598877\n",
      "epoch: 5 step: 25, loss is 1.5674161911010742\n",
      "epoch: 5 step: 26, loss is 1.593377947807312\n",
      "epoch: 5 step: 27, loss is 1.5060465335845947\n",
      "epoch: 5 step: 28, loss is 1.5357329845428467\n",
      "epoch: 5 step: 29, loss is 1.6592845916748047\n",
      "epoch: 5 step: 30, loss is 1.5536839962005615\n",
      "epoch: 5 step: 31, loss is 1.6478557586669922\n",
      "epoch: 5 step: 32, loss is 1.5679852962493896\n",
      "epoch: 5 step: 33, loss is 1.5441337823867798\n",
      "epoch: 5 step: 34, loss is 1.5844101905822754\n",
      "epoch: 5 step: 35, loss is 1.6567375659942627\n",
      "epoch: 5 step: 36, loss is 1.6299352645874023\n",
      "epoch: 5 step: 37, loss is 1.6287115812301636\n",
      "epoch: 5 step: 38, loss is 1.6185719966888428\n",
      "epoch: 5 step: 39, loss is 1.5929713249206543\n",
      "epoch: 5 step: 40, loss is 1.5683752298355103\n",
      "epoch: 5 step: 41, loss is 1.484606146812439\n",
      "epoch: 5 step: 42, loss is 1.6268587112426758\n",
      "epoch: 5 step: 43, loss is 1.6249732971191406\n",
      "epoch: 5 step: 44, loss is 1.5212035179138184\n",
      "epoch: 5 step: 45, loss is 1.5764508247375488\n",
      "epoch: 5 step: 46, loss is 1.6732534170150757\n",
      "epoch: 5 step: 47, loss is 1.5817232131958008\n",
      "epoch: 5 step: 48, loss is 1.533501386642456\n",
      "epoch: 5 step: 49, loss is 1.5011398792266846\n",
      "epoch: 5 step: 50, loss is 1.576478362083435\n",
      "epoch: 5 step: 51, loss is 1.617769479751587\n",
      "epoch: 5 step: 52, loss is 1.7040863037109375\n",
      "epoch: 5 step: 53, loss is 1.631706953048706\n",
      "epoch: 5 step: 54, loss is 1.6125715970993042\n",
      "epoch: 5 step: 55, loss is 1.5388073921203613\n",
      "epoch: 5 step: 56, loss is 1.5230032205581665\n",
      "epoch: 5 step: 57, loss is 1.539459466934204\n",
      "epoch: 5 step: 58, loss is 1.6178522109985352\n",
      "epoch: 5 step: 59, loss is 1.5272750854492188\n",
      "epoch: 5 step: 60, loss is 1.6866400241851807\n",
      "epoch: 5 step: 61, loss is 1.523990273475647\n",
      "epoch: 5 step: 62, loss is 1.583640694618225\n",
      "epoch: 5 step: 63, loss is 1.4960353374481201\n",
      "epoch: 5 step: 64, loss is 1.6405665874481201\n",
      "epoch: 5 step: 65, loss is 1.4217936992645264\n",
      "epoch: 5 step: 66, loss is 1.5881519317626953\n",
      "epoch: 5 step: 67, loss is 1.5960378646850586\n",
      "epoch: 5 step: 68, loss is 1.596116304397583\n",
      "epoch: 5 step: 69, loss is 1.6887626647949219\n",
      "epoch: 5 step: 70, loss is 1.5859503746032715\n",
      "epoch: 5 step: 71, loss is 1.6122305393218994\n",
      "epoch: 5 step: 72, loss is 1.6131759881973267\n",
      "epoch: 5 step: 73, loss is 1.6459898948669434\n",
      "epoch: 5 step: 74, loss is 1.5270884037017822\n",
      "epoch: 5 step: 75, loss is 1.4766569137573242\n",
      "epoch: 5 step: 76, loss is 1.6216111183166504\n",
      "epoch: 5 step: 77, loss is 1.6059341430664062\n",
      "epoch: 5 step: 78, loss is 1.57723069190979\n",
      "epoch: 5 step: 79, loss is 1.6515305042266846\n",
      "epoch: 5 step: 80, loss is 1.540886640548706\n",
      "epoch: 5 step: 81, loss is 1.4827969074249268\n",
      "epoch: 5 step: 82, loss is 1.5437556505203247\n",
      "epoch: 5 step: 83, loss is 1.6531968116760254\n",
      "epoch: 5 step: 84, loss is 1.4779186248779297\n",
      "epoch: 5 step: 85, loss is 1.6166597604751587\n",
      "epoch: 5 step: 86, loss is 1.694826602935791\n",
      "epoch: 5 step: 87, loss is 1.4293179512023926\n",
      "epoch: 5 step: 88, loss is 1.5895930528640747\n",
      "epoch: 5 step: 89, loss is 1.6449815034866333\n",
      "epoch: 5 step: 90, loss is 1.5859065055847168\n",
      "epoch: 5 step: 91, loss is 1.5099663734436035\n",
      "epoch: 5 step: 92, loss is 1.4775810241699219\n",
      "epoch: 5 step: 93, loss is 1.696537733078003\n",
      "epoch: 5 step: 94, loss is 1.6258515119552612\n",
      "epoch: 5 step: 95, loss is 1.6183615922927856\n",
      "epoch: 5 step: 96, loss is 1.6686079502105713\n",
      "epoch: 5 step: 97, loss is 1.4857474565505981\n",
      "epoch: 5 step: 98, loss is 1.61214280128479\n",
      "epoch: 5 step: 99, loss is 1.6363483667373657\n",
      "epoch: 5 step: 100, loss is 1.7597359418869019\n",
      "epoch: 5 step: 101, loss is 1.6410131454467773\n",
      "epoch: 5 step: 102, loss is 1.659865379333496\n",
      "epoch: 5 step: 103, loss is 1.6071597337722778\n",
      "epoch: 5 step: 104, loss is 1.7475638389587402\n",
      "epoch: 5 step: 105, loss is 1.5782842636108398\n",
      "epoch: 5 step: 106, loss is 1.670239806175232\n",
      "epoch: 5 step: 107, loss is 1.6388649940490723\n",
      "epoch: 5 step: 108, loss is 1.664285659790039\n",
      "epoch: 5 step: 109, loss is 1.7205458879470825\n",
      "epoch: 5 step: 110, loss is 1.5673367977142334\n",
      "epoch: 5 step: 111, loss is 1.6337332725524902\n",
      "epoch: 5 step: 112, loss is 1.5034061670303345\n",
      "epoch: 5 step: 113, loss is 1.621305227279663\n",
      "epoch: 5 step: 114, loss is 1.6122874021530151\n",
      "epoch: 5 step: 115, loss is 1.5612525939941406\n",
      "epoch: 5 step: 116, loss is 1.6016356945037842\n",
      "epoch: 5 step: 117, loss is 1.5695737600326538\n",
      "epoch: 5 step: 118, loss is 1.6411163806915283\n",
      "epoch: 5 step: 119, loss is 1.4821281433105469\n",
      "epoch: 5 step: 120, loss is 1.6070208549499512\n",
      "epoch: 5 step: 121, loss is 1.550037145614624\n",
      "epoch: 5 step: 122, loss is 1.6029123067855835\n",
      "epoch: 5 step: 123, loss is 1.6480590105056763\n",
      "epoch: 5 step: 124, loss is 1.6453557014465332\n",
      "epoch: 5 step: 125, loss is 1.4768117666244507\n",
      "epoch: 5 step: 126, loss is 1.4869537353515625\n",
      "epoch: 5 step: 127, loss is 1.5715429782867432\n",
      "epoch: 5 step: 128, loss is 1.6567471027374268\n",
      "epoch: 5 step: 129, loss is 1.721691370010376\n",
      "epoch: 5 step: 130, loss is 1.5098938941955566\n",
      "epoch: 5 step: 131, loss is 1.6283531188964844\n",
      "epoch: 5 step: 132, loss is 1.5661402940750122\n",
      "epoch: 5 step: 133, loss is 1.611137866973877\n",
      "epoch: 5 step: 134, loss is 1.5590457916259766\n",
      "epoch: 5 step: 135, loss is 1.6531877517700195\n",
      "epoch: 5 step: 136, loss is 1.549116849899292\n",
      "epoch: 5 step: 137, loss is 1.521770715713501\n",
      "epoch: 5 step: 138, loss is 1.45374596118927\n",
      "epoch: 5 step: 139, loss is 1.6394243240356445\n",
      "epoch: 5 step: 140, loss is 1.5770282745361328\n",
      "epoch: 5 step: 141, loss is 1.611968994140625\n",
      "epoch: 5 step: 142, loss is 1.6431961059570312\n",
      "epoch: 5 step: 143, loss is 1.5618939399719238\n",
      "epoch: 5 step: 144, loss is 1.576810359954834\n",
      "epoch: 5 step: 145, loss is 1.49359130859375\n",
      "epoch: 5 step: 146, loss is 1.5973814725875854\n",
      "epoch: 5 step: 147, loss is 1.6808369159698486\n",
      "epoch: 5 step: 148, loss is 1.5356249809265137\n",
      "epoch: 5 step: 149, loss is 1.632577657699585\n",
      "epoch: 5 step: 150, loss is 1.5300371646881104\n",
      "epoch: 5 step: 151, loss is 1.4797654151916504\n",
      "epoch: 5 step: 152, loss is 1.5336917638778687\n",
      "epoch: 5 step: 153, loss is 1.715250849723816\n",
      "epoch: 5 step: 154, loss is 1.6787410974502563\n",
      "epoch: 5 step: 155, loss is 1.4332622289657593\n",
      "epoch: 5 step: 156, loss is 1.568777084350586\n",
      "epoch: 5 step: 157, loss is 1.600273609161377\n",
      "epoch: 5 step: 158, loss is 1.721707820892334\n",
      "epoch: 5 step: 159, loss is 1.5739078521728516\n",
      "epoch: 5 step: 160, loss is 1.6331738233566284\n",
      "epoch: 5 step: 161, loss is 1.559624433517456\n",
      "epoch: 5 step: 162, loss is 1.5413503646850586\n",
      "epoch: 5 step: 163, loss is 1.4876439571380615\n",
      "epoch: 5 step: 164, loss is 1.5487666130065918\n",
      "epoch: 5 step: 165, loss is 1.6665501594543457\n",
      "epoch: 5 step: 166, loss is 1.661744475364685\n",
      "epoch: 5 step: 167, loss is 1.6257250308990479\n",
      "epoch: 5 step: 168, loss is 1.594691276550293\n",
      "epoch: 5 step: 169, loss is 1.5353808403015137\n",
      "epoch: 5 step: 170, loss is 1.6279922723770142\n",
      "epoch: 5 step: 171, loss is 1.5899595022201538\n",
      "epoch: 5 step: 172, loss is 1.552979826927185\n",
      "epoch: 5 step: 173, loss is 1.5704314708709717\n",
      "epoch: 5 step: 174, loss is 1.6508928537368774\n",
      "epoch: 5 step: 175, loss is 1.5318944454193115\n",
      "epoch: 5 step: 176, loss is 1.639949083328247\n",
      "epoch: 5 step: 177, loss is 1.59335458278656\n",
      "epoch: 5 step: 178, loss is 1.6979154348373413\n",
      "epoch: 5 step: 179, loss is 1.5188778638839722\n",
      "epoch: 5 step: 180, loss is 1.5960217714309692\n",
      "epoch: 5 step: 181, loss is 1.658451795578003\n",
      "epoch: 5 step: 182, loss is 1.6178882122039795\n",
      "epoch: 5 step: 183, loss is 1.5884093046188354\n",
      "epoch: 5 step: 184, loss is 1.596993088722229\n",
      "epoch: 5 step: 185, loss is 1.592862844467163\n",
      "epoch: 5 step: 186, loss is 1.6266838312149048\n",
      "epoch: 5 step: 187, loss is 1.4318382740020752\n",
      "epoch: 5 step: 188, loss is 1.6033856868743896\n",
      "epoch: 5 step: 189, loss is 1.5441882610321045\n",
      "epoch: 5 step: 190, loss is 1.704966425895691\n",
      "epoch: 5 step: 191, loss is 1.5629806518554688\n",
      "epoch: 5 step: 192, loss is 1.5637731552124023\n",
      "epoch: 5 step: 193, loss is 1.4918631315231323\n",
      "epoch: 5 step: 194, loss is 1.7539608478546143\n",
      "epoch: 5 step: 195, loss is 1.6185312271118164\n",
      "epoch: 5 step: 196, loss is 1.534182071685791\n",
      "epoch: 5 step: 197, loss is 1.4929778575897217\n",
      "epoch: 5 step: 198, loss is 1.5535825490951538\n",
      "epoch: 5 step: 199, loss is 1.5183157920837402\n",
      "epoch: 5 step: 200, loss is 1.6753275394439697\n",
      "epoch: 5 step: 201, loss is 1.5516932010650635\n",
      "epoch: 5 step: 202, loss is 1.565264344215393\n",
      "epoch: 5 step: 203, loss is 1.5602996349334717\n",
      "epoch: 5 step: 204, loss is 1.6030555963516235\n",
      "epoch: 5 step: 205, loss is 1.4489166736602783\n",
      "epoch: 5 step: 206, loss is 1.6565074920654297\n",
      "epoch: 5 step: 207, loss is 1.5822193622589111\n",
      "epoch: 5 step: 208, loss is 1.5245294570922852\n",
      "epoch: 5 step: 209, loss is 1.4967293739318848\n",
      "epoch: 5 step: 210, loss is 1.5656166076660156\n",
      "epoch: 5 step: 211, loss is 1.4492418766021729\n",
      "epoch: 5 step: 212, loss is 1.5515522956848145\n",
      "epoch: 5 step: 213, loss is 1.6686230897903442\n",
      "epoch: 5 step: 214, loss is 1.4905998706817627\n",
      "epoch: 5 step: 215, loss is 1.6074508428573608\n",
      "epoch: 5 step: 216, loss is 1.5087645053863525\n",
      "epoch: 5 step: 217, loss is 1.6810450553894043\n",
      "epoch: 5 step: 218, loss is 1.6752065420150757\n",
      "epoch: 5 step: 219, loss is 1.4774295091629028\n",
      "epoch: 5 step: 220, loss is 1.562633991241455\n",
      "epoch: 5 step: 221, loss is 1.658509612083435\n",
      "epoch: 5 step: 222, loss is 1.6374821662902832\n",
      "epoch: 5 step: 223, loss is 1.5885391235351562\n",
      "epoch: 5 step: 224, loss is 1.5405168533325195\n",
      "epoch: 5 step: 225, loss is 1.675573706626892\n",
      "epoch: 5 step: 226, loss is 1.6123617887496948\n",
      "epoch: 5 step: 227, loss is 1.6118996143341064\n",
      "epoch: 5 step: 228, loss is 1.5501693487167358\n",
      "epoch: 5 step: 229, loss is 1.608310341835022\n",
      "epoch: 5 step: 230, loss is 1.45640230178833\n",
      "epoch: 5 step: 231, loss is 1.577791452407837\n",
      "epoch: 5 step: 232, loss is 1.558332085609436\n",
      "epoch: 5 step: 233, loss is 1.4639273881912231\n",
      "epoch: 5 step: 234, loss is 1.616842269897461\n",
      "epoch: 5 step: 235, loss is 1.5418272018432617\n",
      "epoch: 5 step: 236, loss is 1.5589896440505981\n",
      "epoch: 5 step: 237, loss is 1.5591669082641602\n",
      "epoch: 5 step: 238, loss is 1.6342034339904785\n",
      "epoch: 5 step: 239, loss is 1.6340771913528442\n",
      "epoch: 5 step: 240, loss is 1.6700581312179565\n",
      "epoch: 5 step: 241, loss is 1.6719651222229004\n",
      "epoch: 5 step: 242, loss is 1.6272319555282593\n",
      "epoch: 5 step: 243, loss is 1.4764395952224731\n",
      "epoch: 5 step: 244, loss is 1.6490730047225952\n",
      "epoch: 5 step: 245, loss is 1.6149941682815552\n",
      "epoch: 5 step: 246, loss is 1.6493473052978516\n",
      "epoch: 5 step: 247, loss is 1.6394038200378418\n",
      "epoch: 5 step: 248, loss is 1.556216835975647\n",
      "epoch: 5 step: 249, loss is 1.563538908958435\n",
      "epoch: 5 step: 250, loss is 1.4858753681182861\n",
      "epoch: 5 step: 251, loss is 1.703303337097168\n",
      "epoch: 5 step: 252, loss is 1.6179320812225342\n",
      "epoch: 5 step: 253, loss is 1.6248292922973633\n",
      "epoch: 5 step: 254, loss is 1.5577232837677002\n",
      "epoch: 5 step: 255, loss is 1.5683281421661377\n",
      "epoch: 5 step: 256, loss is 1.7048996686935425\n",
      "epoch: 5 step: 257, loss is 1.4635065793991089\n",
      "epoch: 5 step: 258, loss is 1.530427098274231\n",
      "epoch: 5 step: 259, loss is 1.4924559593200684\n",
      "epoch: 5 step: 260, loss is 1.5630654096603394\n",
      "epoch: 5 step: 261, loss is 1.5495367050170898\n",
      "epoch: 5 step: 262, loss is 1.5803210735321045\n",
      "epoch: 5 step: 263, loss is 1.5102927684783936\n",
      "epoch: 5 step: 264, loss is 1.6193883419036865\n",
      "epoch: 5 step: 265, loss is 1.5856653451919556\n",
      "epoch: 5 step: 266, loss is 1.673060655593872\n",
      "epoch: 5 step: 267, loss is 1.5287460088729858\n",
      "epoch: 5 step: 268, loss is 1.5767624378204346\n",
      "epoch: 5 step: 269, loss is 1.524388313293457\n",
      "epoch: 5 step: 270, loss is 1.575049638748169\n",
      "epoch: 5 step: 271, loss is 1.6740589141845703\n",
      "epoch: 5 step: 272, loss is 1.4533535242080688\n",
      "epoch: 5 step: 273, loss is 1.5412497520446777\n",
      "epoch: 5 step: 274, loss is 1.6645145416259766\n",
      "epoch: 5 step: 275, loss is 1.4983799457550049\n",
      "epoch: 5 step: 276, loss is 1.63446843624115\n",
      "epoch: 5 step: 277, loss is 1.6736328601837158\n",
      "epoch: 5 step: 278, loss is 1.63313627243042\n",
      "epoch: 5 step: 279, loss is 1.561011552810669\n",
      "epoch: 5 step: 280, loss is 1.569746494293213\n",
      "epoch: 5 step: 281, loss is 1.5215401649475098\n",
      "epoch: 5 step: 282, loss is 1.658857822418213\n",
      "epoch: 5 step: 283, loss is 1.584457516670227\n",
      "epoch: 5 step: 284, loss is 1.5816457271575928\n",
      "epoch: 5 step: 285, loss is 1.51859450340271\n",
      "epoch: 5 step: 286, loss is 1.686232566833496\n",
      "epoch: 5 step: 287, loss is 1.548433542251587\n",
      "epoch: 5 step: 288, loss is 1.6895747184753418\n",
      "epoch: 5 step: 289, loss is 1.6980128288269043\n",
      "epoch: 5 step: 290, loss is 1.4947118759155273\n",
      "epoch: 5 step: 291, loss is 1.4796066284179688\n",
      "epoch: 5 step: 292, loss is 1.3749942779541016\n",
      "epoch: 5 step: 293, loss is 1.6045349836349487\n",
      "epoch: 5 step: 294, loss is 1.5134693384170532\n",
      "epoch: 5 step: 295, loss is 1.593095064163208\n",
      "epoch: 5 step: 296, loss is 1.568855881690979\n",
      "epoch: 5 step: 297, loss is 1.558824896812439\n",
      "epoch: 5 step: 298, loss is 1.65879487991333\n",
      "epoch: 5 step: 299, loss is 1.5214064121246338\n",
      "epoch: 5 step: 300, loss is 1.427110195159912\n",
      "epoch: 5 step: 301, loss is 1.7033133506774902\n",
      "epoch: 5 step: 302, loss is 1.5483317375183105\n",
      "epoch: 5 step: 303, loss is 1.5821595191955566\n",
      "epoch: 5 step: 304, loss is 1.6160764694213867\n",
      "epoch: 5 step: 305, loss is 1.586634635925293\n",
      "epoch: 5 step: 306, loss is 1.6227514743804932\n",
      "epoch: 5 step: 307, loss is 1.6074209213256836\n",
      "epoch: 5 step: 308, loss is 1.6341460943222046\n",
      "epoch: 5 step: 309, loss is 1.6396832466125488\n",
      "epoch: 5 step: 310, loss is 1.6477820873260498\n",
      "epoch: 5 step: 311, loss is 1.50780189037323\n",
      "epoch: 5 step: 312, loss is 1.5445566177368164\n",
      "epoch: 5 step: 313, loss is 1.8639657497406006\n",
      "epoch: 5 step: 314, loss is 1.5811489820480347\n",
      "epoch: 5 step: 315, loss is 1.5469176769256592\n",
      "epoch: 5 step: 316, loss is 1.5920979976654053\n",
      "epoch: 5 step: 317, loss is 1.6523443460464478\n",
      "epoch: 5 step: 318, loss is 1.588801622390747\n",
      "epoch: 5 step: 319, loss is 1.4611811637878418\n",
      "epoch: 5 step: 320, loss is 1.5759812593460083\n",
      "epoch: 5 step: 321, loss is 1.6801717281341553\n",
      "epoch: 5 step: 322, loss is 1.412517786026001\n",
      "epoch: 5 step: 323, loss is 1.4733649492263794\n",
      "epoch: 5 step: 324, loss is 1.5265779495239258\n",
      "epoch: 5 step: 325, loss is 1.5056893825531006\n",
      "epoch: 5 step: 326, loss is 1.5677136182785034\n",
      "epoch: 5 step: 327, loss is 1.5332273244857788\n",
      "epoch: 5 step: 328, loss is 1.6357636451721191\n",
      "epoch: 5 step: 329, loss is 1.4522860050201416\n",
      "epoch: 5 step: 330, loss is 1.5082472562789917\n",
      "epoch: 5 step: 331, loss is 1.5686923265457153\n",
      "epoch: 5 step: 332, loss is 1.6437386274337769\n",
      "epoch: 5 step: 333, loss is 1.6112325191497803\n",
      "epoch: 5 step: 334, loss is 1.6286437511444092\n",
      "epoch: 5 step: 335, loss is 1.4681527614593506\n",
      "epoch: 5 step: 336, loss is 1.6392227411270142\n",
      "epoch: 5 step: 337, loss is 1.6492201089859009\n",
      "epoch: 5 step: 338, loss is 1.7366830110549927\n",
      "epoch: 5 step: 339, loss is 1.4860106706619263\n",
      "epoch: 5 step: 340, loss is 1.6644991636276245\n",
      "epoch: 5 step: 341, loss is 1.5431532859802246\n",
      "epoch: 5 step: 342, loss is 1.6136953830718994\n",
      "epoch: 5 step: 343, loss is 1.5596961975097656\n",
      "epoch: 5 step: 344, loss is 1.558932900428772\n",
      "epoch: 5 step: 345, loss is 1.6374828815460205\n",
      "epoch: 5 step: 346, loss is 1.5180007219314575\n",
      "epoch: 5 step: 347, loss is 1.6379892826080322\n",
      "epoch: 5 step: 348, loss is 1.6241679191589355\n",
      "epoch: 5 step: 349, loss is 1.6377544403076172\n",
      "epoch: 5 step: 350, loss is 1.5961253643035889\n",
      "epoch: 5 step: 351, loss is 1.5375986099243164\n",
      "epoch: 5 step: 352, loss is 1.5656367540359497\n",
      "epoch: 5 step: 353, loss is 1.479157567024231\n",
      "epoch: 5 step: 354, loss is 1.5018720626831055\n",
      "epoch: 5 step: 355, loss is 1.5857901573181152\n",
      "epoch: 5 step: 356, loss is 1.6011841297149658\n",
      "epoch: 5 step: 357, loss is 1.5148341655731201\n",
      "epoch: 5 step: 358, loss is 1.4817345142364502\n",
      "epoch: 5 step: 359, loss is 1.5203421115875244\n",
      "epoch: 5 step: 360, loss is 1.6331579685211182\n",
      "epoch: 5 step: 361, loss is 1.5015935897827148\n",
      "epoch: 5 step: 362, loss is 1.7164051532745361\n",
      "epoch: 5 step: 363, loss is 1.7008264064788818\n",
      "epoch: 5 step: 364, loss is 1.5548473596572876\n",
      "epoch: 5 step: 365, loss is 1.6139651536941528\n",
      "epoch: 5 step: 366, loss is 1.6094807386398315\n",
      "epoch: 5 step: 367, loss is 1.5670101642608643\n",
      "epoch: 5 step: 368, loss is 1.5830786228179932\n",
      "epoch: 5 step: 369, loss is 1.5350313186645508\n",
      "epoch: 5 step: 370, loss is 1.4754741191864014\n",
      "epoch: 5 step: 371, loss is 1.4926860332489014\n",
      "epoch: 5 step: 372, loss is 1.547879934310913\n",
      "epoch: 5 step: 373, loss is 1.5906093120574951\n",
      "epoch: 5 step: 374, loss is 1.6491690874099731\n",
      "epoch: 5 step: 375, loss is 1.5116182565689087\n",
      "epoch: 5 step: 376, loss is 1.6546770334243774\n",
      "epoch: 5 step: 377, loss is 1.5403027534484863\n",
      "epoch: 5 step: 378, loss is 1.608306646347046\n",
      "epoch: 5 step: 379, loss is 1.4441384077072144\n",
      "epoch: 5 step: 380, loss is 1.6022777557373047\n",
      "epoch: 5 step: 381, loss is 1.5039403438568115\n",
      "epoch: 5 step: 382, loss is 1.5593748092651367\n",
      "epoch: 5 step: 383, loss is 1.5085943937301636\n",
      "epoch: 5 step: 384, loss is 1.495922565460205\n",
      "epoch: 5 step: 385, loss is 1.6003928184509277\n",
      "epoch: 5 step: 386, loss is 1.6461018323898315\n",
      "epoch: 5 step: 387, loss is 1.6505961418151855\n",
      "epoch: 5 step: 388, loss is 1.4950575828552246\n",
      "epoch: 5 step: 389, loss is 1.616695761680603\n",
      "epoch: 5 step: 390, loss is 1.5886279344558716\n",
      "Train epoch time: 153381.135 ms, per step time: 393.285 ms\n",
      "epoch: 6 step: 1, loss is 1.565586805343628\n",
      "epoch: 6 step: 2, loss is 1.4993308782577515\n",
      "epoch: 6 step: 3, loss is 1.600111961364746\n",
      "epoch: 6 step: 4, loss is 1.5700170993804932\n",
      "epoch: 6 step: 5, loss is 1.5280811786651611\n",
      "epoch: 6 step: 6, loss is 1.608863115310669\n",
      "epoch: 6 step: 7, loss is 1.5079401731491089\n",
      "epoch: 6 step: 8, loss is 1.5283880233764648\n",
      "epoch: 6 step: 9, loss is 1.486999273300171\n",
      "epoch: 6 step: 10, loss is 1.4997620582580566\n",
      "epoch: 6 step: 11, loss is 1.5612157583236694\n",
      "epoch: 6 step: 12, loss is 1.6089789867401123\n",
      "epoch: 6 step: 13, loss is 1.5525715351104736\n",
      "epoch: 6 step: 14, loss is 1.6470140218734741\n",
      "epoch: 6 step: 15, loss is 1.501657247543335\n",
      "epoch: 6 step: 16, loss is 1.6303470134735107\n",
      "epoch: 6 step: 17, loss is 1.4303803443908691\n",
      "epoch: 6 step: 18, loss is 1.526267170906067\n",
      "epoch: 6 step: 19, loss is 1.590327262878418\n",
      "epoch: 6 step: 20, loss is 1.5643144845962524\n",
      "epoch: 6 step: 21, loss is 1.5361518859863281\n",
      "epoch: 6 step: 22, loss is 1.6993740797042847\n",
      "epoch: 6 step: 23, loss is 1.6236982345581055\n",
      "epoch: 6 step: 24, loss is 1.7303786277770996\n",
      "epoch: 6 step: 25, loss is 1.6358366012573242\n",
      "epoch: 6 step: 26, loss is 1.500091791152954\n",
      "epoch: 6 step: 27, loss is 1.6187044382095337\n",
      "epoch: 6 step: 28, loss is 1.531909704208374\n",
      "epoch: 6 step: 29, loss is 1.5079838037490845\n",
      "epoch: 6 step: 30, loss is 1.5756614208221436\n",
      "epoch: 6 step: 31, loss is 1.5335667133331299\n",
      "epoch: 6 step: 32, loss is 1.54270339012146\n",
      "epoch: 6 step: 33, loss is 1.631855845451355\n",
      "epoch: 6 step: 34, loss is 1.4721975326538086\n",
      "epoch: 6 step: 35, loss is 1.4219019412994385\n",
      "epoch: 6 step: 36, loss is 1.511681079864502\n",
      "epoch: 6 step: 37, loss is 1.6160836219787598\n",
      "epoch: 6 step: 38, loss is 1.5749776363372803\n",
      "epoch: 6 step: 39, loss is 1.6008055210113525\n",
      "epoch: 6 step: 40, loss is 1.5886322259902954\n",
      "epoch: 6 step: 41, loss is 1.4931325912475586\n",
      "epoch: 6 step: 42, loss is 1.5179176330566406\n",
      "epoch: 6 step: 43, loss is 1.663953423500061\n",
      "epoch: 6 step: 44, loss is 1.553452491760254\n",
      "epoch: 6 step: 45, loss is 1.5835844278335571\n",
      "epoch: 6 step: 46, loss is 1.4831223487854004\n",
      "epoch: 6 step: 47, loss is 1.454785943031311\n",
      "epoch: 6 step: 48, loss is 1.5498192310333252\n",
      "epoch: 6 step: 49, loss is 1.7561824321746826\n",
      "epoch: 6 step: 50, loss is 1.5386680364608765\n",
      "epoch: 6 step: 51, loss is 1.6133192777633667\n",
      "epoch: 6 step: 52, loss is 1.6322976350784302\n",
      "epoch: 6 step: 53, loss is 1.6038159132003784\n",
      "epoch: 6 step: 54, loss is 1.6247332096099854\n",
      "epoch: 6 step: 55, loss is 1.6453666687011719\n",
      "epoch: 6 step: 56, loss is 1.5314054489135742\n",
      "epoch: 6 step: 57, loss is 1.4436582326889038\n",
      "epoch: 6 step: 58, loss is 1.5119452476501465\n",
      "epoch: 6 step: 59, loss is 1.501593828201294\n",
      "epoch: 6 step: 60, loss is 1.5464119911193848\n",
      "epoch: 6 step: 61, loss is 1.5863416194915771\n",
      "epoch: 6 step: 62, loss is 1.6775901317596436\n",
      "epoch: 6 step: 63, loss is 1.5054173469543457\n",
      "epoch: 6 step: 64, loss is 1.5359346866607666\n",
      "epoch: 6 step: 65, loss is 1.5724741220474243\n",
      "epoch: 6 step: 66, loss is 1.5988094806671143\n",
      "epoch: 6 step: 67, loss is 1.6401231288909912\n",
      "epoch: 6 step: 68, loss is 1.496132254600525\n",
      "epoch: 6 step: 69, loss is 1.4924991130828857\n",
      "epoch: 6 step: 70, loss is 1.3236777782440186\n",
      "epoch: 6 step: 71, loss is 1.5804314613342285\n",
      "epoch: 6 step: 72, loss is 1.5018937587738037\n",
      "epoch: 6 step: 73, loss is 1.473308801651001\n",
      "epoch: 6 step: 74, loss is 1.5634307861328125\n",
      "epoch: 6 step: 75, loss is 1.6118403673171997\n",
      "epoch: 6 step: 76, loss is 1.5310378074645996\n",
      "epoch: 6 step: 77, loss is 1.4531898498535156\n",
      "epoch: 6 step: 78, loss is 1.5988332033157349\n",
      "epoch: 6 step: 79, loss is 1.4964289665222168\n",
      "epoch: 6 step: 80, loss is 1.506807804107666\n",
      "epoch: 6 step: 81, loss is 1.5957973003387451\n",
      "epoch: 6 step: 82, loss is 1.5360666513442993\n",
      "epoch: 6 step: 83, loss is 1.5406404733657837\n",
      "epoch: 6 step: 84, loss is 1.5522949695587158\n",
      "epoch: 6 step: 85, loss is 1.5213409662246704\n",
      "epoch: 6 step: 86, loss is 1.4799822568893433\n",
      "epoch: 6 step: 87, loss is 1.5899888277053833\n",
      "epoch: 6 step: 88, loss is 1.4785696268081665\n",
      "epoch: 6 step: 89, loss is 1.6110656261444092\n",
      "epoch: 6 step: 90, loss is 1.616820216178894\n",
      "epoch: 6 step: 91, loss is 1.7484219074249268\n",
      "epoch: 6 step: 92, loss is 1.6080840826034546\n",
      "epoch: 6 step: 93, loss is 1.602646827697754\n",
      "epoch: 6 step: 94, loss is 1.660607099533081\n",
      "epoch: 6 step: 95, loss is 1.6191389560699463\n",
      "epoch: 6 step: 96, loss is 1.5669595003128052\n",
      "epoch: 6 step: 97, loss is 1.6059510707855225\n",
      "epoch: 6 step: 98, loss is 1.5233793258666992\n",
      "epoch: 6 step: 99, loss is 1.5146255493164062\n",
      "epoch: 6 step: 100, loss is 1.6255992650985718\n",
      "epoch: 6 step: 101, loss is 1.6687489748001099\n",
      "epoch: 6 step: 102, loss is 1.6068670749664307\n",
      "epoch: 6 step: 103, loss is 1.5893867015838623\n",
      "epoch: 6 step: 104, loss is 1.572099208831787\n",
      "epoch: 6 step: 105, loss is 1.4617297649383545\n",
      "epoch: 6 step: 106, loss is 1.43076491355896\n",
      "epoch: 6 step: 107, loss is 1.5131405591964722\n",
      "epoch: 6 step: 108, loss is 1.5251888036727905\n",
      "epoch: 6 step: 109, loss is 1.6991758346557617\n",
      "epoch: 6 step: 110, loss is 1.5678520202636719\n",
      "epoch: 6 step: 111, loss is 1.531793236732483\n",
      "epoch: 6 step: 112, loss is 1.6264300346374512\n",
      "epoch: 6 step: 113, loss is 1.5827183723449707\n",
      "epoch: 6 step: 114, loss is 1.4352396726608276\n",
      "epoch: 6 step: 115, loss is 1.4659738540649414\n",
      "epoch: 6 step: 116, loss is 1.6466726064682007\n",
      "epoch: 6 step: 117, loss is 1.5846641063690186\n",
      "epoch: 6 step: 118, loss is 1.529785394668579\n",
      "epoch: 6 step: 119, loss is 1.63285231590271\n",
      "epoch: 6 step: 120, loss is 1.6019163131713867\n",
      "epoch: 6 step: 121, loss is 1.475883960723877\n",
      "epoch: 6 step: 122, loss is 1.6361966133117676\n",
      "epoch: 6 step: 123, loss is 1.4980905055999756\n",
      "epoch: 6 step: 124, loss is 1.5733188390731812\n",
      "epoch: 6 step: 125, loss is 1.5087807178497314\n",
      "epoch: 6 step: 126, loss is 1.669301986694336\n",
      "epoch: 6 step: 127, loss is 1.682594656944275\n",
      "epoch: 6 step: 128, loss is 1.5247570276260376\n",
      "epoch: 6 step: 129, loss is 1.6094226837158203\n",
      "epoch: 6 step: 130, loss is 1.6220035552978516\n",
      "epoch: 6 step: 131, loss is 1.6521320343017578\n",
      "epoch: 6 step: 132, loss is 1.7062668800354004\n",
      "epoch: 6 step: 133, loss is 1.5593849420547485\n",
      "epoch: 6 step: 134, loss is 1.7056595087051392\n",
      "epoch: 6 step: 135, loss is 1.531843662261963\n",
      "epoch: 6 step: 136, loss is 1.506941795349121\n",
      "epoch: 6 step: 137, loss is 1.5452666282653809\n",
      "epoch: 6 step: 138, loss is 1.5527653694152832\n",
      "epoch: 6 step: 139, loss is 1.5819357633590698\n",
      "epoch: 6 step: 140, loss is 1.6072134971618652\n",
      "epoch: 6 step: 141, loss is 1.5664870738983154\n",
      "epoch: 6 step: 142, loss is 1.5451949834823608\n",
      "epoch: 6 step: 143, loss is 1.445901870727539\n",
      "epoch: 6 step: 144, loss is 1.6765443086624146\n",
      "epoch: 6 step: 145, loss is 1.5278723239898682\n",
      "epoch: 6 step: 146, loss is 1.4951523542404175\n",
      "epoch: 6 step: 147, loss is 1.6156787872314453\n",
      "epoch: 6 step: 148, loss is 1.5054621696472168\n",
      "epoch: 6 step: 149, loss is 1.4467217922210693\n",
      "epoch: 6 step: 150, loss is 1.6746301651000977\n",
      "epoch: 6 step: 151, loss is 1.4909759759902954\n",
      "epoch: 6 step: 152, loss is 1.568279504776001\n",
      "epoch: 6 step: 153, loss is 1.4585762023925781\n",
      "epoch: 6 step: 154, loss is 1.5223557949066162\n",
      "epoch: 6 step: 155, loss is 1.6629202365875244\n",
      "epoch: 6 step: 156, loss is 1.4807052612304688\n",
      "epoch: 6 step: 157, loss is 1.4201710224151611\n",
      "epoch: 6 step: 158, loss is 1.5957896709442139\n",
      "epoch: 6 step: 159, loss is 1.7684242725372314\n",
      "epoch: 6 step: 160, loss is 1.5105022192001343\n",
      "epoch: 6 step: 161, loss is 1.4932401180267334\n",
      "epoch: 6 step: 162, loss is 1.593070149421692\n",
      "epoch: 6 step: 163, loss is 1.6356115341186523\n",
      "epoch: 6 step: 164, loss is 1.6387600898742676\n",
      "epoch: 6 step: 165, loss is 1.5524730682373047\n",
      "epoch: 6 step: 166, loss is 1.6083036661148071\n",
      "epoch: 6 step: 167, loss is 1.544285535812378\n",
      "epoch: 6 step: 168, loss is 1.5810773372650146\n",
      "epoch: 6 step: 169, loss is 1.5682727098464966\n",
      "epoch: 6 step: 170, loss is 1.6470115184783936\n",
      "epoch: 6 step: 171, loss is 1.4937779903411865\n",
      "epoch: 6 step: 172, loss is 1.7319233417510986\n",
      "epoch: 6 step: 173, loss is 1.5768488645553589\n",
      "epoch: 6 step: 174, loss is 1.5432833433151245\n",
      "epoch: 6 step: 175, loss is 1.5666778087615967\n",
      "epoch: 6 step: 176, loss is 1.5083640813827515\n",
      "epoch: 6 step: 177, loss is 1.575564980506897\n",
      "epoch: 6 step: 178, loss is 1.6651510000228882\n",
      "epoch: 6 step: 179, loss is 1.5929594039916992\n",
      "epoch: 6 step: 180, loss is 1.5730526447296143\n",
      "epoch: 6 step: 181, loss is 1.330108404159546\n",
      "epoch: 6 step: 182, loss is 1.4717422723770142\n",
      "epoch: 6 step: 183, loss is 1.4073286056518555\n",
      "epoch: 6 step: 184, loss is 1.4559725522994995\n",
      "epoch: 6 step: 185, loss is 1.5157870054244995\n",
      "epoch: 6 step: 186, loss is 1.5994939804077148\n",
      "epoch: 6 step: 187, loss is 1.6122450828552246\n",
      "epoch: 6 step: 188, loss is 1.5411090850830078\n",
      "epoch: 6 step: 189, loss is 1.5102226734161377\n",
      "epoch: 6 step: 190, loss is 1.6032445430755615\n",
      "epoch: 6 step: 191, loss is 1.3780841827392578\n",
      "epoch: 6 step: 192, loss is 1.5404281616210938\n",
      "epoch: 6 step: 193, loss is 1.4739127159118652\n",
      "epoch: 6 step: 194, loss is 1.585615873336792\n",
      "epoch: 6 step: 195, loss is 1.4945818185806274\n",
      "epoch: 6 step: 196, loss is 1.5370852947235107\n",
      "epoch: 6 step: 197, loss is 1.578393816947937\n",
      "epoch: 6 step: 198, loss is 1.5206938982009888\n",
      "epoch: 6 step: 199, loss is 1.6550489664077759\n",
      "epoch: 6 step: 200, loss is 1.6613051891326904\n",
      "epoch: 6 step: 201, loss is 1.5266849994659424\n",
      "epoch: 6 step: 202, loss is 1.53572678565979\n",
      "epoch: 6 step: 203, loss is 1.5514798164367676\n",
      "epoch: 6 step: 204, loss is 1.6575767993927002\n",
      "epoch: 6 step: 205, loss is 1.4422082901000977\n",
      "epoch: 6 step: 206, loss is 1.419928789138794\n",
      "epoch: 6 step: 207, loss is 1.490402102470398\n",
      "epoch: 6 step: 208, loss is 1.3558969497680664\n",
      "epoch: 6 step: 209, loss is 1.4824092388153076\n",
      "epoch: 6 step: 210, loss is 1.4984710216522217\n",
      "epoch: 6 step: 211, loss is 1.671372890472412\n",
      "epoch: 6 step: 212, loss is 1.4603523015975952\n",
      "epoch: 6 step: 213, loss is 1.492262840270996\n",
      "epoch: 6 step: 214, loss is 1.5639557838439941\n",
      "epoch: 6 step: 215, loss is 1.554969072341919\n",
      "epoch: 6 step: 216, loss is 1.5582172870635986\n",
      "epoch: 6 step: 217, loss is 1.6347458362579346\n",
      "epoch: 6 step: 218, loss is 1.6334506273269653\n",
      "epoch: 6 step: 219, loss is 1.501328468322754\n",
      "epoch: 6 step: 220, loss is 1.5232255458831787\n",
      "epoch: 6 step: 221, loss is 1.5982776880264282\n",
      "epoch: 6 step: 222, loss is 1.5217220783233643\n",
      "epoch: 6 step: 223, loss is 1.5836732387542725\n",
      "epoch: 6 step: 224, loss is 1.4566819667816162\n",
      "epoch: 6 step: 225, loss is 1.378349781036377\n",
      "epoch: 6 step: 226, loss is 1.701306939125061\n",
      "epoch: 6 step: 227, loss is 1.4459011554718018\n",
      "epoch: 6 step: 228, loss is 1.4961618185043335\n",
      "epoch: 6 step: 229, loss is 1.5388237237930298\n",
      "epoch: 6 step: 230, loss is 1.533425211906433\n",
      "epoch: 6 step: 231, loss is 1.6368216276168823\n",
      "epoch: 6 step: 232, loss is 1.428985357284546\n",
      "epoch: 6 step: 233, loss is 1.6259602308273315\n",
      "epoch: 6 step: 234, loss is 1.5302422046661377\n",
      "epoch: 6 step: 235, loss is 1.6578096151351929\n",
      "epoch: 6 step: 236, loss is 1.5119385719299316\n",
      "epoch: 6 step: 237, loss is 1.6036999225616455\n",
      "epoch: 6 step: 238, loss is 1.564316749572754\n",
      "epoch: 6 step: 239, loss is 1.4918808937072754\n",
      "epoch: 6 step: 240, loss is 1.5494248867034912\n",
      "epoch: 6 step: 241, loss is 1.4754133224487305\n",
      "epoch: 6 step: 242, loss is 1.554903507232666\n",
      "epoch: 6 step: 243, loss is 1.5745291709899902\n",
      "epoch: 6 step: 244, loss is 1.6653283834457397\n",
      "epoch: 6 step: 245, loss is 1.5540529489517212\n",
      "epoch: 6 step: 246, loss is 1.4559271335601807\n",
      "epoch: 6 step: 247, loss is 1.5815250873565674\n",
      "epoch: 6 step: 248, loss is 1.5298374891281128\n",
      "epoch: 6 step: 249, loss is 1.6034495830535889\n",
      "epoch: 6 step: 250, loss is 1.5484840869903564\n",
      "epoch: 6 step: 251, loss is 1.4853794574737549\n",
      "epoch: 6 step: 252, loss is 1.5321038961410522\n",
      "epoch: 6 step: 253, loss is 1.4619603157043457\n",
      "epoch: 6 step: 254, loss is 1.6250629425048828\n",
      "epoch: 6 step: 255, loss is 1.5083627700805664\n",
      "epoch: 6 step: 256, loss is 1.5931452512741089\n",
      "epoch: 6 step: 257, loss is 1.4984629154205322\n",
      "epoch: 6 step: 258, loss is 1.6578176021575928\n",
      "epoch: 6 step: 259, loss is 1.492484450340271\n",
      "epoch: 6 step: 260, loss is 1.5217316150665283\n",
      "epoch: 6 step: 261, loss is 1.5988391637802124\n",
      "epoch: 6 step: 262, loss is 1.5634019374847412\n",
      "epoch: 6 step: 263, loss is 1.5266234874725342\n",
      "epoch: 6 step: 264, loss is 1.6113508939743042\n",
      "epoch: 6 step: 265, loss is 1.449882984161377\n",
      "epoch: 6 step: 266, loss is 1.567259669303894\n",
      "epoch: 6 step: 267, loss is 1.488381028175354\n",
      "epoch: 6 step: 268, loss is 1.4724940061569214\n",
      "epoch: 6 step: 269, loss is 1.5677845478057861\n",
      "epoch: 6 step: 270, loss is 1.6471939086914062\n",
      "epoch: 6 step: 271, loss is 1.635873794555664\n",
      "epoch: 6 step: 272, loss is 1.595261573791504\n",
      "epoch: 6 step: 273, loss is 1.3924915790557861\n",
      "epoch: 6 step: 274, loss is 1.3678295612335205\n",
      "epoch: 6 step: 275, loss is 1.4291549921035767\n",
      "epoch: 6 step: 276, loss is 1.574383020401001\n",
      "epoch: 6 step: 277, loss is 1.5029693841934204\n",
      "epoch: 6 step: 278, loss is 1.7206549644470215\n",
      "epoch: 6 step: 279, loss is 1.6617743968963623\n",
      "epoch: 6 step: 280, loss is 1.5701321363449097\n",
      "epoch: 6 step: 281, loss is 1.493404507637024\n",
      "epoch: 6 step: 282, loss is 1.5507370233535767\n",
      "epoch: 6 step: 283, loss is 1.5866509675979614\n",
      "epoch: 6 step: 284, loss is 1.5492209196090698\n",
      "epoch: 6 step: 285, loss is 1.5226272344589233\n",
      "epoch: 6 step: 286, loss is 1.645470142364502\n",
      "epoch: 6 step: 287, loss is 1.5649263858795166\n",
      "epoch: 6 step: 288, loss is 1.4843961000442505\n",
      "epoch: 6 step: 289, loss is 1.531566858291626\n",
      "epoch: 6 step: 290, loss is 1.684746503829956\n",
      "epoch: 6 step: 291, loss is 1.5109684467315674\n",
      "epoch: 6 step: 292, loss is 1.5117878913879395\n",
      "epoch: 6 step: 293, loss is 1.5602935552597046\n",
      "epoch: 6 step: 294, loss is 1.618908166885376\n",
      "epoch: 6 step: 295, loss is 1.540900468826294\n",
      "epoch: 6 step: 296, loss is 1.558140754699707\n",
      "epoch: 6 step: 297, loss is 1.4904108047485352\n",
      "epoch: 6 step: 298, loss is 1.4692399501800537\n",
      "epoch: 6 step: 299, loss is 1.5373260974884033\n",
      "epoch: 6 step: 300, loss is 1.5580161809921265\n",
      "epoch: 6 step: 301, loss is 1.5258065462112427\n",
      "epoch: 6 step: 302, loss is 1.4324734210968018\n",
      "epoch: 6 step: 303, loss is 1.550022006034851\n",
      "epoch: 6 step: 304, loss is 1.550607442855835\n",
      "epoch: 6 step: 305, loss is 1.6115641593933105\n",
      "epoch: 6 step: 306, loss is 1.5833346843719482\n",
      "epoch: 6 step: 307, loss is 1.4445509910583496\n",
      "epoch: 6 step: 308, loss is 1.4966994524002075\n",
      "epoch: 6 step: 309, loss is 1.601759433746338\n",
      "epoch: 6 step: 310, loss is 1.6854147911071777\n",
      "epoch: 6 step: 311, loss is 1.5799009799957275\n",
      "epoch: 6 step: 312, loss is 1.7273669242858887\n",
      "epoch: 6 step: 313, loss is 1.5824686288833618\n",
      "epoch: 6 step: 314, loss is 1.5359410047531128\n",
      "epoch: 6 step: 315, loss is 1.452476978302002\n",
      "epoch: 6 step: 316, loss is 1.5860435962677002\n",
      "epoch: 6 step: 317, loss is 1.4422460794448853\n",
      "epoch: 6 step: 318, loss is 1.5104031562805176\n",
      "epoch: 6 step: 319, loss is 1.508713960647583\n",
      "epoch: 6 step: 320, loss is 1.5639193058013916\n",
      "epoch: 6 step: 321, loss is 1.3520243167877197\n",
      "epoch: 6 step: 322, loss is 1.4979640245437622\n",
      "epoch: 6 step: 323, loss is 1.5116066932678223\n",
      "epoch: 6 step: 324, loss is 1.5152149200439453\n",
      "epoch: 6 step: 325, loss is 1.5765600204467773\n",
      "epoch: 6 step: 326, loss is 1.544485092163086\n",
      "epoch: 6 step: 327, loss is 1.4769501686096191\n",
      "epoch: 6 step: 328, loss is 1.5672470331192017\n",
      "epoch: 6 step: 329, loss is 1.7016289234161377\n",
      "epoch: 6 step: 330, loss is 1.6098726987838745\n",
      "epoch: 6 step: 331, loss is 1.5021378993988037\n",
      "epoch: 6 step: 332, loss is 1.5848596096038818\n",
      "epoch: 6 step: 333, loss is 1.505998134613037\n",
      "epoch: 6 step: 334, loss is 1.4650945663452148\n",
      "epoch: 6 step: 335, loss is 1.6529346704483032\n",
      "epoch: 6 step: 336, loss is 1.3819522857666016\n",
      "epoch: 6 step: 337, loss is 1.4633917808532715\n",
      "epoch: 6 step: 338, loss is 1.4703480005264282\n",
      "epoch: 6 step: 339, loss is 1.453094482421875\n",
      "epoch: 6 step: 340, loss is 1.5526607036590576\n",
      "epoch: 6 step: 341, loss is 1.5617015361785889\n",
      "epoch: 6 step: 342, loss is 1.4920934438705444\n",
      "epoch: 6 step: 343, loss is 1.5347673892974854\n",
      "epoch: 6 step: 344, loss is 1.4927833080291748\n",
      "epoch: 6 step: 345, loss is 1.5521256923675537\n",
      "epoch: 6 step: 346, loss is 1.621452808380127\n",
      "epoch: 6 step: 347, loss is 1.5456485748291016\n",
      "epoch: 6 step: 348, loss is 1.4453198909759521\n",
      "epoch: 6 step: 349, loss is 1.5524117946624756\n",
      "epoch: 6 step: 350, loss is 1.5816948413848877\n",
      "epoch: 6 step: 351, loss is 1.4902589321136475\n",
      "epoch: 6 step: 352, loss is 1.5228277444839478\n",
      "epoch: 6 step: 353, loss is 1.57585871219635\n",
      "epoch: 6 step: 354, loss is 1.659492015838623\n",
      "epoch: 6 step: 355, loss is 1.5355043411254883\n",
      "epoch: 6 step: 356, loss is 1.4787086248397827\n",
      "epoch: 6 step: 357, loss is 1.541299819946289\n",
      "epoch: 6 step: 358, loss is 1.48649263381958\n",
      "epoch: 6 step: 359, loss is 1.5591692924499512\n",
      "epoch: 6 step: 360, loss is 1.542729377746582\n",
      "epoch: 6 step: 361, loss is 1.5840511322021484\n",
      "epoch: 6 step: 362, loss is 1.5087860822677612\n",
      "epoch: 6 step: 363, loss is 1.6039785146713257\n",
      "epoch: 6 step: 364, loss is 1.4644997119903564\n",
      "epoch: 6 step: 365, loss is 1.5328654050827026\n",
      "epoch: 6 step: 366, loss is 1.5517117977142334\n",
      "epoch: 6 step: 367, loss is 1.6074109077453613\n",
      "epoch: 6 step: 368, loss is 1.543803334236145\n",
      "epoch: 6 step: 369, loss is 1.4982911348342896\n",
      "epoch: 6 step: 370, loss is 1.5000215768814087\n",
      "epoch: 6 step: 371, loss is 1.6229758262634277\n",
      "epoch: 6 step: 372, loss is 1.5714401006698608\n",
      "epoch: 6 step: 373, loss is 1.54378342628479\n",
      "epoch: 6 step: 374, loss is 1.532149314880371\n",
      "epoch: 6 step: 375, loss is 1.4938249588012695\n",
      "epoch: 6 step: 376, loss is 1.4472419023513794\n",
      "epoch: 6 step: 377, loss is 1.5924360752105713\n",
      "epoch: 6 step: 378, loss is 1.5504693984985352\n",
      "epoch: 6 step: 379, loss is 1.491994857788086\n",
      "epoch: 6 step: 380, loss is 1.6437022686004639\n",
      "epoch: 6 step: 381, loss is 1.5256884098052979\n",
      "epoch: 6 step: 382, loss is 1.5422533750534058\n",
      "epoch: 6 step: 383, loss is 1.4978787899017334\n",
      "epoch: 6 step: 384, loss is 1.5583754777908325\n",
      "epoch: 6 step: 385, loss is 1.4516420364379883\n",
      "epoch: 6 step: 386, loss is 1.5245431661605835\n",
      "epoch: 6 step: 387, loss is 1.3806862831115723\n",
      "epoch: 6 step: 388, loss is 1.393424153327942\n",
      "epoch: 6 step: 389, loss is 1.484189748764038\n",
      "epoch: 6 step: 390, loss is 1.3940329551696777\n",
      "Train epoch time: 151864.475 ms, per step time: 389.396 ms\n",
      "epoch: 7 step: 1, loss is 1.4820144176483154\n",
      "epoch: 7 step: 2, loss is 1.5358738899230957\n",
      "epoch: 7 step: 3, loss is 1.4891012907028198\n",
      "epoch: 7 step: 4, loss is 1.5338561534881592\n",
      "epoch: 7 step: 5, loss is 1.4843417406082153\n",
      "epoch: 7 step: 6, loss is 1.696803092956543\n",
      "epoch: 7 step: 7, loss is 1.5179387331008911\n",
      "epoch: 7 step: 8, loss is 1.5512969493865967\n",
      "epoch: 7 step: 9, loss is 1.5645184516906738\n",
      "epoch: 7 step: 10, loss is 1.5288352966308594\n",
      "epoch: 7 step: 11, loss is 1.4215524196624756\n",
      "epoch: 7 step: 12, loss is 1.6127238273620605\n",
      "epoch: 7 step: 13, loss is 1.5102708339691162\n",
      "epoch: 7 step: 14, loss is 1.6085920333862305\n",
      "epoch: 7 step: 15, loss is 1.543104887008667\n",
      "epoch: 7 step: 16, loss is 1.524449110031128\n",
      "epoch: 7 step: 17, loss is 1.6438357830047607\n",
      "epoch: 7 step: 18, loss is 1.4874534606933594\n",
      "epoch: 7 step: 19, loss is 1.664794683456421\n",
      "epoch: 7 step: 20, loss is 1.4059504270553589\n",
      "epoch: 7 step: 21, loss is 1.4238078594207764\n",
      "epoch: 7 step: 22, loss is 1.4020732641220093\n",
      "epoch: 7 step: 23, loss is 1.5954811573028564\n",
      "epoch: 7 step: 24, loss is 1.5878424644470215\n",
      "epoch: 7 step: 25, loss is 1.5389586687088013\n",
      "epoch: 7 step: 26, loss is 1.7183761596679688\n",
      "epoch: 7 step: 27, loss is 1.6218414306640625\n",
      "epoch: 7 step: 28, loss is 1.4021072387695312\n",
      "epoch: 7 step: 29, loss is 1.605531096458435\n",
      "epoch: 7 step: 30, loss is 1.4552857875823975\n",
      "epoch: 7 step: 31, loss is 1.5176937580108643\n",
      "epoch: 7 step: 32, loss is 1.4036235809326172\n",
      "epoch: 7 step: 33, loss is 1.592960238456726\n",
      "epoch: 7 step: 34, loss is 1.602339506149292\n",
      "epoch: 7 step: 35, loss is 1.5778019428253174\n",
      "epoch: 7 step: 36, loss is 1.502991795539856\n",
      "epoch: 7 step: 37, loss is 1.5291630029678345\n",
      "epoch: 7 step: 38, loss is 1.5891938209533691\n",
      "epoch: 7 step: 39, loss is 1.5672430992126465\n",
      "epoch: 7 step: 40, loss is 1.380429744720459\n",
      "epoch: 7 step: 41, loss is 1.5072259902954102\n",
      "epoch: 7 step: 42, loss is 1.4327309131622314\n",
      "epoch: 7 step: 43, loss is 1.5567258596420288\n",
      "epoch: 7 step: 44, loss is 1.5991549491882324\n",
      "epoch: 7 step: 45, loss is 1.6363989114761353\n",
      "epoch: 7 step: 46, loss is 1.5003859996795654\n",
      "epoch: 7 step: 47, loss is 1.4777953624725342\n",
      "epoch: 7 step: 48, loss is 1.5995192527770996\n",
      "epoch: 7 step: 49, loss is 1.3707598447799683\n",
      "epoch: 7 step: 50, loss is 1.5877196788787842\n",
      "epoch: 7 step: 51, loss is 1.449927568435669\n",
      "epoch: 7 step: 52, loss is 1.4909369945526123\n",
      "epoch: 7 step: 53, loss is 1.5523357391357422\n",
      "epoch: 7 step: 54, loss is 1.4071195125579834\n",
      "epoch: 7 step: 55, loss is 1.6212759017944336\n",
      "epoch: 7 step: 56, loss is 1.595902919769287\n",
      "epoch: 7 step: 57, loss is 1.4344093799591064\n",
      "epoch: 7 step: 58, loss is 1.4249101877212524\n",
      "epoch: 7 step: 59, loss is 1.488660454750061\n",
      "epoch: 7 step: 60, loss is 1.5191638469696045\n",
      "epoch: 7 step: 61, loss is 1.5035674571990967\n",
      "epoch: 7 step: 62, loss is 1.5764011144638062\n",
      "epoch: 7 step: 63, loss is 1.4705513715744019\n",
      "epoch: 7 step: 64, loss is 1.6332005262374878\n",
      "epoch: 7 step: 65, loss is 1.5672935247421265\n",
      "epoch: 7 step: 66, loss is 1.621472716331482\n",
      "epoch: 7 step: 67, loss is 1.6568324565887451\n",
      "epoch: 7 step: 68, loss is 1.4700987339019775\n",
      "epoch: 7 step: 69, loss is 1.5981731414794922\n",
      "epoch: 7 step: 70, loss is 1.6118203401565552\n",
      "epoch: 7 step: 71, loss is 1.6883621215820312\n",
      "epoch: 7 step: 72, loss is 1.4868266582489014\n",
      "epoch: 7 step: 73, loss is 1.3898115158081055\n",
      "epoch: 7 step: 74, loss is 1.6245986223220825\n",
      "epoch: 7 step: 75, loss is 1.5548253059387207\n",
      "epoch: 7 step: 76, loss is 1.5204896926879883\n",
      "epoch: 7 step: 77, loss is 1.5078407526016235\n",
      "epoch: 7 step: 78, loss is 1.6184701919555664\n",
      "epoch: 7 step: 79, loss is 1.549187183380127\n",
      "epoch: 7 step: 80, loss is 1.4841949939727783\n",
      "epoch: 7 step: 81, loss is 1.552040457725525\n",
      "epoch: 7 step: 82, loss is 1.3757824897766113\n",
      "epoch: 7 step: 83, loss is 1.5893443822860718\n",
      "epoch: 7 step: 84, loss is 1.5019036531448364\n",
      "epoch: 7 step: 85, loss is 1.4268674850463867\n",
      "epoch: 7 step: 86, loss is 1.4941842555999756\n",
      "epoch: 7 step: 87, loss is 1.5383164882659912\n",
      "epoch: 7 step: 88, loss is 1.5746339559555054\n",
      "epoch: 7 step: 89, loss is 1.548266887664795\n",
      "epoch: 7 step: 90, loss is 1.5876290798187256\n",
      "epoch: 7 step: 91, loss is 1.410891056060791\n",
      "epoch: 7 step: 92, loss is 1.4751636981964111\n",
      "epoch: 7 step: 93, loss is 1.649699330329895\n",
      "epoch: 7 step: 94, loss is 1.494714617729187\n",
      "epoch: 7 step: 95, loss is 1.5426506996154785\n",
      "epoch: 7 step: 96, loss is 1.5895227193832397\n",
      "epoch: 7 step: 97, loss is 1.4889949560165405\n",
      "epoch: 7 step: 98, loss is 1.576007604598999\n",
      "epoch: 7 step: 99, loss is 1.4689242839813232\n",
      "epoch: 7 step: 100, loss is 1.4164578914642334\n",
      "epoch: 7 step: 101, loss is 1.5597810745239258\n",
      "epoch: 7 step: 102, loss is 1.6429321765899658\n",
      "epoch: 7 step: 103, loss is 1.59334135055542\n",
      "epoch: 7 step: 104, loss is 1.5276964902877808\n",
      "epoch: 7 step: 105, loss is 1.5717709064483643\n",
      "epoch: 7 step: 106, loss is 1.5075920820236206\n",
      "epoch: 7 step: 107, loss is 1.4703259468078613\n",
      "epoch: 7 step: 108, loss is 1.5053699016571045\n",
      "epoch: 7 step: 109, loss is 1.485256314277649\n",
      "epoch: 7 step: 110, loss is 1.4893797636032104\n",
      "epoch: 7 step: 111, loss is 1.582798719406128\n",
      "epoch: 7 step: 112, loss is 1.3878353834152222\n",
      "epoch: 7 step: 113, loss is 1.5349156856536865\n",
      "epoch: 7 step: 114, loss is 1.5552934408187866\n",
      "epoch: 7 step: 115, loss is 1.6357619762420654\n",
      "epoch: 7 step: 116, loss is 1.5388047695159912\n",
      "epoch: 7 step: 117, loss is 1.4892208576202393\n",
      "epoch: 7 step: 118, loss is 1.5721242427825928\n",
      "epoch: 7 step: 119, loss is 1.4747707843780518\n",
      "epoch: 7 step: 120, loss is 1.6333537101745605\n",
      "epoch: 7 step: 121, loss is 1.5075726509094238\n",
      "epoch: 7 step: 122, loss is 1.587907314300537\n",
      "epoch: 7 step: 123, loss is 1.558164119720459\n",
      "epoch: 7 step: 124, loss is 1.5157175064086914\n",
      "epoch: 7 step: 125, loss is 1.6053814888000488\n",
      "epoch: 7 step: 126, loss is 1.431105613708496\n",
      "epoch: 7 step: 127, loss is 1.5361287593841553\n",
      "epoch: 7 step: 128, loss is 1.4435278177261353\n",
      "epoch: 7 step: 129, loss is 1.5905650854110718\n",
      "epoch: 7 step: 130, loss is 1.5542964935302734\n",
      "epoch: 7 step: 131, loss is 1.5566155910491943\n",
      "epoch: 7 step: 132, loss is 1.4586284160614014\n",
      "epoch: 7 step: 133, loss is 1.5617916584014893\n",
      "epoch: 7 step: 134, loss is 1.4861910343170166\n",
      "epoch: 7 step: 135, loss is 1.4638497829437256\n",
      "epoch: 7 step: 136, loss is 1.3712517023086548\n",
      "epoch: 7 step: 137, loss is 1.4984257221221924\n",
      "epoch: 7 step: 138, loss is 1.412426471710205\n",
      "epoch: 7 step: 139, loss is 1.483001947402954\n",
      "epoch: 7 step: 140, loss is 1.6162350177764893\n",
      "epoch: 7 step: 141, loss is 1.558847188949585\n",
      "epoch: 7 step: 142, loss is 1.4812273979187012\n",
      "epoch: 7 step: 143, loss is 1.5379948616027832\n",
      "epoch: 7 step: 144, loss is 1.5028820037841797\n",
      "epoch: 7 step: 145, loss is 1.540477991104126\n",
      "epoch: 7 step: 146, loss is 1.5777112245559692\n",
      "epoch: 7 step: 147, loss is 1.4277514219284058\n",
      "epoch: 7 step: 148, loss is 1.5029312372207642\n",
      "epoch: 7 step: 149, loss is 1.588871717453003\n",
      "epoch: 7 step: 150, loss is 1.5004501342773438\n",
      "epoch: 7 step: 151, loss is 1.5233381986618042\n",
      "epoch: 7 step: 152, loss is 1.5240824222564697\n",
      "epoch: 7 step: 153, loss is 1.5194441080093384\n",
      "epoch: 7 step: 154, loss is 1.7072370052337646\n",
      "epoch: 7 step: 155, loss is 1.564374566078186\n",
      "epoch: 7 step: 156, loss is 1.5198991298675537\n",
      "epoch: 7 step: 157, loss is 1.570144772529602\n",
      "epoch: 7 step: 158, loss is 1.3959267139434814\n",
      "epoch: 7 step: 159, loss is 1.5621416568756104\n",
      "epoch: 7 step: 160, loss is 1.4993844032287598\n",
      "epoch: 7 step: 161, loss is 1.5547044277191162\n",
      "epoch: 7 step: 162, loss is 1.3923065662384033\n",
      "epoch: 7 step: 163, loss is 1.4730278253555298\n",
      "epoch: 7 step: 164, loss is 1.608035922050476\n",
      "epoch: 7 step: 165, loss is 1.4448332786560059\n",
      "epoch: 7 step: 166, loss is 1.5089333057403564\n",
      "epoch: 7 step: 167, loss is 1.4370057582855225\n",
      "epoch: 7 step: 168, loss is 1.425804853439331\n",
      "epoch: 7 step: 169, loss is 1.5152814388275146\n",
      "epoch: 7 step: 170, loss is 1.5589542388916016\n",
      "epoch: 7 step: 171, loss is 1.565525770187378\n",
      "epoch: 7 step: 172, loss is 1.5918574333190918\n",
      "epoch: 7 step: 173, loss is 1.5467448234558105\n",
      "epoch: 7 step: 174, loss is 1.6368253231048584\n",
      "epoch: 7 step: 175, loss is 1.5316646099090576\n",
      "epoch: 7 step: 176, loss is 1.481982707977295\n",
      "epoch: 7 step: 177, loss is 1.4646942615509033\n",
      "epoch: 7 step: 178, loss is 1.4131579399108887\n",
      "epoch: 7 step: 179, loss is 1.6280531883239746\n",
      "epoch: 7 step: 180, loss is 1.5038255453109741\n",
      "epoch: 7 step: 181, loss is 1.5195403099060059\n",
      "epoch: 7 step: 182, loss is 1.4021737575531006\n",
      "epoch: 7 step: 183, loss is 1.4098573923110962\n",
      "epoch: 7 step: 184, loss is 1.5025742053985596\n",
      "epoch: 7 step: 185, loss is 1.5962494611740112\n",
      "epoch: 7 step: 186, loss is 1.4298152923583984\n",
      "epoch: 7 step: 187, loss is 1.558235764503479\n",
      "epoch: 7 step: 188, loss is 1.4861494302749634\n",
      "epoch: 7 step: 189, loss is 1.524181842803955\n",
      "epoch: 7 step: 190, loss is 1.4919451475143433\n",
      "epoch: 7 step: 191, loss is 1.5350191593170166\n",
      "epoch: 7 step: 192, loss is 1.5022876262664795\n",
      "epoch: 7 step: 193, loss is 1.6617560386657715\n",
      "epoch: 7 step: 194, loss is 1.4762042760849\n",
      "epoch: 7 step: 195, loss is 1.4749494791030884\n",
      "epoch: 7 step: 196, loss is 1.4670332670211792\n",
      "epoch: 7 step: 197, loss is 1.5307074785232544\n",
      "epoch: 7 step: 198, loss is 1.3962910175323486\n",
      "epoch: 7 step: 199, loss is 1.412091851234436\n",
      "epoch: 7 step: 200, loss is 1.4878069162368774\n",
      "epoch: 7 step: 201, loss is 1.4374250173568726\n",
      "epoch: 7 step: 202, loss is 1.468526840209961\n",
      "epoch: 7 step: 203, loss is 1.5676881074905396\n",
      "epoch: 7 step: 204, loss is 1.6007295846939087\n",
      "epoch: 7 step: 205, loss is 1.4395670890808105\n",
      "epoch: 7 step: 206, loss is 1.4505445957183838\n",
      "epoch: 7 step: 207, loss is 1.5179411172866821\n",
      "epoch: 7 step: 208, loss is 1.6173243522644043\n",
      "epoch: 7 step: 209, loss is 1.5088200569152832\n",
      "epoch: 7 step: 210, loss is 1.4669373035430908\n",
      "epoch: 7 step: 211, loss is 1.509019374847412\n",
      "epoch: 7 step: 212, loss is 1.515617847442627\n",
      "epoch: 7 step: 213, loss is 1.588252305984497\n",
      "epoch: 7 step: 214, loss is 1.4769995212554932\n",
      "epoch: 7 step: 215, loss is 1.4487775564193726\n",
      "epoch: 7 step: 216, loss is 1.418100118637085\n",
      "epoch: 7 step: 217, loss is 1.4640591144561768\n",
      "epoch: 7 step: 218, loss is 1.4784655570983887\n",
      "epoch: 7 step: 219, loss is 1.5304230451583862\n",
      "epoch: 7 step: 220, loss is 1.5265545845031738\n",
      "epoch: 7 step: 221, loss is 1.421887993812561\n",
      "epoch: 7 step: 222, loss is 1.4931303262710571\n",
      "epoch: 7 step: 223, loss is 1.3621628284454346\n",
      "epoch: 7 step: 224, loss is 1.5272208452224731\n",
      "epoch: 7 step: 225, loss is 1.5057622194290161\n",
      "epoch: 7 step: 226, loss is 1.4416744709014893\n",
      "epoch: 7 step: 227, loss is 1.4851229190826416\n",
      "epoch: 7 step: 228, loss is 1.397284984588623\n",
      "epoch: 7 step: 229, loss is 1.6067861318588257\n",
      "epoch: 7 step: 230, loss is 1.4668687582015991\n",
      "epoch: 7 step: 231, loss is 1.5099148750305176\n",
      "epoch: 7 step: 232, loss is 1.3832366466522217\n",
      "epoch: 7 step: 233, loss is 1.489577293395996\n",
      "epoch: 7 step: 234, loss is 1.4375826120376587\n",
      "epoch: 7 step: 235, loss is 1.51701021194458\n",
      "epoch: 7 step: 236, loss is 1.471653699874878\n",
      "epoch: 7 step: 237, loss is 1.5333917140960693\n",
      "epoch: 7 step: 238, loss is 1.50908625125885\n",
      "epoch: 7 step: 239, loss is 1.4091105461120605\n",
      "epoch: 7 step: 240, loss is 1.4444016218185425\n",
      "epoch: 7 step: 241, loss is 1.4461042881011963\n",
      "epoch: 7 step: 242, loss is 1.4514703750610352\n",
      "epoch: 7 step: 243, loss is 1.5625706911087036\n",
      "epoch: 7 step: 244, loss is 1.4126678705215454\n",
      "epoch: 7 step: 245, loss is 1.542110800743103\n",
      "epoch: 7 step: 246, loss is 1.4554040431976318\n",
      "epoch: 7 step: 247, loss is 1.487863302230835\n",
      "epoch: 7 step: 248, loss is 1.4793486595153809\n",
      "epoch: 7 step: 249, loss is 1.4493279457092285\n",
      "epoch: 7 step: 250, loss is 1.5458934307098389\n",
      "epoch: 7 step: 251, loss is 1.5679740905761719\n",
      "epoch: 7 step: 252, loss is 1.3913421630859375\n",
      "epoch: 7 step: 253, loss is 1.4727951288223267\n",
      "epoch: 7 step: 254, loss is 1.5493686199188232\n",
      "epoch: 7 step: 255, loss is 1.416840672492981\n",
      "epoch: 7 step: 256, loss is 1.5186264514923096\n",
      "epoch: 7 step: 257, loss is 1.5601401329040527\n",
      "epoch: 7 step: 258, loss is 1.5075466632843018\n",
      "epoch: 7 step: 259, loss is 1.584505558013916\n",
      "epoch: 7 step: 260, loss is 1.4371029138565063\n",
      "epoch: 7 step: 261, loss is 1.412757158279419\n",
      "epoch: 7 step: 262, loss is 1.4331610202789307\n",
      "epoch: 7 step: 263, loss is 1.5817545652389526\n",
      "epoch: 7 step: 264, loss is 1.5437285900115967\n",
      "epoch: 7 step: 265, loss is 1.4478529691696167\n",
      "epoch: 7 step: 266, loss is 1.4810731410980225\n",
      "epoch: 7 step: 267, loss is 1.4423024654388428\n",
      "epoch: 7 step: 268, loss is 1.5125641822814941\n",
      "epoch: 7 step: 269, loss is 1.3632920980453491\n",
      "epoch: 7 step: 270, loss is 1.6025428771972656\n",
      "epoch: 7 step: 271, loss is 1.3613542318344116\n",
      "epoch: 7 step: 272, loss is 1.5766817331314087\n",
      "epoch: 7 step: 273, loss is 1.483181118965149\n",
      "epoch: 7 step: 274, loss is 1.5037994384765625\n",
      "epoch: 7 step: 275, loss is 1.5284103155136108\n",
      "epoch: 7 step: 276, loss is 1.5428112745285034\n",
      "epoch: 7 step: 277, loss is 1.4651211500167847\n",
      "epoch: 7 step: 278, loss is 1.4747345447540283\n",
      "epoch: 7 step: 279, loss is 1.561877965927124\n",
      "epoch: 7 step: 280, loss is 1.4548630714416504\n",
      "epoch: 7 step: 281, loss is 1.4926663637161255\n",
      "epoch: 7 step: 282, loss is 1.6009266376495361\n",
      "epoch: 7 step: 283, loss is 1.462247610092163\n",
      "epoch: 7 step: 284, loss is 1.555004358291626\n",
      "epoch: 7 step: 285, loss is 1.475102424621582\n",
      "epoch: 7 step: 286, loss is 1.613940954208374\n",
      "epoch: 7 step: 287, loss is 1.5329656600952148\n",
      "epoch: 7 step: 288, loss is 1.4940003156661987\n",
      "epoch: 7 step: 289, loss is 1.5677109956741333\n",
      "epoch: 7 step: 290, loss is 1.5245730876922607\n",
      "epoch: 7 step: 291, loss is 1.540090560913086\n",
      "epoch: 7 step: 292, loss is 1.4055551290512085\n",
      "epoch: 7 step: 293, loss is 1.4572745561599731\n",
      "epoch: 7 step: 294, loss is 1.4933420419692993\n",
      "epoch: 7 step: 295, loss is 1.4991881847381592\n",
      "epoch: 7 step: 296, loss is 1.5287930965423584\n",
      "epoch: 7 step: 297, loss is 1.404765009880066\n",
      "epoch: 7 step: 298, loss is 1.4084144830703735\n",
      "epoch: 7 step: 299, loss is 1.4368764162063599\n",
      "epoch: 7 step: 300, loss is 1.5585243701934814\n",
      "epoch: 7 step: 301, loss is 1.3627371788024902\n",
      "epoch: 7 step: 302, loss is 1.5211904048919678\n",
      "epoch: 7 step: 303, loss is 1.4836838245391846\n",
      "epoch: 7 step: 304, loss is 1.5390644073486328\n",
      "epoch: 7 step: 305, loss is 1.475705862045288\n",
      "epoch: 7 step: 306, loss is 1.3663243055343628\n",
      "epoch: 7 step: 307, loss is 1.4403839111328125\n",
      "epoch: 7 step: 308, loss is 1.3537113666534424\n",
      "epoch: 7 step: 309, loss is 1.4276820421218872\n",
      "epoch: 7 step: 310, loss is 1.5886188745498657\n",
      "epoch: 7 step: 311, loss is 1.506626009941101\n",
      "epoch: 7 step: 312, loss is 1.527119517326355\n",
      "epoch: 7 step: 313, loss is 1.5883768796920776\n",
      "epoch: 7 step: 314, loss is 1.4587047100067139\n",
      "epoch: 7 step: 315, loss is 1.4578003883361816\n",
      "epoch: 7 step: 316, loss is 1.5285500288009644\n",
      "epoch: 7 step: 317, loss is 1.6400444507598877\n",
      "epoch: 7 step: 318, loss is 1.5775978565216064\n",
      "epoch: 7 step: 319, loss is 1.512736201286316\n",
      "epoch: 7 step: 320, loss is 1.568980097770691\n",
      "epoch: 7 step: 321, loss is 1.5527974367141724\n",
      "epoch: 7 step: 322, loss is 1.528021216392517\n",
      "epoch: 7 step: 323, loss is 1.6745707988739014\n",
      "epoch: 7 step: 324, loss is 1.3616366386413574\n",
      "epoch: 7 step: 325, loss is 1.569129228591919\n",
      "epoch: 7 step: 326, loss is 1.383955717086792\n",
      "epoch: 7 step: 327, loss is 1.4337217807769775\n",
      "epoch: 7 step: 328, loss is 1.5363781452178955\n",
      "epoch: 7 step: 329, loss is 1.5348013639450073\n",
      "epoch: 7 step: 330, loss is 1.442512035369873\n",
      "epoch: 7 step: 331, loss is 1.524972915649414\n",
      "epoch: 7 step: 332, loss is 1.6221030950546265\n",
      "epoch: 7 step: 333, loss is 1.5084056854248047\n",
      "epoch: 7 step: 334, loss is 1.4593896865844727\n",
      "epoch: 7 step: 335, loss is 1.3819448947906494\n",
      "epoch: 7 step: 336, loss is 1.519800066947937\n",
      "epoch: 7 step: 337, loss is 1.4176756143569946\n",
      "epoch: 7 step: 338, loss is 1.519759178161621\n",
      "epoch: 7 step: 339, loss is 1.5828800201416016\n",
      "epoch: 7 step: 340, loss is 1.4606938362121582\n",
      "epoch: 7 step: 341, loss is 1.5520426034927368\n",
      "epoch: 7 step: 342, loss is 1.5776469707489014\n",
      "epoch: 7 step: 343, loss is 1.4535599946975708\n",
      "epoch: 7 step: 344, loss is 1.5047929286956787\n",
      "epoch: 7 step: 345, loss is 1.7460720539093018\n",
      "epoch: 7 step: 346, loss is 1.721332311630249\n",
      "epoch: 7 step: 347, loss is 1.5524746179580688\n",
      "epoch: 7 step: 348, loss is 1.4840816259384155\n",
      "epoch: 7 step: 349, loss is 1.4170122146606445\n",
      "epoch: 7 step: 350, loss is 1.60019052028656\n",
      "epoch: 7 step: 351, loss is 1.4628171920776367\n",
      "epoch: 7 step: 352, loss is 1.63743257522583\n",
      "epoch: 7 step: 353, loss is 1.5089370012283325\n",
      "epoch: 7 step: 354, loss is 1.5284347534179688\n",
      "epoch: 7 step: 355, loss is 1.5683097839355469\n",
      "epoch: 7 step: 356, loss is 1.591042160987854\n",
      "epoch: 7 step: 357, loss is 1.5550949573516846\n",
      "epoch: 7 step: 358, loss is 1.3622891902923584\n",
      "epoch: 7 step: 359, loss is 1.64197838306427\n",
      "epoch: 7 step: 360, loss is 1.4686387777328491\n",
      "epoch: 7 step: 361, loss is 1.4935840368270874\n",
      "epoch: 7 step: 362, loss is 1.6327656507492065\n",
      "epoch: 7 step: 363, loss is 1.5468339920043945\n",
      "epoch: 7 step: 364, loss is 1.4712644815444946\n",
      "epoch: 7 step: 365, loss is 1.5008187294006348\n",
      "epoch: 7 step: 366, loss is 1.4647722244262695\n",
      "epoch: 7 step: 367, loss is 1.5448557138442993\n",
      "epoch: 7 step: 368, loss is 1.522481918334961\n",
      "epoch: 7 step: 369, loss is 1.3293070793151855\n",
      "epoch: 7 step: 370, loss is 1.6013524532318115\n",
      "epoch: 7 step: 371, loss is 1.4404761791229248\n",
      "epoch: 7 step: 372, loss is 1.5537519454956055\n",
      "epoch: 7 step: 373, loss is 1.495922565460205\n",
      "epoch: 7 step: 374, loss is 1.5356340408325195\n",
      "epoch: 7 step: 375, loss is 1.4586100578308105\n",
      "epoch: 7 step: 376, loss is 1.5116429328918457\n",
      "epoch: 7 step: 377, loss is 1.5786906480789185\n",
      "epoch: 7 step: 378, loss is 1.5109188556671143\n",
      "epoch: 7 step: 379, loss is 1.4975146055221558\n",
      "epoch: 7 step: 380, loss is 1.5376402139663696\n",
      "epoch: 7 step: 381, loss is 1.5154435634613037\n",
      "epoch: 7 step: 382, loss is 1.419454574584961\n",
      "epoch: 7 step: 383, loss is 1.496876835823059\n",
      "epoch: 7 step: 384, loss is 1.4247627258300781\n",
      "epoch: 7 step: 385, loss is 1.4251362085342407\n",
      "epoch: 7 step: 386, loss is 1.4928667545318604\n",
      "epoch: 7 step: 387, loss is 1.4847559928894043\n",
      "epoch: 7 step: 388, loss is 1.38694167137146\n",
      "epoch: 7 step: 389, loss is 1.379678726196289\n",
      "epoch: 7 step: 390, loss is 1.4174227714538574\n",
      "Train epoch time: 159683.080 ms, per step time: 409.444 ms\n",
      "epoch: 8 step: 1, loss is 1.4021053314208984\n",
      "epoch: 8 step: 2, loss is 1.418152093887329\n",
      "epoch: 8 step: 3, loss is 1.4540166854858398\n",
      "epoch: 8 step: 4, loss is 1.5124719142913818\n",
      "epoch: 8 step: 5, loss is 1.3753772974014282\n",
      "epoch: 8 step: 6, loss is 1.4239518642425537\n",
      "epoch: 8 step: 7, loss is 1.507587194442749\n",
      "epoch: 8 step: 8, loss is 1.4951415061950684\n",
      "epoch: 8 step: 9, loss is 1.5206490755081177\n",
      "epoch: 8 step: 10, loss is 1.6003267765045166\n",
      "epoch: 8 step: 11, loss is 1.4286526441574097\n",
      "epoch: 8 step: 12, loss is 1.5782272815704346\n",
      "epoch: 8 step: 13, loss is 1.4840881824493408\n",
      "epoch: 8 step: 14, loss is 1.285062551498413\n",
      "epoch: 8 step: 15, loss is 1.274904489517212\n",
      "epoch: 8 step: 16, loss is 1.3502534627914429\n",
      "epoch: 8 step: 17, loss is 1.561945915222168\n",
      "epoch: 8 step: 18, loss is 1.5151305198669434\n",
      "epoch: 8 step: 19, loss is 1.4600125551223755\n",
      "epoch: 8 step: 20, loss is 1.482060194015503\n",
      "epoch: 8 step: 21, loss is 1.5818203687667847\n",
      "epoch: 8 step: 22, loss is 1.5218559503555298\n",
      "epoch: 8 step: 23, loss is 1.536750316619873\n",
      "epoch: 8 step: 24, loss is 1.458404541015625\n",
      "epoch: 8 step: 25, loss is 1.4933923482894897\n",
      "epoch: 8 step: 26, loss is 1.5272266864776611\n",
      "epoch: 8 step: 27, loss is 1.5347208976745605\n",
      "epoch: 8 step: 28, loss is 1.621921420097351\n",
      "epoch: 8 step: 29, loss is 1.483141541481018\n",
      "epoch: 8 step: 30, loss is 1.4113638401031494\n",
      "epoch: 8 step: 31, loss is 1.4093177318572998\n",
      "epoch: 8 step: 32, loss is 1.5557324886322021\n",
      "epoch: 8 step: 33, loss is 1.4316669702529907\n",
      "epoch: 8 step: 34, loss is 1.5445168018341064\n",
      "epoch: 8 step: 35, loss is 1.5192265510559082\n",
      "epoch: 8 step: 36, loss is 1.6617560386657715\n",
      "epoch: 8 step: 37, loss is 1.4617359638214111\n",
      "epoch: 8 step: 38, loss is 1.4582266807556152\n",
      "epoch: 8 step: 39, loss is 1.4725768566131592\n",
      "epoch: 8 step: 40, loss is 1.4538530111312866\n",
      "epoch: 8 step: 41, loss is 1.5762665271759033\n",
      "epoch: 8 step: 42, loss is 1.532905101776123\n",
      "epoch: 8 step: 43, loss is 1.3784499168395996\n",
      "epoch: 8 step: 44, loss is 1.6016881465911865\n",
      "epoch: 8 step: 45, loss is 1.3764959573745728\n",
      "epoch: 8 step: 46, loss is 1.5307152271270752\n",
      "epoch: 8 step: 47, loss is 1.4339494705200195\n",
      "epoch: 8 step: 48, loss is 1.4334228038787842\n",
      "epoch: 8 step: 49, loss is 1.5302766561508179\n",
      "epoch: 8 step: 50, loss is 1.5759680271148682\n",
      "epoch: 8 step: 51, loss is 1.5109878778457642\n",
      "epoch: 8 step: 52, loss is 1.4950752258300781\n",
      "epoch: 8 step: 53, loss is 1.3836076259613037\n",
      "epoch: 8 step: 54, loss is 1.5716073513031006\n",
      "epoch: 8 step: 55, loss is 1.5253721475601196\n",
      "epoch: 8 step: 56, loss is 1.5527420043945312\n",
      "epoch: 8 step: 57, loss is 1.5232170820236206\n",
      "epoch: 8 step: 58, loss is 1.4871288537979126\n",
      "epoch: 8 step: 59, loss is 1.3826838731765747\n",
      "epoch: 8 step: 60, loss is 1.392909049987793\n",
      "epoch: 8 step: 61, loss is 1.4355658292770386\n",
      "epoch: 8 step: 62, loss is 1.4767427444458008\n",
      "epoch: 8 step: 63, loss is 1.4565362930297852\n",
      "epoch: 8 step: 64, loss is 1.5016958713531494\n",
      "epoch: 8 step: 65, loss is 1.3074761629104614\n",
      "epoch: 8 step: 66, loss is 1.4973044395446777\n",
      "epoch: 8 step: 67, loss is 1.6684060096740723\n",
      "epoch: 8 step: 68, loss is 1.437247395515442\n",
      "epoch: 8 step: 69, loss is 1.5028159618377686\n",
      "epoch: 8 step: 70, loss is 1.50583815574646\n",
      "epoch: 8 step: 71, loss is 1.3971365690231323\n",
      "epoch: 8 step: 72, loss is 1.5170763731002808\n",
      "epoch: 8 step: 73, loss is 1.4047890901565552\n",
      "epoch: 8 step: 74, loss is 1.5041589736938477\n",
      "epoch: 8 step: 75, loss is 1.4423331022262573\n",
      "epoch: 8 step: 76, loss is 1.499335527420044\n",
      "epoch: 8 step: 77, loss is 1.5177475214004517\n",
      "epoch: 8 step: 78, loss is 1.4209891557693481\n",
      "epoch: 8 step: 79, loss is 1.4602065086364746\n",
      "epoch: 8 step: 80, loss is 1.5699857473373413\n",
      "epoch: 8 step: 81, loss is 1.4306271076202393\n",
      "epoch: 8 step: 82, loss is 1.370821237564087\n",
      "epoch: 8 step: 83, loss is 1.519216775894165\n",
      "epoch: 8 step: 84, loss is 1.4927006959915161\n",
      "epoch: 8 step: 85, loss is 1.601792812347412\n",
      "epoch: 8 step: 86, loss is 1.4228264093399048\n",
      "epoch: 8 step: 87, loss is 1.4860901832580566\n",
      "epoch: 8 step: 88, loss is 1.375718116760254\n",
      "epoch: 8 step: 89, loss is 1.4277091026306152\n",
      "epoch: 8 step: 90, loss is 1.4369226694107056\n",
      "epoch: 8 step: 91, loss is 1.417037010192871\n",
      "epoch: 8 step: 92, loss is 1.4136844873428345\n",
      "epoch: 8 step: 93, loss is 1.3250415325164795\n",
      "epoch: 8 step: 94, loss is 1.5668644905090332\n",
      "epoch: 8 step: 95, loss is 1.450277328491211\n",
      "epoch: 8 step: 96, loss is 1.603899359703064\n",
      "epoch: 8 step: 97, loss is 1.505083441734314\n",
      "epoch: 8 step: 98, loss is 1.5150947570800781\n",
      "epoch: 8 step: 99, loss is 1.4023168087005615\n",
      "epoch: 8 step: 100, loss is 1.559932827949524\n",
      "epoch: 8 step: 101, loss is 1.447965383529663\n",
      "epoch: 8 step: 102, loss is 1.3735661506652832\n",
      "epoch: 8 step: 103, loss is 1.452291488647461\n",
      "epoch: 8 step: 104, loss is 1.5205024480819702\n",
      "epoch: 8 step: 105, loss is 1.5254466533660889\n",
      "epoch: 8 step: 106, loss is 1.4873650074005127\n",
      "epoch: 8 step: 107, loss is 1.5303101539611816\n",
      "epoch: 8 step: 108, loss is 1.428245186805725\n",
      "epoch: 8 step: 109, loss is 1.5557256937026978\n",
      "epoch: 8 step: 110, loss is 1.5470058917999268\n",
      "epoch: 8 step: 111, loss is 1.4152907133102417\n",
      "epoch: 8 step: 112, loss is 1.4382702112197876\n",
      "epoch: 8 step: 113, loss is 1.7290247678756714\n",
      "epoch: 8 step: 114, loss is 1.484397292137146\n",
      "epoch: 8 step: 115, loss is 1.427494764328003\n",
      "epoch: 8 step: 116, loss is 1.4056482315063477\n",
      "epoch: 8 step: 117, loss is 1.4065256118774414\n",
      "epoch: 8 step: 118, loss is 1.3881454467773438\n",
      "epoch: 8 step: 119, loss is 1.522014856338501\n",
      "epoch: 8 step: 120, loss is 1.488077163696289\n",
      "epoch: 8 step: 121, loss is 1.5175387859344482\n",
      "epoch: 8 step: 122, loss is 1.460744857788086\n",
      "epoch: 8 step: 123, loss is 1.5394785404205322\n",
      "epoch: 8 step: 124, loss is 1.4104223251342773\n",
      "epoch: 8 step: 125, loss is 1.4344370365142822\n",
      "epoch: 8 step: 126, loss is 1.5728267431259155\n",
      "epoch: 8 step: 127, loss is 1.5339233875274658\n",
      "epoch: 8 step: 128, loss is 1.293635368347168\n",
      "epoch: 8 step: 129, loss is 1.3881927728652954\n",
      "epoch: 8 step: 130, loss is 1.487863540649414\n",
      "epoch: 8 step: 131, loss is 1.5161054134368896\n",
      "epoch: 8 step: 132, loss is 1.4421640634536743\n",
      "epoch: 8 step: 133, loss is 1.4992575645446777\n",
      "epoch: 8 step: 134, loss is 1.4593548774719238\n",
      "epoch: 8 step: 135, loss is 1.4480445384979248\n",
      "epoch: 8 step: 136, loss is 1.3667900562286377\n",
      "epoch: 8 step: 137, loss is 1.5573434829711914\n",
      "epoch: 8 step: 138, loss is 1.3951386213302612\n",
      "epoch: 8 step: 139, loss is 1.4289357662200928\n",
      "epoch: 8 step: 140, loss is 1.384069800376892\n",
      "epoch: 8 step: 141, loss is 1.4391639232635498\n",
      "epoch: 8 step: 142, loss is 1.4411122798919678\n",
      "epoch: 8 step: 143, loss is 1.609886884689331\n",
      "epoch: 8 step: 144, loss is 1.532867431640625\n",
      "epoch: 8 step: 145, loss is 1.4054985046386719\n",
      "epoch: 8 step: 146, loss is 1.4147603511810303\n",
      "epoch: 8 step: 147, loss is 1.4520673751831055\n",
      "epoch: 8 step: 148, loss is 1.4641729593276978\n",
      "epoch: 8 step: 149, loss is 1.3314273357391357\n",
      "epoch: 8 step: 150, loss is 1.4898289442062378\n",
      "epoch: 8 step: 151, loss is 1.331744909286499\n",
      "epoch: 8 step: 152, loss is 1.5991729497909546\n",
      "epoch: 8 step: 153, loss is 1.4885650873184204\n",
      "epoch: 8 step: 154, loss is 1.4530301094055176\n",
      "epoch: 8 step: 155, loss is 1.6018891334533691\n",
      "epoch: 8 step: 156, loss is 1.451012372970581\n",
      "epoch: 8 step: 157, loss is 1.5910699367523193\n",
      "epoch: 8 step: 158, loss is 1.649347186088562\n",
      "epoch: 8 step: 159, loss is 1.5264372825622559\n",
      "epoch: 8 step: 160, loss is 1.3279249668121338\n",
      "epoch: 8 step: 161, loss is 1.5184154510498047\n",
      "epoch: 8 step: 162, loss is 1.4623481035232544\n",
      "epoch: 8 step: 163, loss is 1.4803869724273682\n",
      "epoch: 8 step: 164, loss is 1.4186358451843262\n",
      "epoch: 8 step: 165, loss is 1.4483420848846436\n",
      "epoch: 8 step: 166, loss is 1.314813256263733\n",
      "epoch: 8 step: 167, loss is 1.430216670036316\n",
      "epoch: 8 step: 168, loss is 1.593386173248291\n",
      "epoch: 8 step: 169, loss is 1.362602949142456\n",
      "epoch: 8 step: 170, loss is 1.4039664268493652\n",
      "epoch: 8 step: 171, loss is 1.528160572052002\n",
      "epoch: 8 step: 172, loss is 1.4450135231018066\n",
      "epoch: 8 step: 173, loss is 1.4999356269836426\n",
      "epoch: 8 step: 174, loss is 1.4740548133850098\n",
      "epoch: 8 step: 175, loss is 1.425926923751831\n",
      "epoch: 8 step: 176, loss is 1.3551347255706787\n",
      "epoch: 8 step: 177, loss is 1.4293813705444336\n",
      "epoch: 8 step: 178, loss is 1.438087821006775\n",
      "epoch: 8 step: 179, loss is 1.4821535348892212\n",
      "epoch: 8 step: 180, loss is 1.4145153760910034\n",
      "epoch: 8 step: 181, loss is 1.5034565925598145\n",
      "epoch: 8 step: 182, loss is 1.3814451694488525\n",
      "epoch: 8 step: 183, loss is 1.536637306213379\n",
      "epoch: 8 step: 184, loss is 1.4639581441879272\n",
      "epoch: 8 step: 185, loss is 1.4842219352722168\n",
      "epoch: 8 step: 186, loss is 1.4948469400405884\n",
      "epoch: 8 step: 187, loss is 1.4040346145629883\n",
      "epoch: 8 step: 188, loss is 1.4185435771942139\n",
      "epoch: 8 step: 189, loss is 1.489573359489441\n",
      "epoch: 8 step: 190, loss is 1.3640828132629395\n",
      "epoch: 8 step: 191, loss is 1.5305116176605225\n",
      "epoch: 8 step: 192, loss is 1.5159341096878052\n",
      "epoch: 8 step: 193, loss is 1.44093656539917\n",
      "epoch: 8 step: 194, loss is 1.3523242473602295\n",
      "epoch: 8 step: 195, loss is 1.5900909900665283\n",
      "epoch: 8 step: 196, loss is 1.3186314105987549\n",
      "epoch: 8 step: 197, loss is 1.462524652481079\n",
      "epoch: 8 step: 198, loss is 1.6166908740997314\n",
      "epoch: 8 step: 199, loss is 1.3519541025161743\n",
      "epoch: 8 step: 200, loss is 1.4520049095153809\n",
      "epoch: 8 step: 201, loss is 1.483309268951416\n",
      "epoch: 8 step: 202, loss is 1.4538387060165405\n",
      "epoch: 8 step: 203, loss is 1.472192406654358\n",
      "epoch: 8 step: 204, loss is 1.3745731115341187\n",
      "epoch: 8 step: 205, loss is 1.4926173686981201\n",
      "epoch: 8 step: 206, loss is 1.3263883590698242\n",
      "epoch: 8 step: 207, loss is 1.551303505897522\n",
      "epoch: 8 step: 208, loss is 1.4411283731460571\n",
      "epoch: 8 step: 209, loss is 1.463823676109314\n",
      "epoch: 8 step: 210, loss is 1.4577275514602661\n",
      "epoch: 8 step: 211, loss is 1.328372597694397\n",
      "epoch: 8 step: 212, loss is 1.4678161144256592\n",
      "epoch: 8 step: 213, loss is 1.410888910293579\n",
      "epoch: 8 step: 214, loss is 1.4120723009109497\n",
      "epoch: 8 step: 215, loss is 1.4377305507659912\n",
      "epoch: 8 step: 216, loss is 1.4617499113082886\n",
      "epoch: 8 step: 217, loss is 1.5817081928253174\n",
      "epoch: 8 step: 218, loss is 1.4273957014083862\n",
      "epoch: 8 step: 219, loss is 1.522106409072876\n",
      "epoch: 8 step: 220, loss is 1.5534676313400269\n",
      "epoch: 8 step: 221, loss is 1.3997200727462769\n",
      "epoch: 8 step: 222, loss is 1.430558681488037\n",
      "epoch: 8 step: 223, loss is 1.3658452033996582\n",
      "epoch: 8 step: 224, loss is 1.5321125984191895\n",
      "epoch: 8 step: 225, loss is 1.507480502128601\n",
      "epoch: 8 step: 226, loss is 1.498950481414795\n",
      "epoch: 8 step: 227, loss is 1.561760663986206\n",
      "epoch: 8 step: 228, loss is 1.4057209491729736\n",
      "epoch: 8 step: 229, loss is 1.5500091314315796\n",
      "epoch: 8 step: 230, loss is 1.508884310722351\n",
      "epoch: 8 step: 231, loss is 1.3664042949676514\n",
      "epoch: 8 step: 232, loss is 1.5772569179534912\n",
      "epoch: 8 step: 233, loss is 1.593772053718567\n",
      "epoch: 8 step: 234, loss is 1.4460617303848267\n",
      "epoch: 8 step: 235, loss is 1.3865034580230713\n",
      "epoch: 8 step: 236, loss is 1.5315022468566895\n",
      "epoch: 8 step: 237, loss is 1.438228726387024\n",
      "epoch: 8 step: 238, loss is 1.504820466041565\n",
      "epoch: 8 step: 239, loss is 1.5492058992385864\n",
      "epoch: 8 step: 240, loss is 1.7535786628723145\n",
      "epoch: 8 step: 241, loss is 1.4553152322769165\n",
      "epoch: 8 step: 242, loss is 1.5447510480880737\n",
      "epoch: 8 step: 243, loss is 1.4287559986114502\n",
      "epoch: 8 step: 244, loss is 1.5235671997070312\n",
      "epoch: 8 step: 245, loss is 1.5243072509765625\n",
      "epoch: 8 step: 246, loss is 1.4404261112213135\n",
      "epoch: 8 step: 247, loss is 1.4796122312545776\n",
      "epoch: 8 step: 248, loss is 1.4665855169296265\n",
      "epoch: 8 step: 249, loss is 1.4245967864990234\n",
      "epoch: 8 step: 250, loss is 1.4246214628219604\n",
      "epoch: 8 step: 251, loss is 1.3796577453613281\n",
      "epoch: 8 step: 252, loss is 1.541312575340271\n",
      "epoch: 8 step: 253, loss is 1.3715639114379883\n",
      "epoch: 8 step: 254, loss is 1.429457426071167\n",
      "epoch: 8 step: 255, loss is 1.6428273916244507\n",
      "epoch: 8 step: 256, loss is 1.4999282360076904\n",
      "epoch: 8 step: 257, loss is 1.4772074222564697\n",
      "epoch: 8 step: 258, loss is 1.3018934726715088\n",
      "epoch: 8 step: 259, loss is 1.4056555032730103\n",
      "epoch: 8 step: 260, loss is 1.4329088926315308\n",
      "epoch: 8 step: 261, loss is 1.5439937114715576\n",
      "epoch: 8 step: 262, loss is 1.535062313079834\n",
      "epoch: 8 step: 263, loss is 1.415586233139038\n",
      "epoch: 8 step: 264, loss is 1.5362638235092163\n",
      "epoch: 8 step: 265, loss is 1.5225074291229248\n",
      "epoch: 8 step: 266, loss is 1.5525565147399902\n",
      "epoch: 8 step: 267, loss is 1.478561282157898\n",
      "epoch: 8 step: 268, loss is 1.4610209465026855\n",
      "epoch: 8 step: 269, loss is 1.5315321683883667\n",
      "epoch: 8 step: 270, loss is 1.5820270776748657\n",
      "epoch: 8 step: 271, loss is 1.4011657238006592\n",
      "epoch: 8 step: 272, loss is 1.468673825263977\n",
      "epoch: 8 step: 273, loss is 1.4644299745559692\n",
      "epoch: 8 step: 274, loss is 1.4915968179702759\n",
      "epoch: 8 step: 275, loss is 1.4400181770324707\n",
      "epoch: 8 step: 276, loss is 1.357332706451416\n",
      "epoch: 8 step: 277, loss is 1.5471229553222656\n",
      "epoch: 8 step: 278, loss is 1.516358733177185\n",
      "epoch: 8 step: 279, loss is 1.5390254259109497\n",
      "epoch: 8 step: 280, loss is 1.5347821712493896\n",
      "epoch: 8 step: 281, loss is 1.5688616037368774\n",
      "epoch: 8 step: 282, loss is 1.3471404314041138\n",
      "epoch: 8 step: 283, loss is 1.5750732421875\n",
      "epoch: 8 step: 284, loss is 1.4173078536987305\n",
      "epoch: 8 step: 285, loss is 1.5259255170822144\n",
      "epoch: 8 step: 286, loss is 1.5400173664093018\n",
      "epoch: 8 step: 287, loss is 1.4067461490631104\n",
      "epoch: 8 step: 288, loss is 1.5027668476104736\n",
      "epoch: 8 step: 289, loss is 1.4144560098648071\n",
      "epoch: 8 step: 290, loss is 1.3604286909103394\n",
      "epoch: 8 step: 291, loss is 1.3730921745300293\n",
      "epoch: 8 step: 292, loss is 1.4263416528701782\n",
      "epoch: 8 step: 293, loss is 1.5093474388122559\n",
      "epoch: 8 step: 294, loss is 1.5006818771362305\n",
      "epoch: 8 step: 295, loss is 1.5066982507705688\n",
      "epoch: 8 step: 296, loss is 1.6344687938690186\n",
      "epoch: 8 step: 297, loss is 1.4216837882995605\n",
      "epoch: 8 step: 298, loss is 1.3647886514663696\n",
      "epoch: 8 step: 299, loss is 1.4367821216583252\n",
      "epoch: 8 step: 300, loss is 1.576464295387268\n",
      "epoch: 8 step: 301, loss is 1.4775571823120117\n",
      "epoch: 8 step: 302, loss is 1.3826572895050049\n",
      "epoch: 8 step: 303, loss is 1.366113543510437\n",
      "epoch: 8 step: 304, loss is 1.3474509716033936\n",
      "epoch: 8 step: 305, loss is 1.4911115169525146\n",
      "epoch: 8 step: 306, loss is 1.4552218914031982\n",
      "epoch: 8 step: 307, loss is 1.4825694561004639\n",
      "epoch: 8 step: 308, loss is 1.4273864030838013\n",
      "epoch: 8 step: 309, loss is 1.4081647396087646\n",
      "epoch: 8 step: 310, loss is 1.4424480199813843\n",
      "epoch: 8 step: 311, loss is 1.4233698844909668\n",
      "epoch: 8 step: 312, loss is 1.3216952085494995\n",
      "epoch: 8 step: 313, loss is 1.527234435081482\n",
      "epoch: 8 step: 314, loss is 1.5271234512329102\n",
      "epoch: 8 step: 315, loss is 1.4686611890792847\n",
      "epoch: 8 step: 316, loss is 1.407480001449585\n",
      "epoch: 8 step: 317, loss is 1.487440586090088\n",
      "epoch: 8 step: 318, loss is 1.4305191040039062\n",
      "epoch: 8 step: 319, loss is 1.5851491689682007\n",
      "epoch: 8 step: 320, loss is 1.4382202625274658\n",
      "epoch: 8 step: 321, loss is 1.4957261085510254\n",
      "epoch: 8 step: 322, loss is 1.4728758335113525\n",
      "epoch: 8 step: 323, loss is 1.46397066116333\n",
      "epoch: 8 step: 324, loss is 1.35401451587677\n",
      "epoch: 8 step: 325, loss is 1.4802677631378174\n",
      "epoch: 8 step: 326, loss is 1.4576939344406128\n",
      "epoch: 8 step: 327, loss is 1.5272685289382935\n",
      "epoch: 8 step: 328, loss is 1.4146407842636108\n",
      "epoch: 8 step: 329, loss is 1.4698376655578613\n",
      "epoch: 8 step: 330, loss is 1.4429948329925537\n",
      "epoch: 8 step: 331, loss is 1.5669244527816772\n",
      "epoch: 8 step: 332, loss is 1.5344505310058594\n",
      "epoch: 8 step: 333, loss is 1.4100470542907715\n",
      "epoch: 8 step: 334, loss is 1.4559906721115112\n",
      "epoch: 8 step: 335, loss is 1.495165467262268\n",
      "epoch: 8 step: 336, loss is 1.4980438947677612\n",
      "epoch: 8 step: 337, loss is 1.4295061826705933\n",
      "epoch: 8 step: 338, loss is 1.531524896621704\n",
      "epoch: 8 step: 339, loss is 1.56319260597229\n",
      "epoch: 8 step: 340, loss is 1.3818823099136353\n",
      "epoch: 8 step: 341, loss is 1.530118703842163\n",
      "epoch: 8 step: 342, loss is 1.484922170639038\n",
      "epoch: 8 step: 343, loss is 1.4500954151153564\n",
      "epoch: 8 step: 344, loss is 1.4573094844818115\n",
      "epoch: 8 step: 345, loss is 1.4346072673797607\n",
      "epoch: 8 step: 346, loss is 1.422975778579712\n",
      "epoch: 8 step: 347, loss is 1.4928127527236938\n",
      "epoch: 8 step: 348, loss is 1.4593108892440796\n",
      "epoch: 8 step: 349, loss is 1.4872854948043823\n",
      "epoch: 8 step: 350, loss is 1.6270756721496582\n",
      "epoch: 8 step: 351, loss is 1.470075249671936\n",
      "epoch: 8 step: 352, loss is 1.5129756927490234\n",
      "epoch: 8 step: 353, loss is 1.3481090068817139\n",
      "epoch: 8 step: 354, loss is 1.5362577438354492\n",
      "epoch: 8 step: 355, loss is 1.3849045038223267\n",
      "epoch: 8 step: 356, loss is 1.5898919105529785\n",
      "epoch: 8 step: 357, loss is 1.3267384767532349\n",
      "epoch: 8 step: 358, loss is 1.5004894733428955\n",
      "epoch: 8 step: 359, loss is 1.4620335102081299\n",
      "epoch: 8 step: 360, loss is 1.6215674877166748\n",
      "epoch: 8 step: 361, loss is 1.468297004699707\n",
      "epoch: 8 step: 362, loss is 1.4323958158493042\n",
      "epoch: 8 step: 363, loss is 1.4725360870361328\n",
      "epoch: 8 step: 364, loss is 1.4356077909469604\n",
      "epoch: 8 step: 365, loss is 1.4509690999984741\n",
      "epoch: 8 step: 366, loss is 1.4708446264266968\n",
      "epoch: 8 step: 367, loss is 1.4288667440414429\n",
      "epoch: 8 step: 368, loss is 1.476953387260437\n",
      "epoch: 8 step: 369, loss is 1.5084514617919922\n",
      "epoch: 8 step: 370, loss is 1.361833095550537\n",
      "epoch: 8 step: 371, loss is 1.3793751001358032\n",
      "epoch: 8 step: 372, loss is 1.4270557165145874\n",
      "epoch: 8 step: 373, loss is 1.3139890432357788\n",
      "epoch: 8 step: 374, loss is 1.5814015865325928\n",
      "epoch: 8 step: 375, loss is 1.4724631309509277\n",
      "epoch: 8 step: 376, loss is 1.366919994354248\n",
      "epoch: 8 step: 377, loss is 1.4392365217208862\n",
      "epoch: 8 step: 378, loss is 1.4423832893371582\n",
      "epoch: 8 step: 379, loss is 1.4045937061309814\n",
      "epoch: 8 step: 380, loss is 1.4666013717651367\n",
      "epoch: 8 step: 381, loss is 1.43149733543396\n",
      "epoch: 8 step: 382, loss is 1.5499497652053833\n",
      "epoch: 8 step: 383, loss is 1.508276343345642\n",
      "epoch: 8 step: 384, loss is 1.5079187154769897\n",
      "epoch: 8 step: 385, loss is 1.5343046188354492\n",
      "epoch: 8 step: 386, loss is 1.5443766117095947\n",
      "epoch: 8 step: 387, loss is 1.4475337266921997\n",
      "epoch: 8 step: 388, loss is 1.50088369846344\n",
      "epoch: 8 step: 389, loss is 1.4300310611724854\n",
      "epoch: 8 step: 390, loss is 1.5016725063323975\n",
      "Train epoch time: 157746.543 ms, per step time: 404.478 ms\n",
      "epoch: 9 step: 1, loss is 1.4737610816955566\n",
      "epoch: 9 step: 2, loss is 1.4278042316436768\n",
      "epoch: 9 step: 3, loss is 1.3433090448379517\n",
      "epoch: 9 step: 4, loss is 1.4596627950668335\n",
      "epoch: 9 step: 5, loss is 1.439853310585022\n",
      "epoch: 9 step: 6, loss is 1.498945951461792\n",
      "epoch: 9 step: 7, loss is 1.5045310258865356\n",
      "epoch: 9 step: 8, loss is 1.4318782091140747\n",
      "epoch: 9 step: 9, loss is 1.3060253858566284\n",
      "epoch: 9 step: 10, loss is 1.3605823516845703\n",
      "epoch: 9 step: 11, loss is 1.424958348274231\n",
      "epoch: 9 step: 12, loss is 1.4736911058425903\n",
      "epoch: 9 step: 13, loss is 1.4673426151275635\n",
      "epoch: 9 step: 14, loss is 1.4307501316070557\n",
      "epoch: 9 step: 15, loss is 1.5198590755462646\n",
      "epoch: 9 step: 16, loss is 1.4481258392333984\n",
      "epoch: 9 step: 17, loss is 1.5129189491271973\n",
      "epoch: 9 step: 18, loss is 1.409175157546997\n",
      "epoch: 9 step: 19, loss is 1.4405245780944824\n",
      "epoch: 9 step: 20, loss is 1.423136591911316\n",
      "epoch: 9 step: 21, loss is 1.5281381607055664\n",
      "epoch: 9 step: 22, loss is 1.418792963027954\n",
      "epoch: 9 step: 23, loss is 1.4245946407318115\n",
      "epoch: 9 step: 24, loss is 1.3920586109161377\n",
      "epoch: 9 step: 25, loss is 1.4974474906921387\n",
      "epoch: 9 step: 26, loss is 1.4051064252853394\n",
      "epoch: 9 step: 27, loss is 1.4884518384933472\n",
      "epoch: 9 step: 28, loss is 1.3441063165664673\n",
      "epoch: 9 step: 29, loss is 1.5447297096252441\n",
      "epoch: 9 step: 30, loss is 1.3605707883834839\n",
      "epoch: 9 step: 31, loss is 1.4170212745666504\n",
      "epoch: 9 step: 32, loss is 1.4296247959136963\n",
      "epoch: 9 step: 33, loss is 1.5135217905044556\n",
      "epoch: 9 step: 34, loss is 1.4305739402770996\n",
      "epoch: 9 step: 35, loss is 1.3685277700424194\n",
      "epoch: 9 step: 36, loss is 1.508664608001709\n",
      "epoch: 9 step: 37, loss is 1.4528888463974\n",
      "epoch: 9 step: 38, loss is 1.4154255390167236\n",
      "epoch: 9 step: 39, loss is 1.3477191925048828\n",
      "epoch: 9 step: 40, loss is 1.3499155044555664\n",
      "epoch: 9 step: 41, loss is 1.4446135759353638\n",
      "epoch: 9 step: 42, loss is 1.374720811843872\n",
      "epoch: 9 step: 43, loss is 1.3802900314331055\n",
      "epoch: 9 step: 44, loss is 1.3271082639694214\n",
      "epoch: 9 step: 45, loss is 1.627900242805481\n",
      "epoch: 9 step: 46, loss is 1.410124659538269\n",
      "epoch: 9 step: 47, loss is 1.3301392793655396\n",
      "epoch: 9 step: 48, loss is 1.430229902267456\n",
      "epoch: 9 step: 49, loss is 1.4973586797714233\n",
      "epoch: 9 step: 50, loss is 1.3262531757354736\n",
      "epoch: 9 step: 51, loss is 1.4065742492675781\n",
      "epoch: 9 step: 52, loss is 1.3974815607070923\n",
      "epoch: 9 step: 53, loss is 1.3823320865631104\n",
      "epoch: 9 step: 54, loss is 1.446428894996643\n",
      "epoch: 9 step: 55, loss is 1.413935899734497\n",
      "epoch: 9 step: 56, loss is 1.347010850906372\n",
      "epoch: 9 step: 57, loss is 1.3688092231750488\n",
      "epoch: 9 step: 58, loss is 1.4639663696289062\n",
      "epoch: 9 step: 59, loss is 1.4113579988479614\n",
      "epoch: 9 step: 60, loss is 1.439110279083252\n",
      "epoch: 9 step: 61, loss is 1.433319091796875\n",
      "epoch: 9 step: 62, loss is 1.4295196533203125\n",
      "epoch: 9 step: 63, loss is 1.3503121137619019\n",
      "epoch: 9 step: 64, loss is 1.3782570362091064\n",
      "epoch: 9 step: 65, loss is 1.41328763961792\n",
      "epoch: 9 step: 66, loss is 1.3562803268432617\n",
      "epoch: 9 step: 67, loss is 1.3336094617843628\n",
      "epoch: 9 step: 68, loss is 1.4686144590377808\n",
      "epoch: 9 step: 69, loss is 1.6094791889190674\n",
      "epoch: 9 step: 70, loss is 1.438040018081665\n",
      "epoch: 9 step: 71, loss is 1.3384562730789185\n",
      "epoch: 9 step: 72, loss is 1.5173652172088623\n",
      "epoch: 9 step: 73, loss is 1.3449835777282715\n",
      "epoch: 9 step: 74, loss is 1.4870092868804932\n",
      "epoch: 9 step: 75, loss is 1.4258731603622437\n",
      "epoch: 9 step: 76, loss is 1.5001115798950195\n",
      "epoch: 9 step: 77, loss is 1.43660569190979\n",
      "epoch: 9 step: 78, loss is 1.4318852424621582\n",
      "epoch: 9 step: 79, loss is 1.4850860834121704\n",
      "epoch: 9 step: 80, loss is 1.5460927486419678\n",
      "epoch: 9 step: 81, loss is 1.3809754848480225\n",
      "epoch: 9 step: 82, loss is 1.4866957664489746\n",
      "epoch: 9 step: 83, loss is 1.3978204727172852\n",
      "epoch: 9 step: 84, loss is 1.5002875328063965\n",
      "epoch: 9 step: 85, loss is 1.4669650793075562\n",
      "epoch: 9 step: 86, loss is 1.5215997695922852\n",
      "epoch: 9 step: 87, loss is 1.430224895477295\n",
      "epoch: 9 step: 88, loss is 1.4846357107162476\n",
      "epoch: 9 step: 89, loss is 1.4134106636047363\n",
      "epoch: 9 step: 90, loss is 1.5934712886810303\n",
      "epoch: 9 step: 91, loss is 1.4704101085662842\n",
      "epoch: 9 step: 92, loss is 1.5237336158752441\n",
      "epoch: 9 step: 93, loss is 1.3943700790405273\n",
      "epoch: 9 step: 94, loss is 1.4723501205444336\n",
      "epoch: 9 step: 95, loss is 1.3793301582336426\n",
      "epoch: 9 step: 96, loss is 1.53266179561615\n",
      "epoch: 9 step: 97, loss is 1.4291540384292603\n",
      "epoch: 9 step: 98, loss is 1.3981239795684814\n",
      "epoch: 9 step: 99, loss is 1.3846890926361084\n",
      "epoch: 9 step: 100, loss is 1.5738707780838013\n",
      "epoch: 9 step: 101, loss is 1.469835877418518\n",
      "epoch: 9 step: 102, loss is 1.4536786079406738\n",
      "epoch: 9 step: 103, loss is 1.4081811904907227\n",
      "epoch: 9 step: 104, loss is 1.3672351837158203\n",
      "epoch: 9 step: 105, loss is 1.3612709045410156\n",
      "epoch: 9 step: 106, loss is 1.351654052734375\n",
      "epoch: 9 step: 107, loss is 1.605619192123413\n",
      "epoch: 9 step: 108, loss is 1.4512830972671509\n",
      "epoch: 9 step: 109, loss is 1.4740337133407593\n",
      "epoch: 9 step: 110, loss is 1.4939358234405518\n",
      "epoch: 9 step: 111, loss is 1.4129109382629395\n",
      "epoch: 9 step: 112, loss is 1.3801429271697998\n",
      "epoch: 9 step: 113, loss is 1.4096951484680176\n",
      "epoch: 9 step: 114, loss is 1.27598237991333\n",
      "epoch: 9 step: 115, loss is 1.4641826152801514\n",
      "epoch: 9 step: 116, loss is 1.5019019842147827\n",
      "epoch: 9 step: 117, loss is 1.4745628833770752\n",
      "epoch: 9 step: 118, loss is 1.591984748840332\n",
      "epoch: 9 step: 119, loss is 1.439873456954956\n",
      "epoch: 9 step: 120, loss is 1.5554594993591309\n",
      "epoch: 9 step: 121, loss is 1.3241417407989502\n",
      "epoch: 9 step: 122, loss is 1.407827377319336\n",
      "epoch: 9 step: 123, loss is 1.4997920989990234\n",
      "epoch: 9 step: 124, loss is 1.396214246749878\n",
      "epoch: 9 step: 125, loss is 1.4074019193649292\n",
      "epoch: 9 step: 126, loss is 1.4165523052215576\n",
      "epoch: 9 step: 127, loss is 1.3898729085922241\n",
      "epoch: 9 step: 128, loss is 1.521071195602417\n",
      "epoch: 9 step: 129, loss is 1.3835422992706299\n",
      "epoch: 9 step: 130, loss is 1.4462090730667114\n",
      "epoch: 9 step: 131, loss is 1.6116796731948853\n",
      "epoch: 9 step: 132, loss is 1.4571421146392822\n",
      "epoch: 9 step: 133, loss is 1.5130970478057861\n",
      "epoch: 9 step: 134, loss is 1.595003604888916\n",
      "epoch: 9 step: 135, loss is 1.442062497138977\n",
      "epoch: 9 step: 136, loss is 1.597316026687622\n",
      "epoch: 9 step: 137, loss is 1.4058113098144531\n",
      "epoch: 9 step: 138, loss is 1.4921993017196655\n",
      "epoch: 9 step: 139, loss is 1.3614951372146606\n",
      "epoch: 9 step: 140, loss is 1.3964347839355469\n",
      "epoch: 9 step: 141, loss is 1.4625974893569946\n",
      "epoch: 9 step: 142, loss is 1.4101639986038208\n",
      "epoch: 9 step: 143, loss is 1.5569312572479248\n",
      "epoch: 9 step: 144, loss is 1.2582461833953857\n",
      "epoch: 9 step: 145, loss is 1.3637702465057373\n",
      "epoch: 9 step: 146, loss is 1.3591479063034058\n",
      "epoch: 9 step: 147, loss is 1.3777873516082764\n",
      "epoch: 9 step: 148, loss is 1.3689002990722656\n",
      "epoch: 9 step: 149, loss is 1.4059765338897705\n",
      "epoch: 9 step: 150, loss is 1.5525716543197632\n",
      "epoch: 9 step: 151, loss is 1.369553565979004\n",
      "epoch: 9 step: 152, loss is 1.3942127227783203\n",
      "epoch: 9 step: 153, loss is 1.3560525178909302\n",
      "epoch: 9 step: 154, loss is 1.320347785949707\n",
      "epoch: 9 step: 155, loss is 1.5048820972442627\n",
      "epoch: 9 step: 156, loss is 1.5824850797653198\n",
      "epoch: 9 step: 157, loss is 1.401780128479004\n",
      "epoch: 9 step: 158, loss is 1.4284034967422485\n",
      "epoch: 9 step: 159, loss is 1.5710811614990234\n",
      "epoch: 9 step: 160, loss is 1.3583762645721436\n",
      "epoch: 9 step: 161, loss is 1.4065576791763306\n",
      "epoch: 9 step: 162, loss is 1.4354751110076904\n",
      "epoch: 9 step: 163, loss is 1.45675528049469\n",
      "epoch: 9 step: 164, loss is 1.3373678922653198\n",
      "epoch: 9 step: 165, loss is 1.438033103942871\n",
      "epoch: 9 step: 166, loss is 1.370509147644043\n",
      "epoch: 9 step: 167, loss is 1.4345359802246094\n",
      "epoch: 9 step: 168, loss is 1.418947696685791\n",
      "epoch: 9 step: 169, loss is 1.3711154460906982\n",
      "epoch: 9 step: 170, loss is 1.3911677598953247\n",
      "epoch: 9 step: 171, loss is 1.4840196371078491\n",
      "epoch: 9 step: 172, loss is 1.3945162296295166\n",
      "epoch: 9 step: 173, loss is 1.4055063724517822\n",
      "epoch: 9 step: 174, loss is 1.392874002456665\n",
      "epoch: 9 step: 175, loss is 1.518695592880249\n",
      "epoch: 9 step: 176, loss is 1.3465956449508667\n",
      "epoch: 9 step: 177, loss is 1.5635161399841309\n",
      "epoch: 9 step: 178, loss is 1.4440656900405884\n",
      "epoch: 9 step: 179, loss is 1.4560844898223877\n",
      "epoch: 9 step: 180, loss is 1.4402612447738647\n",
      "epoch: 9 step: 181, loss is 1.4595813751220703\n",
      "epoch: 9 step: 182, loss is 1.5155633687973022\n",
      "epoch: 9 step: 183, loss is 1.6121151447296143\n",
      "epoch: 9 step: 184, loss is 1.4405033588409424\n",
      "epoch: 9 step: 185, loss is 1.5107688903808594\n",
      "epoch: 9 step: 186, loss is 1.4478564262390137\n",
      "epoch: 9 step: 187, loss is 1.463112235069275\n",
      "epoch: 9 step: 188, loss is 1.2544925212860107\n",
      "epoch: 9 step: 189, loss is 1.5086414813995361\n",
      "epoch: 9 step: 190, loss is 1.4073925018310547\n",
      "epoch: 9 step: 191, loss is 1.3768061399459839\n",
      "epoch: 9 step: 192, loss is 1.3443481922149658\n",
      "epoch: 9 step: 193, loss is 1.5125246047973633\n",
      "epoch: 9 step: 194, loss is 1.3822848796844482\n",
      "epoch: 9 step: 195, loss is 1.4324378967285156\n",
      "epoch: 9 step: 196, loss is 1.4299079179763794\n",
      "epoch: 9 step: 197, loss is 1.4664541482925415\n",
      "epoch: 9 step: 198, loss is 1.3896212577819824\n",
      "epoch: 9 step: 199, loss is 1.3666625022888184\n",
      "epoch: 9 step: 200, loss is 1.429582953453064\n",
      "epoch: 9 step: 201, loss is 1.4853168725967407\n",
      "epoch: 9 step: 202, loss is 1.48240327835083\n",
      "epoch: 9 step: 203, loss is 1.3896934986114502\n",
      "epoch: 9 step: 204, loss is 1.4719887971878052\n",
      "epoch: 9 step: 205, loss is 1.3692351579666138\n",
      "epoch: 9 step: 206, loss is 1.446628212928772\n",
      "epoch: 9 step: 207, loss is 1.3901052474975586\n",
      "epoch: 9 step: 208, loss is 1.4588327407836914\n",
      "epoch: 9 step: 209, loss is 1.4956779479980469\n",
      "epoch: 9 step: 210, loss is 1.559576153755188\n",
      "epoch: 9 step: 211, loss is 1.4002013206481934\n",
      "epoch: 9 step: 212, loss is 1.4013268947601318\n",
      "epoch: 9 step: 213, loss is 1.4685113430023193\n",
      "epoch: 9 step: 214, loss is 1.2935523986816406\n",
      "epoch: 9 step: 215, loss is 1.4501893520355225\n",
      "epoch: 9 step: 216, loss is 1.5622555017471313\n",
      "epoch: 9 step: 217, loss is 1.4894870519638062\n",
      "epoch: 9 step: 218, loss is 1.367417335510254\n",
      "epoch: 9 step: 219, loss is 1.5369350910186768\n",
      "epoch: 9 step: 220, loss is 1.4405813217163086\n",
      "epoch: 9 step: 221, loss is 1.4325063228607178\n",
      "epoch: 9 step: 222, loss is 1.497898817062378\n",
      "epoch: 9 step: 223, loss is 1.5082732439041138\n",
      "epoch: 9 step: 224, loss is 1.4941571950912476\n",
      "epoch: 9 step: 225, loss is 1.4731183052062988\n",
      "epoch: 9 step: 226, loss is 1.3310273885726929\n",
      "epoch: 9 step: 227, loss is 1.577664852142334\n",
      "epoch: 9 step: 228, loss is 1.481323003768921\n",
      "epoch: 9 step: 229, loss is 1.5121772289276123\n",
      "epoch: 9 step: 230, loss is 1.4707155227661133\n",
      "epoch: 9 step: 231, loss is 1.5235183238983154\n",
      "epoch: 9 step: 232, loss is 1.3749581575393677\n",
      "epoch: 9 step: 233, loss is 1.4181526899337769\n",
      "epoch: 9 step: 234, loss is 1.3814802169799805\n",
      "epoch: 9 step: 235, loss is 1.533522129058838\n",
      "epoch: 9 step: 236, loss is 1.4177122116088867\n",
      "epoch: 9 step: 237, loss is 1.5274066925048828\n",
      "epoch: 9 step: 238, loss is 1.5054625272750854\n",
      "epoch: 9 step: 239, loss is 1.4235928058624268\n",
      "epoch: 9 step: 240, loss is 1.4811781644821167\n",
      "epoch: 9 step: 241, loss is 1.3806328773498535\n",
      "epoch: 9 step: 242, loss is 1.3121061325073242\n",
      "epoch: 9 step: 243, loss is 1.5964248180389404\n",
      "epoch: 9 step: 244, loss is 1.612391471862793\n",
      "epoch: 9 step: 245, loss is 1.5298187732696533\n",
      "epoch: 9 step: 246, loss is 1.5065116882324219\n",
      "epoch: 9 step: 247, loss is 1.3450371026992798\n",
      "epoch: 9 step: 248, loss is 1.4587019681930542\n",
      "epoch: 9 step: 249, loss is 1.5589033365249634\n",
      "epoch: 9 step: 250, loss is 1.4778138399124146\n",
      "epoch: 9 step: 251, loss is 1.376392126083374\n",
      "epoch: 9 step: 252, loss is 1.5278961658477783\n",
      "epoch: 9 step: 253, loss is 1.3343933820724487\n",
      "epoch: 9 step: 254, loss is 1.3312689065933228\n",
      "epoch: 9 step: 255, loss is 1.4004508256912231\n",
      "epoch: 9 step: 256, loss is 1.4265828132629395\n",
      "epoch: 9 step: 257, loss is 1.3043268918991089\n",
      "epoch: 9 step: 258, loss is 1.4142978191375732\n",
      "epoch: 9 step: 259, loss is 1.5646636486053467\n",
      "epoch: 9 step: 260, loss is 1.4027700424194336\n",
      "epoch: 9 step: 261, loss is 1.4143617153167725\n",
      "epoch: 9 step: 262, loss is 1.489669919013977\n",
      "epoch: 9 step: 263, loss is 1.481561541557312\n",
      "epoch: 9 step: 264, loss is 1.3846005201339722\n",
      "epoch: 9 step: 265, loss is 1.4189887046813965\n",
      "epoch: 9 step: 266, loss is 1.3647762537002563\n",
      "epoch: 9 step: 267, loss is 1.3987873792648315\n",
      "epoch: 9 step: 268, loss is 1.4488424062728882\n",
      "epoch: 9 step: 269, loss is 1.4460173845291138\n",
      "epoch: 9 step: 270, loss is 1.3827844858169556\n",
      "epoch: 9 step: 271, loss is 1.5123860836029053\n",
      "epoch: 9 step: 272, loss is 1.4810303449630737\n",
      "epoch: 9 step: 273, loss is 1.4521244764328003\n",
      "epoch: 9 step: 274, loss is 1.3979334831237793\n",
      "epoch: 9 step: 275, loss is 1.4594900608062744\n",
      "epoch: 9 step: 276, loss is 1.555885910987854\n",
      "epoch: 9 step: 277, loss is 1.327934980392456\n",
      "epoch: 9 step: 278, loss is 1.491844654083252\n",
      "epoch: 9 step: 279, loss is 1.4364066123962402\n",
      "epoch: 9 step: 280, loss is 1.4846895933151245\n",
      "epoch: 9 step: 281, loss is 1.5117391347885132\n",
      "epoch: 9 step: 282, loss is 1.3889631032943726\n",
      "epoch: 9 step: 283, loss is 1.3919126987457275\n",
      "epoch: 9 step: 284, loss is 1.5055607557296753\n",
      "epoch: 9 step: 285, loss is 1.446462631225586\n",
      "epoch: 9 step: 286, loss is 1.4571945667266846\n",
      "epoch: 9 step: 287, loss is 1.4350628852844238\n",
      "epoch: 9 step: 288, loss is 1.3601300716400146\n",
      "epoch: 9 step: 289, loss is 1.3701095581054688\n",
      "epoch: 9 step: 290, loss is 1.3116239309310913\n",
      "epoch: 9 step: 291, loss is 1.365700364112854\n",
      "epoch: 9 step: 292, loss is 1.3687493801116943\n",
      "epoch: 9 step: 293, loss is 1.5545859336853027\n",
      "epoch: 9 step: 294, loss is 1.4613817930221558\n",
      "epoch: 9 step: 295, loss is 1.3108704090118408\n",
      "epoch: 9 step: 296, loss is 1.3764315843582153\n",
      "epoch: 9 step: 297, loss is 1.5477814674377441\n",
      "epoch: 9 step: 298, loss is 1.3465299606323242\n",
      "epoch: 9 step: 299, loss is 1.3842692375183105\n",
      "epoch: 9 step: 300, loss is 1.4649568796157837\n",
      "epoch: 9 step: 301, loss is 1.413118600845337\n",
      "epoch: 9 step: 302, loss is 1.2689106464385986\n",
      "epoch: 9 step: 303, loss is 1.541799545288086\n",
      "epoch: 9 step: 304, loss is 1.518890380859375\n",
      "epoch: 9 step: 305, loss is 1.4775643348693848\n",
      "epoch: 9 step: 306, loss is 1.4470757246017456\n",
      "epoch: 9 step: 307, loss is 1.557159423828125\n",
      "epoch: 9 step: 308, loss is 1.4307982921600342\n",
      "epoch: 9 step: 309, loss is 1.4482967853546143\n",
      "epoch: 9 step: 310, loss is 1.5080583095550537\n",
      "epoch: 9 step: 311, loss is 1.4031940698623657\n",
      "epoch: 9 step: 312, loss is 1.3646984100341797\n",
      "epoch: 9 step: 313, loss is 1.5097078084945679\n",
      "epoch: 9 step: 314, loss is 1.5238103866577148\n",
      "epoch: 9 step: 315, loss is 1.489359736442566\n",
      "epoch: 9 step: 316, loss is 1.361464023590088\n",
      "epoch: 9 step: 317, loss is 1.3658115863800049\n",
      "epoch: 9 step: 318, loss is 1.3230791091918945\n",
      "epoch: 9 step: 319, loss is 1.503589153289795\n",
      "epoch: 9 step: 320, loss is 1.425694227218628\n",
      "epoch: 9 step: 321, loss is 1.349456787109375\n",
      "epoch: 9 step: 322, loss is 1.4635549783706665\n",
      "epoch: 9 step: 323, loss is 1.4393064975738525\n",
      "epoch: 9 step: 324, loss is 1.3924195766448975\n",
      "epoch: 9 step: 325, loss is 1.3318133354187012\n",
      "epoch: 9 step: 326, loss is 1.4977684020996094\n",
      "epoch: 9 step: 327, loss is 1.4456567764282227\n",
      "epoch: 9 step: 328, loss is 1.4122751951217651\n",
      "epoch: 9 step: 329, loss is 1.495193600654602\n",
      "epoch: 9 step: 330, loss is 1.4332256317138672\n",
      "epoch: 9 step: 331, loss is 1.5367563962936401\n",
      "epoch: 9 step: 332, loss is 1.3852922916412354\n",
      "epoch: 9 step: 333, loss is 1.4151138067245483\n",
      "epoch: 9 step: 334, loss is 1.4808027744293213\n",
      "epoch: 9 step: 335, loss is 1.5473459959030151\n",
      "epoch: 9 step: 336, loss is 1.450141191482544\n",
      "epoch: 9 step: 337, loss is 1.530683994293213\n",
      "epoch: 9 step: 338, loss is 1.4339540004730225\n",
      "epoch: 9 step: 339, loss is 1.380033016204834\n",
      "epoch: 9 step: 340, loss is 1.3802093267440796\n",
      "epoch: 9 step: 341, loss is 1.6107572317123413\n",
      "epoch: 9 step: 342, loss is 1.5097360610961914\n",
      "epoch: 9 step: 343, loss is 1.3971424102783203\n",
      "epoch: 9 step: 344, loss is 1.486131191253662\n",
      "epoch: 9 step: 345, loss is 1.3692948818206787\n",
      "epoch: 9 step: 346, loss is 1.440065622329712\n",
      "epoch: 9 step: 347, loss is 1.4743878841400146\n",
      "epoch: 9 step: 348, loss is 1.4549906253814697\n",
      "epoch: 9 step: 349, loss is 1.3470219373703003\n",
      "epoch: 9 step: 350, loss is 1.4371883869171143\n",
      "epoch: 9 step: 351, loss is 1.5831751823425293\n",
      "epoch: 9 step: 352, loss is 1.4633469581604004\n",
      "epoch: 9 step: 353, loss is 1.5359997749328613\n",
      "epoch: 9 step: 354, loss is 1.492952823638916\n",
      "epoch: 9 step: 355, loss is 1.3962645530700684\n",
      "epoch: 9 step: 356, loss is 1.3919004201889038\n",
      "epoch: 9 step: 357, loss is 1.4642014503479004\n",
      "epoch: 9 step: 358, loss is 1.5345540046691895\n",
      "epoch: 9 step: 359, loss is 1.521216869354248\n",
      "epoch: 9 step: 360, loss is 1.3455700874328613\n",
      "epoch: 9 step: 361, loss is 1.4067219495773315\n",
      "epoch: 9 step: 362, loss is 1.4623830318450928\n",
      "epoch: 9 step: 363, loss is 1.5351388454437256\n",
      "epoch: 9 step: 364, loss is 1.4647955894470215\n",
      "epoch: 9 step: 365, loss is 1.5268127918243408\n",
      "epoch: 9 step: 366, loss is 1.4757179021835327\n",
      "epoch: 9 step: 367, loss is 1.5284063816070557\n",
      "epoch: 9 step: 368, loss is 1.4376068115234375\n",
      "epoch: 9 step: 369, loss is 1.416051983833313\n",
      "epoch: 9 step: 370, loss is 1.507351279258728\n",
      "epoch: 9 step: 371, loss is 1.436951756477356\n",
      "epoch: 9 step: 372, loss is 1.4518663883209229\n",
      "epoch: 9 step: 373, loss is 1.5906983613967896\n",
      "epoch: 9 step: 374, loss is 1.5779473781585693\n",
      "epoch: 9 step: 375, loss is 1.408064365386963\n",
      "epoch: 9 step: 376, loss is 1.5342836380004883\n",
      "epoch: 9 step: 377, loss is 1.4314239025115967\n",
      "epoch: 9 step: 378, loss is 1.4581204652786255\n",
      "epoch: 9 step: 379, loss is 1.2928191423416138\n",
      "epoch: 9 step: 380, loss is 1.462073564529419\n",
      "epoch: 9 step: 381, loss is 1.378496527671814\n",
      "epoch: 9 step: 382, loss is 1.4038872718811035\n",
      "epoch: 9 step: 383, loss is 1.3931055068969727\n",
      "epoch: 9 step: 384, loss is 1.3542702198028564\n",
      "epoch: 9 step: 385, loss is 1.5439268350601196\n",
      "epoch: 9 step: 386, loss is 1.5707677602767944\n",
      "epoch: 9 step: 387, loss is 1.388087511062622\n",
      "epoch: 9 step: 388, loss is 1.361182689666748\n",
      "epoch: 9 step: 389, loss is 1.383829951286316\n",
      "epoch: 9 step: 390, loss is 1.2901922464370728\n",
      "Train epoch time: 160001.950 ms, per step time: 410.261 ms\n",
      "epoch: 10 step: 1, loss is 1.3990360498428345\n",
      "epoch: 10 step: 2, loss is 1.4782894849777222\n",
      "epoch: 10 step: 3, loss is 1.41673743724823\n",
      "epoch: 10 step: 4, loss is 1.306221604347229\n",
      "epoch: 10 step: 5, loss is 1.429288387298584\n",
      "epoch: 10 step: 6, loss is 1.4804019927978516\n",
      "epoch: 10 step: 7, loss is 1.3737043142318726\n",
      "epoch: 10 step: 8, loss is 1.3701043128967285\n",
      "epoch: 10 step: 9, loss is 1.4096059799194336\n",
      "epoch: 10 step: 10, loss is 1.4564133882522583\n",
      "epoch: 10 step: 11, loss is 1.508805513381958\n",
      "epoch: 10 step: 12, loss is 1.358223795890808\n",
      "epoch: 10 step: 13, loss is 1.4039674997329712\n",
      "epoch: 10 step: 14, loss is 1.4346387386322021\n",
      "epoch: 10 step: 15, loss is 1.3837475776672363\n",
      "epoch: 10 step: 16, loss is 1.3986895084381104\n",
      "epoch: 10 step: 17, loss is 1.255757451057434\n",
      "epoch: 10 step: 18, loss is 1.4365366697311401\n",
      "epoch: 10 step: 19, loss is 1.390352725982666\n",
      "epoch: 10 step: 20, loss is 1.4842133522033691\n",
      "epoch: 10 step: 21, loss is 1.4020742177963257\n",
      "epoch: 10 step: 22, loss is 1.419957160949707\n",
      "epoch: 10 step: 23, loss is 1.5021635293960571\n",
      "epoch: 10 step: 24, loss is 1.4662621021270752\n",
      "epoch: 10 step: 25, loss is 1.4045777320861816\n",
      "epoch: 10 step: 26, loss is 1.4360352754592896\n",
      "epoch: 10 step: 27, loss is 1.3636549711227417\n",
      "epoch: 10 step: 28, loss is 1.4057812690734863\n",
      "epoch: 10 step: 29, loss is 1.4503898620605469\n",
      "epoch: 10 step: 30, loss is 1.4116593599319458\n",
      "epoch: 10 step: 31, loss is 1.3263020515441895\n",
      "epoch: 10 step: 32, loss is 1.5058480501174927\n",
      "epoch: 10 step: 33, loss is 1.516197919845581\n",
      "epoch: 10 step: 34, loss is 1.3874163627624512\n",
      "epoch: 10 step: 35, loss is 1.41354501247406\n",
      "epoch: 10 step: 36, loss is 1.314368724822998\n",
      "epoch: 10 step: 37, loss is 1.328434705734253\n",
      "epoch: 10 step: 38, loss is 1.4973863363265991\n",
      "epoch: 10 step: 39, loss is 1.3824776411056519\n",
      "epoch: 10 step: 40, loss is 1.5896574258804321\n",
      "epoch: 10 step: 41, loss is 1.4655301570892334\n",
      "epoch: 10 step: 42, loss is 1.4531199932098389\n",
      "epoch: 10 step: 43, loss is 1.3682684898376465\n",
      "epoch: 10 step: 44, loss is 1.4505481719970703\n",
      "epoch: 10 step: 45, loss is 1.3996784687042236\n",
      "epoch: 10 step: 46, loss is 1.4671921730041504\n",
      "epoch: 10 step: 47, loss is 1.4559272527694702\n",
      "epoch: 10 step: 48, loss is 1.331982135772705\n",
      "epoch: 10 step: 49, loss is 1.3949846029281616\n",
      "epoch: 10 step: 50, loss is 1.3727433681488037\n",
      "epoch: 10 step: 51, loss is 1.481406569480896\n",
      "epoch: 10 step: 52, loss is 1.5626158714294434\n",
      "epoch: 10 step: 53, loss is 1.407529354095459\n",
      "epoch: 10 step: 54, loss is 1.5147223472595215\n",
      "epoch: 10 step: 55, loss is 1.4245684146881104\n",
      "epoch: 10 step: 56, loss is 1.3957698345184326\n",
      "epoch: 10 step: 57, loss is 1.5324082374572754\n",
      "epoch: 10 step: 58, loss is 1.5247374773025513\n",
      "epoch: 10 step: 59, loss is 1.346667766571045\n",
      "epoch: 10 step: 60, loss is 1.3803824186325073\n",
      "epoch: 10 step: 61, loss is 1.397390365600586\n",
      "epoch: 10 step: 62, loss is 1.5536237955093384\n",
      "epoch: 10 step: 63, loss is 1.429768443107605\n",
      "epoch: 10 step: 64, loss is 1.4736030101776123\n",
      "epoch: 10 step: 65, loss is 1.520302653312683\n",
      "epoch: 10 step: 66, loss is 1.3619483709335327\n",
      "epoch: 10 step: 67, loss is 1.4733867645263672\n",
      "epoch: 10 step: 68, loss is 1.3725707530975342\n",
      "epoch: 10 step: 69, loss is 1.3997701406478882\n",
      "epoch: 10 step: 70, loss is 1.5216193199157715\n",
      "epoch: 10 step: 71, loss is 1.445729374885559\n",
      "epoch: 10 step: 72, loss is 1.423256278038025\n",
      "epoch: 10 step: 73, loss is 1.3746047019958496\n",
      "epoch: 10 step: 74, loss is 1.4101276397705078\n",
      "epoch: 10 step: 75, loss is 1.369474172592163\n",
      "epoch: 10 step: 76, loss is 1.5519555807113647\n",
      "epoch: 10 step: 77, loss is 1.4437183141708374\n",
      "epoch: 10 step: 78, loss is 1.3838796615600586\n",
      "epoch: 10 step: 79, loss is 1.4239128828048706\n",
      "epoch: 10 step: 80, loss is 1.4243721961975098\n",
      "epoch: 10 step: 81, loss is 1.2491763830184937\n",
      "epoch: 10 step: 82, loss is 1.4681912660598755\n",
      "epoch: 10 step: 83, loss is 1.549702525138855\n",
      "epoch: 10 step: 84, loss is 1.4197158813476562\n",
      "epoch: 10 step: 85, loss is 1.4389896392822266\n",
      "epoch: 10 step: 86, loss is 1.3572752475738525\n",
      "epoch: 10 step: 87, loss is 1.2854583263397217\n",
      "epoch: 10 step: 88, loss is 1.4152603149414062\n",
      "epoch: 10 step: 89, loss is 1.489219307899475\n",
      "epoch: 10 step: 90, loss is 1.6256576776504517\n",
      "epoch: 10 step: 91, loss is 1.4823702573776245\n",
      "epoch: 10 step: 92, loss is 1.3547179698944092\n",
      "epoch: 10 step: 93, loss is 1.2688974142074585\n",
      "epoch: 10 step: 94, loss is 1.3774802684783936\n",
      "epoch: 10 step: 95, loss is 1.4514164924621582\n",
      "epoch: 10 step: 96, loss is 1.479731559753418\n",
      "epoch: 10 step: 97, loss is 1.2845488786697388\n",
      "epoch: 10 step: 98, loss is 1.364551067352295\n",
      "epoch: 10 step: 99, loss is 1.500006079673767\n",
      "epoch: 10 step: 100, loss is 1.4435961246490479\n",
      "epoch: 10 step: 101, loss is 1.4950047731399536\n",
      "epoch: 10 step: 102, loss is 1.4928900003433228\n",
      "epoch: 10 step: 103, loss is 1.5250163078308105\n",
      "epoch: 10 step: 104, loss is 1.4758821725845337\n",
      "epoch: 10 step: 105, loss is 1.4761834144592285\n",
      "epoch: 10 step: 106, loss is 1.304107427597046\n",
      "epoch: 10 step: 107, loss is 1.4577630758285522\n",
      "epoch: 10 step: 108, loss is 1.4365464448928833\n",
      "epoch: 10 step: 109, loss is 1.4553061723709106\n",
      "epoch: 10 step: 110, loss is 1.4324270486831665\n",
      "epoch: 10 step: 111, loss is 1.3959956169128418\n",
      "epoch: 10 step: 112, loss is 1.4415022134780884\n",
      "epoch: 10 step: 113, loss is 1.5016162395477295\n",
      "epoch: 10 step: 114, loss is 1.403846263885498\n",
      "epoch: 10 step: 115, loss is 1.4340708255767822\n",
      "epoch: 10 step: 116, loss is 1.391270637512207\n",
      "epoch: 10 step: 117, loss is 1.4350272417068481\n",
      "epoch: 10 step: 118, loss is 1.599487066268921\n",
      "epoch: 10 step: 119, loss is 1.359555959701538\n",
      "epoch: 10 step: 120, loss is 1.4863004684448242\n",
      "epoch: 10 step: 121, loss is 1.4754047393798828\n",
      "epoch: 10 step: 122, loss is 1.375072956085205\n",
      "epoch: 10 step: 123, loss is 1.3311042785644531\n",
      "epoch: 10 step: 124, loss is 1.509623408317566\n",
      "epoch: 10 step: 125, loss is 1.4149153232574463\n",
      "epoch: 10 step: 126, loss is 1.457214593887329\n",
      "epoch: 10 step: 127, loss is 1.394838571548462\n",
      "epoch: 10 step: 128, loss is 1.3954880237579346\n",
      "epoch: 10 step: 129, loss is 1.3132288455963135\n",
      "epoch: 10 step: 130, loss is 1.4970958232879639\n",
      "epoch: 10 step: 131, loss is 1.4308348894119263\n",
      "epoch: 10 step: 132, loss is 1.354781985282898\n",
      "epoch: 10 step: 133, loss is 1.4358525276184082\n",
      "epoch: 10 step: 134, loss is 1.4534660577774048\n",
      "epoch: 10 step: 135, loss is 1.409698247909546\n",
      "epoch: 10 step: 136, loss is 1.2656688690185547\n",
      "epoch: 10 step: 137, loss is 1.5356465578079224\n",
      "epoch: 10 step: 138, loss is 1.3733761310577393\n",
      "epoch: 10 step: 139, loss is 1.4436789751052856\n",
      "epoch: 10 step: 140, loss is 1.4934461116790771\n",
      "epoch: 10 step: 141, loss is 1.4105517864227295\n",
      "epoch: 10 step: 142, loss is 1.376538634300232\n",
      "epoch: 10 step: 143, loss is 1.4838624000549316\n",
      "epoch: 10 step: 144, loss is 1.5345919132232666\n",
      "epoch: 10 step: 145, loss is 1.3858698606491089\n",
      "epoch: 10 step: 146, loss is 1.4179044961929321\n",
      "epoch: 10 step: 147, loss is 1.4569969177246094\n",
      "epoch: 10 step: 148, loss is 1.383131980895996\n",
      "epoch: 10 step: 149, loss is 1.3084707260131836\n",
      "epoch: 10 step: 150, loss is 1.4028908014297485\n",
      "epoch: 10 step: 151, loss is 1.5040751695632935\n",
      "epoch: 10 step: 152, loss is 1.3424123525619507\n",
      "epoch: 10 step: 153, loss is 1.481092929840088\n",
      "epoch: 10 step: 154, loss is 1.4375941753387451\n",
      "epoch: 10 step: 155, loss is 1.411681890487671\n",
      "epoch: 10 step: 156, loss is 1.3508011102676392\n",
      "epoch: 10 step: 157, loss is 1.437112808227539\n",
      "epoch: 10 step: 158, loss is 1.3548719882965088\n",
      "epoch: 10 step: 159, loss is 1.5806151628494263\n",
      "epoch: 10 step: 160, loss is 1.4943406581878662\n",
      "epoch: 10 step: 161, loss is 1.4283822774887085\n",
      "epoch: 10 step: 162, loss is 1.4297887086868286\n",
      "epoch: 10 step: 163, loss is 1.358820915222168\n",
      "epoch: 10 step: 164, loss is 1.4694030284881592\n",
      "epoch: 10 step: 165, loss is 1.4897093772888184\n",
      "epoch: 10 step: 166, loss is 1.4918357133865356\n",
      "epoch: 10 step: 167, loss is 1.37608802318573\n",
      "epoch: 10 step: 168, loss is 1.3432285785675049\n",
      "epoch: 10 step: 169, loss is 1.4634732007980347\n",
      "epoch: 10 step: 170, loss is 1.3541861772537231\n",
      "epoch: 10 step: 171, loss is 1.3573083877563477\n",
      "epoch: 10 step: 172, loss is 1.5319814682006836\n",
      "epoch: 10 step: 173, loss is 1.514073133468628\n",
      "epoch: 10 step: 174, loss is 1.3390674591064453\n",
      "epoch: 10 step: 175, loss is 1.4390780925750732\n",
      "epoch: 10 step: 176, loss is 1.4185720682144165\n",
      "epoch: 10 step: 177, loss is 1.5014269351959229\n",
      "epoch: 10 step: 178, loss is 1.5124952793121338\n",
      "epoch: 10 step: 179, loss is 1.3647645711898804\n",
      "epoch: 10 step: 180, loss is 1.4627636671066284\n",
      "epoch: 10 step: 181, loss is 1.4616161584854126\n",
      "epoch: 10 step: 182, loss is 1.3625093698501587\n",
      "epoch: 10 step: 183, loss is 1.4079291820526123\n",
      "epoch: 10 step: 184, loss is 1.2768287658691406\n",
      "epoch: 10 step: 185, loss is 1.302158236503601\n",
      "epoch: 10 step: 186, loss is 1.5139268636703491\n",
      "epoch: 10 step: 187, loss is 1.4301700592041016\n",
      "epoch: 10 step: 188, loss is 1.3209600448608398\n",
      "epoch: 10 step: 189, loss is 1.4217684268951416\n",
      "epoch: 10 step: 190, loss is 1.4775609970092773\n",
      "epoch: 10 step: 191, loss is 1.4097005128860474\n",
      "epoch: 10 step: 192, loss is 1.388981819152832\n",
      "epoch: 10 step: 193, loss is 1.4433209896087646\n",
      "epoch: 10 step: 194, loss is 1.302496075630188\n",
      "epoch: 10 step: 195, loss is 1.5263211727142334\n",
      "epoch: 10 step: 196, loss is 1.2845580577850342\n",
      "epoch: 10 step: 197, loss is 1.4509119987487793\n",
      "epoch: 10 step: 198, loss is 1.4181116819381714\n",
      "epoch: 10 step: 199, loss is 1.4955576658248901\n",
      "epoch: 10 step: 200, loss is 1.4173626899719238\n",
      "epoch: 10 step: 201, loss is 1.3072549104690552\n",
      "epoch: 10 step: 202, loss is 1.3691096305847168\n",
      "epoch: 10 step: 203, loss is 1.4916385412216187\n",
      "epoch: 10 step: 204, loss is 1.3348129987716675\n",
      "epoch: 10 step: 205, loss is 1.4009424448013306\n",
      "epoch: 10 step: 206, loss is 1.4292529821395874\n",
      "epoch: 10 step: 207, loss is 1.3751035928726196\n",
      "epoch: 10 step: 208, loss is 1.3035991191864014\n",
      "epoch: 10 step: 209, loss is 1.4268440008163452\n",
      "epoch: 10 step: 210, loss is 1.3952219486236572\n",
      "epoch: 10 step: 211, loss is 1.4034377336502075\n",
      "epoch: 10 step: 212, loss is 1.3635430335998535\n",
      "epoch: 10 step: 213, loss is 1.4148406982421875\n",
      "epoch: 10 step: 214, loss is 1.403229832649231\n",
      "epoch: 10 step: 215, loss is 1.4417269229888916\n",
      "epoch: 10 step: 216, loss is 1.4916632175445557\n",
      "epoch: 10 step: 217, loss is 1.4397008419036865\n",
      "epoch: 10 step: 218, loss is 1.5011696815490723\n",
      "epoch: 10 step: 219, loss is 1.4781092405319214\n",
      "epoch: 10 step: 220, loss is 1.3784904479980469\n",
      "epoch: 10 step: 221, loss is 1.2967666387557983\n",
      "epoch: 10 step: 222, loss is 1.3830097913742065\n",
      "epoch: 10 step: 223, loss is 1.418813943862915\n",
      "epoch: 10 step: 224, loss is 1.5962743759155273\n",
      "epoch: 10 step: 225, loss is 1.5090703964233398\n",
      "epoch: 10 step: 226, loss is 1.4144904613494873\n",
      "epoch: 10 step: 227, loss is 1.4151054620742798\n",
      "epoch: 10 step: 228, loss is 1.3640038967132568\n",
      "epoch: 10 step: 229, loss is 1.480387568473816\n",
      "epoch: 10 step: 230, loss is 1.3327505588531494\n",
      "epoch: 10 step: 231, loss is 1.3633034229278564\n",
      "epoch: 10 step: 232, loss is 1.492480754852295\n",
      "epoch: 10 step: 233, loss is 1.3644146919250488\n",
      "epoch: 10 step: 234, loss is 1.4410758018493652\n",
      "epoch: 10 step: 235, loss is 1.50874662399292\n",
      "epoch: 10 step: 236, loss is 1.5259652137756348\n",
      "epoch: 10 step: 237, loss is 1.3908400535583496\n",
      "epoch: 10 step: 238, loss is 1.3575825691223145\n",
      "epoch: 10 step: 239, loss is 1.3778893947601318\n",
      "epoch: 10 step: 240, loss is 1.41816246509552\n",
      "epoch: 10 step: 241, loss is 1.5194977521896362\n",
      "epoch: 10 step: 242, loss is 1.3423168659210205\n",
      "epoch: 10 step: 243, loss is 1.3252742290496826\n",
      "epoch: 10 step: 244, loss is 1.357385277748108\n",
      "epoch: 10 step: 245, loss is 1.3749215602874756\n",
      "epoch: 10 step: 246, loss is 1.4080734252929688\n",
      "epoch: 10 step: 247, loss is 1.4725407361984253\n",
      "epoch: 10 step: 248, loss is 1.3717520236968994\n",
      "epoch: 10 step: 249, loss is 1.3361761569976807\n",
      "epoch: 10 step: 250, loss is 1.4013748168945312\n",
      "epoch: 10 step: 251, loss is 1.6463385820388794\n",
      "epoch: 10 step: 252, loss is 1.369153380393982\n",
      "epoch: 10 step: 253, loss is 1.3935294151306152\n",
      "epoch: 10 step: 254, loss is 1.40248441696167\n",
      "epoch: 10 step: 255, loss is 1.3551896810531616\n",
      "epoch: 10 step: 256, loss is 1.4712543487548828\n",
      "epoch: 10 step: 257, loss is 1.3434057235717773\n",
      "epoch: 10 step: 258, loss is 1.40899658203125\n",
      "epoch: 10 step: 259, loss is 1.322641372680664\n",
      "epoch: 10 step: 260, loss is 1.2581284046173096\n",
      "epoch: 10 step: 261, loss is 1.4272472858428955\n",
      "epoch: 10 step: 262, loss is 1.4754951000213623\n",
      "epoch: 10 step: 263, loss is 1.3468773365020752\n",
      "epoch: 10 step: 264, loss is 1.4574878215789795\n",
      "epoch: 10 step: 265, loss is 1.4819691181182861\n",
      "epoch: 10 step: 266, loss is 1.4005104303359985\n",
      "epoch: 10 step: 267, loss is 1.345023512840271\n",
      "epoch: 10 step: 268, loss is 1.4036189317703247\n",
      "epoch: 10 step: 269, loss is 1.4942049980163574\n",
      "epoch: 10 step: 270, loss is 1.3514699935913086\n",
      "epoch: 10 step: 271, loss is 1.3297892808914185\n",
      "epoch: 10 step: 272, loss is 1.3702313899993896\n",
      "epoch: 10 step: 273, loss is 1.427927851676941\n",
      "epoch: 10 step: 274, loss is 1.220078945159912\n",
      "epoch: 10 step: 275, loss is 1.4005016088485718\n",
      "epoch: 10 step: 276, loss is 1.3391598463058472\n",
      "epoch: 10 step: 277, loss is 1.3810564279556274\n",
      "epoch: 10 step: 278, loss is 1.4151084423065186\n",
      "epoch: 10 step: 279, loss is 1.3463934659957886\n",
      "epoch: 10 step: 280, loss is 1.4554483890533447\n",
      "epoch: 10 step: 281, loss is 1.5625789165496826\n",
      "epoch: 10 step: 282, loss is 1.346510648727417\n",
      "epoch: 10 step: 283, loss is 1.4992635250091553\n",
      "epoch: 10 step: 284, loss is 1.5040206909179688\n",
      "epoch: 10 step: 285, loss is 1.4027769565582275\n",
      "epoch: 10 step: 286, loss is 1.3951466083526611\n",
      "epoch: 10 step: 287, loss is 1.4912261962890625\n",
      "epoch: 10 step: 288, loss is 1.5321651697158813\n",
      "epoch: 10 step: 289, loss is 1.3845505714416504\n",
      "epoch: 10 step: 290, loss is 1.3323293924331665\n",
      "epoch: 10 step: 291, loss is 1.474090337753296\n",
      "epoch: 10 step: 292, loss is 1.4794713258743286\n",
      "epoch: 10 step: 293, loss is 1.4115345478057861\n",
      "epoch: 10 step: 294, loss is 1.4923486709594727\n",
      "epoch: 10 step: 295, loss is 1.4805209636688232\n",
      "epoch: 10 step: 296, loss is 1.5238926410675049\n",
      "epoch: 10 step: 297, loss is 1.5692869424819946\n",
      "epoch: 10 step: 298, loss is 1.436819314956665\n",
      "epoch: 10 step: 299, loss is 1.5949362516403198\n",
      "epoch: 10 step: 300, loss is 1.4301948547363281\n",
      "epoch: 10 step: 301, loss is 1.374549388885498\n",
      "epoch: 10 step: 302, loss is 1.3786051273345947\n",
      "epoch: 10 step: 303, loss is 1.403440237045288\n",
      "epoch: 10 step: 304, loss is 1.5971510410308838\n",
      "epoch: 10 step: 305, loss is 1.5042171478271484\n",
      "epoch: 10 step: 306, loss is 1.3665661811828613\n",
      "epoch: 10 step: 307, loss is 1.3953237533569336\n",
      "epoch: 10 step: 308, loss is 1.3361872434616089\n",
      "epoch: 10 step: 309, loss is 1.4729557037353516\n",
      "epoch: 10 step: 310, loss is 1.3891372680664062\n",
      "epoch: 10 step: 311, loss is 1.430635690689087\n",
      "epoch: 10 step: 312, loss is 1.3235746622085571\n",
      "epoch: 10 step: 313, loss is 1.3370637893676758\n",
      "epoch: 10 step: 314, loss is 1.309460163116455\n",
      "epoch: 10 step: 315, loss is 1.2870289087295532\n",
      "epoch: 10 step: 316, loss is 1.277299165725708\n",
      "epoch: 10 step: 317, loss is 1.4983407258987427\n",
      "epoch: 10 step: 318, loss is 1.4781322479248047\n",
      "epoch: 10 step: 319, loss is 1.453725814819336\n",
      "epoch: 10 step: 320, loss is 1.4478719234466553\n",
      "epoch: 10 step: 321, loss is 1.370972990989685\n",
      "epoch: 10 step: 322, loss is 1.4028041362762451\n",
      "epoch: 10 step: 323, loss is 1.299850583076477\n",
      "epoch: 10 step: 324, loss is 1.5622531175613403\n",
      "epoch: 10 step: 325, loss is 1.4188071489334106\n",
      "epoch: 10 step: 326, loss is 1.3967514038085938\n",
      "epoch: 10 step: 327, loss is 1.4412944316864014\n",
      "epoch: 10 step: 328, loss is 1.404923677444458\n",
      "epoch: 10 step: 329, loss is 1.3674139976501465\n",
      "epoch: 10 step: 330, loss is 1.496813416481018\n",
      "epoch: 10 step: 331, loss is 1.4962341785430908\n",
      "epoch: 10 step: 332, loss is 1.3748892545700073\n",
      "epoch: 10 step: 333, loss is 1.4157999753952026\n",
      "epoch: 10 step: 334, loss is 1.4469225406646729\n",
      "epoch: 10 step: 335, loss is 1.3328373432159424\n",
      "epoch: 10 step: 336, loss is 1.4115924835205078\n",
      "epoch: 10 step: 337, loss is 1.4239914417266846\n",
      "epoch: 10 step: 338, loss is 1.3905354738235474\n",
      "epoch: 10 step: 339, loss is 1.4243111610412598\n",
      "epoch: 10 step: 340, loss is 1.4102673530578613\n",
      "epoch: 10 step: 341, loss is 1.5096548795700073\n",
      "epoch: 10 step: 342, loss is 1.5059409141540527\n",
      "epoch: 10 step: 343, loss is 1.4406945705413818\n",
      "epoch: 10 step: 344, loss is 1.586336612701416\n",
      "epoch: 10 step: 345, loss is 1.4521769285202026\n",
      "epoch: 10 step: 346, loss is 1.2758768796920776\n",
      "epoch: 10 step: 347, loss is 1.4453564882278442\n",
      "epoch: 10 step: 348, loss is 1.277294397354126\n",
      "epoch: 10 step: 349, loss is 1.405962586402893\n",
      "epoch: 10 step: 350, loss is 1.4659819602966309\n",
      "epoch: 10 step: 351, loss is 1.519511342048645\n",
      "epoch: 10 step: 352, loss is 1.3373225927352905\n",
      "epoch: 10 step: 353, loss is 1.3387470245361328\n",
      "epoch: 10 step: 354, loss is 1.381969928741455\n",
      "epoch: 10 step: 355, loss is 1.3549901247024536\n",
      "epoch: 10 step: 356, loss is 1.4337685108184814\n",
      "epoch: 10 step: 357, loss is 1.5138843059539795\n",
      "epoch: 10 step: 358, loss is 1.3251214027404785\n",
      "epoch: 10 step: 359, loss is 1.4757680892944336\n",
      "epoch: 10 step: 360, loss is 1.3622207641601562\n",
      "epoch: 10 step: 361, loss is 1.5004916191101074\n",
      "epoch: 10 step: 362, loss is 1.4219094514846802\n",
      "epoch: 10 step: 363, loss is 1.3670084476470947\n",
      "epoch: 10 step: 364, loss is 1.3872053623199463\n",
      "epoch: 10 step: 365, loss is 1.4404555559158325\n",
      "epoch: 10 step: 366, loss is 1.3644156455993652\n",
      "epoch: 10 step: 367, loss is 1.3780715465545654\n",
      "epoch: 10 step: 368, loss is 1.364194393157959\n",
      "epoch: 10 step: 369, loss is 1.5265395641326904\n",
      "epoch: 10 step: 370, loss is 1.4132108688354492\n",
      "epoch: 10 step: 371, loss is 1.3942673206329346\n",
      "epoch: 10 step: 372, loss is 1.3722362518310547\n",
      "epoch: 10 step: 373, loss is 1.3966928720474243\n",
      "epoch: 10 step: 374, loss is 1.3420835733413696\n",
      "epoch: 10 step: 375, loss is 1.3886182308197021\n",
      "epoch: 10 step: 376, loss is 1.3622480630874634\n",
      "epoch: 10 step: 377, loss is 1.2558598518371582\n",
      "epoch: 10 step: 378, loss is 1.3735873699188232\n",
      "epoch: 10 step: 379, loss is 1.4606744050979614\n",
      "epoch: 10 step: 380, loss is 1.4801418781280518\n",
      "epoch: 10 step: 381, loss is 1.413651704788208\n",
      "epoch: 10 step: 382, loss is 1.416835069656372\n",
      "epoch: 10 step: 383, loss is 1.4356770515441895\n",
      "epoch: 10 step: 384, loss is 1.3416517972946167\n",
      "epoch: 10 step: 385, loss is 1.4611378908157349\n",
      "epoch: 10 step: 386, loss is 1.468030571937561\n",
      "epoch: 10 step: 387, loss is 1.491861343383789\n",
      "epoch: 10 step: 388, loss is 1.3852598667144775\n",
      "epoch: 10 step: 389, loss is 1.3153661489486694\n",
      "epoch: 10 step: 390, loss is 1.3507962226867676\n",
      "Train epoch time: 166893.798 ms, per step time: 427.933 ms\n",
      "epoch: 11 step: 1, loss is 1.3533527851104736\n",
      "epoch: 11 step: 2, loss is 1.4062724113464355\n",
      "epoch: 11 step: 3, loss is 1.4549012184143066\n",
      "epoch: 11 step: 4, loss is 1.378347635269165\n",
      "epoch: 11 step: 5, loss is 1.3194539546966553\n",
      "epoch: 11 step: 6, loss is 1.3313697576522827\n",
      "epoch: 11 step: 7, loss is 1.413964867591858\n",
      "epoch: 11 step: 8, loss is 1.4095133543014526\n",
      "epoch: 11 step: 9, loss is 1.3980214595794678\n",
      "epoch: 11 step: 10, loss is 1.4514240026474\n",
      "epoch: 11 step: 11, loss is 1.446897029876709\n",
      "epoch: 11 step: 12, loss is 1.3032047748565674\n",
      "epoch: 11 step: 13, loss is 1.4627212285995483\n",
      "epoch: 11 step: 14, loss is 1.4765592813491821\n",
      "epoch: 11 step: 15, loss is 1.3482005596160889\n",
      "epoch: 11 step: 16, loss is 1.4559322595596313\n",
      "epoch: 11 step: 17, loss is 1.4119315147399902\n",
      "epoch: 11 step: 18, loss is 1.3738964796066284\n",
      "epoch: 11 step: 19, loss is 1.3994795083999634\n",
      "epoch: 11 step: 20, loss is 1.3764667510986328\n",
      "epoch: 11 step: 21, loss is 1.3502411842346191\n",
      "epoch: 11 step: 22, loss is 1.4120409488677979\n",
      "epoch: 11 step: 23, loss is 1.3886629343032837\n",
      "epoch: 11 step: 24, loss is 1.4313806295394897\n",
      "epoch: 11 step: 25, loss is 1.368979573249817\n",
      "epoch: 11 step: 26, loss is 1.4773855209350586\n",
      "epoch: 11 step: 27, loss is 1.385495662689209\n",
      "epoch: 11 step: 28, loss is 1.3862419128417969\n",
      "epoch: 11 step: 29, loss is 1.3790316581726074\n",
      "epoch: 11 step: 30, loss is 1.3757513761520386\n",
      "epoch: 11 step: 31, loss is 1.2675219774246216\n",
      "epoch: 11 step: 32, loss is 1.4752767086029053\n",
      "epoch: 11 step: 33, loss is 1.3277755975723267\n",
      "epoch: 11 step: 34, loss is 1.4477455615997314\n",
      "epoch: 11 step: 35, loss is 1.3339709043502808\n",
      "epoch: 11 step: 36, loss is 1.3176097869873047\n",
      "epoch: 11 step: 37, loss is 1.4296035766601562\n",
      "epoch: 11 step: 38, loss is 1.2999026775360107\n",
      "epoch: 11 step: 39, loss is 1.3984770774841309\n",
      "epoch: 11 step: 40, loss is 1.4455740451812744\n",
      "epoch: 11 step: 41, loss is 1.248934030532837\n",
      "epoch: 11 step: 42, loss is 1.487965703010559\n",
      "epoch: 11 step: 43, loss is 1.4236429929733276\n",
      "epoch: 11 step: 44, loss is 1.4022821187973022\n",
      "epoch: 11 step: 45, loss is 1.3670328855514526\n",
      "epoch: 11 step: 46, loss is 1.535492181777954\n",
      "epoch: 11 step: 47, loss is 1.3634408712387085\n",
      "epoch: 11 step: 48, loss is 1.258226752281189\n",
      "epoch: 11 step: 49, loss is 1.3437583446502686\n",
      "epoch: 11 step: 50, loss is 1.373476266860962\n",
      "epoch: 11 step: 51, loss is 1.3210713863372803\n",
      "epoch: 11 step: 52, loss is 1.478869915008545\n",
      "epoch: 11 step: 53, loss is 1.3247915506362915\n",
      "epoch: 11 step: 54, loss is 1.4393281936645508\n",
      "epoch: 11 step: 55, loss is 1.4216246604919434\n",
      "epoch: 11 step: 56, loss is 1.4379068613052368\n",
      "epoch: 11 step: 57, loss is 1.2610034942626953\n",
      "epoch: 11 step: 58, loss is 1.4731764793395996\n",
      "epoch: 11 step: 59, loss is 1.4094010591506958\n",
      "epoch: 11 step: 60, loss is 1.4593790769577026\n",
      "epoch: 11 step: 61, loss is 1.3194137811660767\n",
      "epoch: 11 step: 62, loss is 1.4291915893554688\n",
      "epoch: 11 step: 63, loss is 1.4412848949432373\n",
      "epoch: 11 step: 64, loss is 1.3538215160369873\n",
      "epoch: 11 step: 65, loss is 1.4474265575408936\n",
      "epoch: 11 step: 66, loss is 1.582277774810791\n",
      "epoch: 11 step: 67, loss is 1.337233066558838\n",
      "epoch: 11 step: 68, loss is 1.260064721107483\n",
      "epoch: 11 step: 69, loss is 1.4425774812698364\n",
      "epoch: 11 step: 70, loss is 1.3938069343566895\n",
      "epoch: 11 step: 71, loss is 1.3866219520568848\n",
      "epoch: 11 step: 72, loss is 1.31806480884552\n",
      "epoch: 11 step: 73, loss is 1.3720413446426392\n",
      "epoch: 11 step: 74, loss is 1.349621295928955\n",
      "epoch: 11 step: 75, loss is 1.5204771757125854\n",
      "epoch: 11 step: 76, loss is 1.3484408855438232\n",
      "epoch: 11 step: 77, loss is 1.446104884147644\n",
      "epoch: 11 step: 78, loss is 1.458963394165039\n",
      "epoch: 11 step: 79, loss is 1.330952525138855\n",
      "epoch: 11 step: 80, loss is 1.4337408542633057\n",
      "epoch: 11 step: 81, loss is 1.4102795124053955\n",
      "epoch: 11 step: 82, loss is 1.5061075687408447\n",
      "epoch: 11 step: 83, loss is 1.3993396759033203\n",
      "epoch: 11 step: 84, loss is 1.390384316444397\n",
      "epoch: 11 step: 85, loss is 1.2354481220245361\n",
      "epoch: 11 step: 86, loss is 1.4702732563018799\n",
      "epoch: 11 step: 87, loss is 1.489715576171875\n",
      "epoch: 11 step: 88, loss is 1.3408019542694092\n",
      "epoch: 11 step: 89, loss is 1.3413748741149902\n",
      "epoch: 11 step: 90, loss is 1.3799667358398438\n",
      "epoch: 11 step: 91, loss is 1.4441359043121338\n",
      "epoch: 11 step: 92, loss is 1.3294175863265991\n",
      "epoch: 11 step: 93, loss is 1.3671581745147705\n",
      "epoch: 11 step: 94, loss is 1.3549970388412476\n",
      "epoch: 11 step: 95, loss is 1.583524465560913\n",
      "epoch: 11 step: 96, loss is 1.2667567729949951\n",
      "epoch: 11 step: 97, loss is 1.4077101945877075\n",
      "epoch: 11 step: 98, loss is 1.4462599754333496\n",
      "epoch: 11 step: 99, loss is 1.4898452758789062\n",
      "epoch: 11 step: 100, loss is 1.4015710353851318\n",
      "epoch: 11 step: 101, loss is 1.2925924062728882\n",
      "epoch: 11 step: 102, loss is 1.4605573415756226\n",
      "epoch: 11 step: 103, loss is 1.479811191558838\n",
      "epoch: 11 step: 104, loss is 1.3396928310394287\n",
      "epoch: 11 step: 105, loss is 1.4850950241088867\n",
      "epoch: 11 step: 106, loss is 1.3079216480255127\n",
      "epoch: 11 step: 107, loss is 1.5143330097198486\n",
      "epoch: 11 step: 108, loss is 1.3560043573379517\n",
      "epoch: 11 step: 109, loss is 1.3110387325286865\n",
      "epoch: 11 step: 110, loss is 1.4422246217727661\n",
      "epoch: 11 step: 111, loss is 1.3385676145553589\n",
      "epoch: 11 step: 112, loss is 1.322063684463501\n",
      "epoch: 11 step: 113, loss is 1.4437427520751953\n",
      "epoch: 11 step: 114, loss is 1.3934013843536377\n",
      "epoch: 11 step: 115, loss is 1.2859951257705688\n",
      "epoch: 11 step: 116, loss is 1.3800276517868042\n",
      "epoch: 11 step: 117, loss is 1.3599295616149902\n",
      "epoch: 11 step: 118, loss is 1.3264497518539429\n",
      "epoch: 11 step: 119, loss is 1.3413710594177246\n",
      "epoch: 11 step: 120, loss is 1.3839188814163208\n",
      "epoch: 11 step: 121, loss is 1.4004703760147095\n",
      "epoch: 11 step: 122, loss is 1.3988046646118164\n",
      "epoch: 11 step: 123, loss is 1.5754880905151367\n",
      "epoch: 11 step: 124, loss is 1.4102118015289307\n",
      "epoch: 11 step: 125, loss is 1.2960498332977295\n",
      "epoch: 11 step: 126, loss is 1.417183518409729\n",
      "epoch: 11 step: 127, loss is 1.3173322677612305\n",
      "epoch: 11 step: 128, loss is 1.3623685836791992\n",
      "epoch: 11 step: 129, loss is 1.3926174640655518\n",
      "epoch: 11 step: 130, loss is 1.4973173141479492\n",
      "epoch: 11 step: 131, loss is 1.5188047885894775\n",
      "epoch: 11 step: 132, loss is 1.3999998569488525\n",
      "epoch: 11 step: 133, loss is 1.3448197841644287\n",
      "epoch: 11 step: 134, loss is 1.3212790489196777\n",
      "epoch: 11 step: 135, loss is 1.3997563123703003\n",
      "epoch: 11 step: 136, loss is 1.414564847946167\n",
      "epoch: 11 step: 137, loss is 1.307569980621338\n",
      "epoch: 11 step: 138, loss is 1.3816776275634766\n",
      "epoch: 11 step: 139, loss is 1.4171704053878784\n",
      "epoch: 11 step: 140, loss is 1.374145269393921\n",
      "epoch: 11 step: 141, loss is 1.3539669513702393\n",
      "epoch: 11 step: 142, loss is 1.4469828605651855\n",
      "epoch: 11 step: 143, loss is 1.3512680530548096\n",
      "epoch: 11 step: 144, loss is 1.272857427597046\n",
      "epoch: 11 step: 145, loss is 1.4336013793945312\n",
      "epoch: 11 step: 146, loss is 1.40696120262146\n",
      "epoch: 11 step: 147, loss is 1.3887630701065063\n",
      "epoch: 11 step: 148, loss is 1.4422775506973267\n",
      "epoch: 11 step: 149, loss is 1.3130606412887573\n",
      "epoch: 11 step: 150, loss is 1.362059473991394\n",
      "epoch: 11 step: 151, loss is 1.4288421869277954\n",
      "epoch: 11 step: 152, loss is 1.4854607582092285\n",
      "epoch: 11 step: 153, loss is 1.4028363227844238\n",
      "epoch: 11 step: 154, loss is 1.3024743795394897\n",
      "epoch: 11 step: 155, loss is 1.3944108486175537\n",
      "epoch: 11 step: 156, loss is 1.439556360244751\n",
      "epoch: 11 step: 157, loss is 1.4030683040618896\n",
      "epoch: 11 step: 158, loss is 1.4644542932510376\n",
      "epoch: 11 step: 159, loss is 1.3816485404968262\n",
      "epoch: 11 step: 160, loss is 1.432952642440796\n",
      "epoch: 11 step: 161, loss is 1.4248652458190918\n",
      "epoch: 11 step: 162, loss is 1.3220343589782715\n",
      "epoch: 11 step: 163, loss is 1.3353644609451294\n",
      "epoch: 11 step: 164, loss is 1.5137667655944824\n",
      "epoch: 11 step: 165, loss is 1.3338927030563354\n",
      "epoch: 11 step: 166, loss is 1.404653549194336\n",
      "epoch: 11 step: 167, loss is 1.4015593528747559\n",
      "epoch: 11 step: 168, loss is 1.2760610580444336\n",
      "epoch: 11 step: 169, loss is 1.5459489822387695\n",
      "epoch: 11 step: 170, loss is 1.4113609790802002\n",
      "epoch: 11 step: 171, loss is 1.416873574256897\n",
      "epoch: 11 step: 172, loss is 1.298328161239624\n",
      "epoch: 11 step: 173, loss is 1.4121034145355225\n",
      "epoch: 11 step: 174, loss is 1.3179926872253418\n",
      "epoch: 11 step: 175, loss is 1.447324275970459\n",
      "epoch: 11 step: 176, loss is 1.3243072032928467\n",
      "epoch: 11 step: 177, loss is 1.3955705165863037\n",
      "epoch: 11 step: 178, loss is 1.3118891716003418\n",
      "epoch: 11 step: 179, loss is 1.366258978843689\n",
      "epoch: 11 step: 180, loss is 1.3616197109222412\n",
      "epoch: 11 step: 181, loss is 1.3230892419815063\n",
      "epoch: 11 step: 182, loss is 1.3891236782073975\n",
      "epoch: 11 step: 183, loss is 1.4749574661254883\n",
      "epoch: 11 step: 184, loss is 1.432591199874878\n",
      "epoch: 11 step: 185, loss is 1.3988263607025146\n",
      "epoch: 11 step: 186, loss is 1.4282829761505127\n",
      "epoch: 11 step: 187, loss is 1.3395062685012817\n",
      "epoch: 11 step: 188, loss is 1.421129584312439\n",
      "epoch: 11 step: 189, loss is 1.2263506650924683\n",
      "epoch: 11 step: 190, loss is 1.3906924724578857\n",
      "epoch: 11 step: 191, loss is 1.3985228538513184\n",
      "epoch: 11 step: 192, loss is 1.4578449726104736\n",
      "epoch: 11 step: 193, loss is 1.390750765800476\n",
      "epoch: 11 step: 194, loss is 1.3700335025787354\n",
      "epoch: 11 step: 195, loss is 1.4661622047424316\n",
      "epoch: 11 step: 196, loss is 1.3318250179290771\n",
      "epoch: 11 step: 197, loss is 1.3488110303878784\n",
      "epoch: 11 step: 198, loss is 1.4707268476486206\n",
      "epoch: 11 step: 199, loss is 1.4784820079803467\n",
      "epoch: 11 step: 200, loss is 1.3667353391647339\n",
      "epoch: 11 step: 201, loss is 1.356465458869934\n",
      "epoch: 11 step: 202, loss is 1.4262945652008057\n",
      "epoch: 11 step: 203, loss is 1.3287864923477173\n",
      "epoch: 11 step: 204, loss is 1.5008039474487305\n",
      "epoch: 11 step: 205, loss is 1.4583933353424072\n",
      "epoch: 11 step: 206, loss is 1.4054971933364868\n",
      "epoch: 11 step: 207, loss is 1.383581280708313\n",
      "epoch: 11 step: 208, loss is 1.2892675399780273\n",
      "epoch: 11 step: 209, loss is 1.385603904724121\n",
      "epoch: 11 step: 210, loss is 1.3681204319000244\n",
      "epoch: 11 step: 211, loss is 1.3423211574554443\n",
      "epoch: 11 step: 212, loss is 1.277796745300293\n",
      "epoch: 11 step: 213, loss is 1.4479535818099976\n",
      "epoch: 11 step: 214, loss is 1.428305745124817\n",
      "epoch: 11 step: 215, loss is 1.433957576751709\n",
      "epoch: 11 step: 216, loss is 1.346726417541504\n",
      "epoch: 11 step: 217, loss is 1.300902247428894\n",
      "epoch: 11 step: 218, loss is 1.3987129926681519\n",
      "epoch: 11 step: 219, loss is 1.3634790182113647\n",
      "epoch: 11 step: 220, loss is 1.3441460132598877\n",
      "epoch: 11 step: 221, loss is 1.356316328048706\n",
      "epoch: 11 step: 222, loss is 1.2719693183898926\n",
      "epoch: 11 step: 223, loss is 1.2744780778884888\n",
      "epoch: 11 step: 224, loss is 1.3600095510482788\n",
      "epoch: 11 step: 225, loss is 1.347761631011963\n",
      "epoch: 11 step: 226, loss is 1.481785774230957\n",
      "epoch: 11 step: 227, loss is 1.5844058990478516\n",
      "epoch: 11 step: 228, loss is 1.3291141986846924\n",
      "epoch: 11 step: 229, loss is 1.493661642074585\n",
      "epoch: 11 step: 230, loss is 1.3214017152786255\n",
      "epoch: 11 step: 231, loss is 1.4439191818237305\n",
      "epoch: 11 step: 232, loss is 1.4384679794311523\n",
      "epoch: 11 step: 233, loss is 1.4580453634262085\n",
      "epoch: 11 step: 234, loss is 1.4866786003112793\n",
      "epoch: 11 step: 235, loss is 1.4153372049331665\n",
      "epoch: 11 step: 236, loss is 1.4343430995941162\n",
      "epoch: 11 step: 237, loss is 1.336265206336975\n",
      "epoch: 11 step: 238, loss is 1.5088971853256226\n",
      "epoch: 11 step: 239, loss is 1.5194402933120728\n",
      "epoch: 11 step: 240, loss is 1.229167103767395\n",
      "epoch: 11 step: 241, loss is 1.3337863683700562\n",
      "epoch: 11 step: 242, loss is 1.3405325412750244\n",
      "epoch: 11 step: 243, loss is 1.3515753746032715\n",
      "epoch: 11 step: 244, loss is 1.4611397981643677\n",
      "epoch: 11 step: 245, loss is 1.352358341217041\n",
      "epoch: 11 step: 246, loss is 1.3381837606430054\n",
      "epoch: 11 step: 247, loss is 1.347774863243103\n",
      "epoch: 11 step: 248, loss is 1.3855226039886475\n",
      "epoch: 11 step: 249, loss is 1.3602197170257568\n",
      "epoch: 11 step: 250, loss is 1.4864296913146973\n",
      "epoch: 11 step: 251, loss is 1.4003422260284424\n",
      "epoch: 11 step: 252, loss is 1.3491640090942383\n",
      "epoch: 11 step: 253, loss is 1.2819952964782715\n",
      "epoch: 11 step: 254, loss is 1.4326190948486328\n",
      "epoch: 11 step: 255, loss is 1.4927953481674194\n",
      "epoch: 11 step: 256, loss is 1.4319833517074585\n",
      "epoch: 11 step: 257, loss is 1.2701239585876465\n",
      "epoch: 11 step: 258, loss is 1.4443472623825073\n",
      "epoch: 11 step: 259, loss is 1.429513931274414\n",
      "epoch: 11 step: 260, loss is 1.4601808786392212\n",
      "epoch: 11 step: 261, loss is 1.417459487915039\n",
      "epoch: 11 step: 262, loss is 1.478568196296692\n",
      "epoch: 11 step: 263, loss is 1.475114107131958\n",
      "epoch: 11 step: 264, loss is 1.5635631084442139\n",
      "epoch: 11 step: 265, loss is 1.4410490989685059\n",
      "epoch: 11 step: 266, loss is 1.4492052793502808\n",
      "epoch: 11 step: 267, loss is 1.2940921783447266\n",
      "epoch: 11 step: 268, loss is 1.353484034538269\n",
      "epoch: 11 step: 269, loss is 1.341071367263794\n",
      "epoch: 11 step: 270, loss is 1.5062919855117798\n",
      "epoch: 11 step: 271, loss is 1.4915366172790527\n",
      "epoch: 11 step: 272, loss is 1.4932606220245361\n",
      "epoch: 11 step: 273, loss is 1.3964672088623047\n",
      "epoch: 11 step: 274, loss is 1.4229185581207275\n",
      "epoch: 11 step: 275, loss is 1.3717806339263916\n",
      "epoch: 11 step: 276, loss is 1.3248496055603027\n",
      "epoch: 11 step: 277, loss is 1.356281042098999\n",
      "epoch: 11 step: 278, loss is 1.4601656198501587\n",
      "epoch: 11 step: 279, loss is 1.4212642908096313\n",
      "epoch: 11 step: 280, loss is 1.3513823747634888\n",
      "epoch: 11 step: 281, loss is 1.31831955909729\n",
      "epoch: 11 step: 282, loss is 1.4718033075332642\n",
      "epoch: 11 step: 283, loss is 1.3636474609375\n",
      "epoch: 11 step: 284, loss is 1.3929444551467896\n",
      "epoch: 11 step: 285, loss is 1.4264798164367676\n",
      "epoch: 11 step: 286, loss is 1.2531975507736206\n",
      "epoch: 11 step: 287, loss is 1.3490179777145386\n",
      "epoch: 11 step: 288, loss is 1.417919397354126\n",
      "epoch: 11 step: 289, loss is 1.359269142150879\n",
      "epoch: 11 step: 290, loss is 1.4866435527801514\n",
      "epoch: 11 step: 291, loss is 1.3576548099517822\n",
      "epoch: 11 step: 292, loss is 1.4730823040008545\n",
      "epoch: 11 step: 293, loss is 1.4803136587142944\n",
      "epoch: 11 step: 294, loss is 1.369815707206726\n",
      "epoch: 11 step: 295, loss is 1.4805171489715576\n",
      "epoch: 11 step: 296, loss is 1.4026119709014893\n",
      "epoch: 11 step: 297, loss is 1.425007700920105\n",
      "epoch: 11 step: 298, loss is 1.5132466554641724\n",
      "epoch: 11 step: 299, loss is 1.4814239740371704\n",
      "epoch: 11 step: 300, loss is 1.4026249647140503\n",
      "epoch: 11 step: 301, loss is 1.5351133346557617\n",
      "epoch: 11 step: 302, loss is 1.4556937217712402\n",
      "epoch: 11 step: 303, loss is 1.4273338317871094\n",
      "epoch: 11 step: 304, loss is 1.3950221538543701\n",
      "epoch: 11 step: 305, loss is 1.3133511543273926\n",
      "epoch: 11 step: 306, loss is 1.4452564716339111\n",
      "epoch: 11 step: 307, loss is 1.4648261070251465\n",
      "epoch: 11 step: 308, loss is 1.4382638931274414\n",
      "epoch: 11 step: 309, loss is 1.4423233270645142\n",
      "epoch: 11 step: 310, loss is 1.3584327697753906\n",
      "epoch: 11 step: 311, loss is 1.4147613048553467\n",
      "epoch: 11 step: 312, loss is 1.3242967128753662\n",
      "epoch: 11 step: 313, loss is 1.3904320001602173\n",
      "epoch: 11 step: 314, loss is 1.4447929859161377\n",
      "epoch: 11 step: 315, loss is 1.4764578342437744\n",
      "epoch: 11 step: 316, loss is 1.342456579208374\n",
      "epoch: 11 step: 317, loss is 1.429892897605896\n",
      "epoch: 11 step: 318, loss is 1.387159824371338\n",
      "epoch: 11 step: 319, loss is 1.4837874174118042\n",
      "epoch: 11 step: 320, loss is 1.378340721130371\n",
      "epoch: 11 step: 321, loss is 1.4348971843719482\n",
      "epoch: 11 step: 322, loss is 1.3672142028808594\n",
      "epoch: 11 step: 323, loss is 1.3137508630752563\n",
      "epoch: 11 step: 324, loss is 1.2960377931594849\n",
      "epoch: 11 step: 325, loss is 1.440106987953186\n",
      "epoch: 11 step: 326, loss is 1.3720749616622925\n",
      "epoch: 11 step: 327, loss is 1.483551263809204\n",
      "epoch: 11 step: 328, loss is 1.4096275568008423\n",
      "epoch: 11 step: 329, loss is 1.3406565189361572\n",
      "epoch: 11 step: 330, loss is 1.29783034324646\n",
      "epoch: 11 step: 331, loss is 1.5218614339828491\n",
      "epoch: 11 step: 332, loss is 1.4084123373031616\n",
      "epoch: 11 step: 333, loss is 1.4366717338562012\n",
      "epoch: 11 step: 334, loss is 1.3426389694213867\n",
      "epoch: 11 step: 335, loss is 1.460587739944458\n",
      "epoch: 11 step: 336, loss is 1.3692121505737305\n",
      "epoch: 11 step: 337, loss is 1.4245879650115967\n",
      "epoch: 11 step: 338, loss is 1.5525734424591064\n",
      "epoch: 11 step: 339, loss is 1.2867975234985352\n",
      "epoch: 11 step: 340, loss is 1.4031615257263184\n",
      "epoch: 11 step: 341, loss is 1.2985094785690308\n",
      "epoch: 11 step: 342, loss is 1.2897820472717285\n",
      "epoch: 11 step: 343, loss is 1.5351217985153198\n",
      "epoch: 11 step: 344, loss is 1.4401195049285889\n",
      "epoch: 11 step: 345, loss is 1.3951390981674194\n",
      "epoch: 11 step: 346, loss is 1.4500149488449097\n",
      "epoch: 11 step: 347, loss is 1.367995262145996\n",
      "epoch: 11 step: 348, loss is 1.5214788913726807\n",
      "epoch: 11 step: 349, loss is 1.40605628490448\n",
      "epoch: 11 step: 350, loss is 1.376253604888916\n",
      "epoch: 11 step: 351, loss is 1.3354754447937012\n",
      "epoch: 11 step: 352, loss is 1.3392751216888428\n",
      "epoch: 11 step: 353, loss is 1.4040002822875977\n",
      "epoch: 11 step: 354, loss is 1.3284302949905396\n",
      "epoch: 11 step: 355, loss is 1.3684272766113281\n",
      "epoch: 11 step: 356, loss is 1.3963615894317627\n",
      "epoch: 11 step: 357, loss is 1.3212673664093018\n",
      "epoch: 11 step: 358, loss is 1.4052062034606934\n",
      "epoch: 11 step: 359, loss is 1.4188754558563232\n",
      "epoch: 11 step: 360, loss is 1.3397767543792725\n",
      "epoch: 11 step: 361, loss is 1.2197970151901245\n",
      "epoch: 11 step: 362, loss is 1.4973106384277344\n",
      "epoch: 11 step: 363, loss is 1.5414187908172607\n",
      "epoch: 11 step: 364, loss is 1.3911266326904297\n",
      "epoch: 11 step: 365, loss is 1.4793121814727783\n",
      "epoch: 11 step: 366, loss is 1.279910683631897\n",
      "epoch: 11 step: 367, loss is 1.3476223945617676\n",
      "epoch: 11 step: 368, loss is 1.3811497688293457\n",
      "epoch: 11 step: 369, loss is 1.3210270404815674\n",
      "epoch: 11 step: 370, loss is 1.3725947141647339\n",
      "epoch: 11 step: 371, loss is 1.309668779373169\n",
      "epoch: 11 step: 372, loss is 1.4352355003356934\n",
      "epoch: 11 step: 373, loss is 1.3224611282348633\n",
      "epoch: 11 step: 374, loss is 1.3348135948181152\n",
      "epoch: 11 step: 375, loss is 1.359503149986267\n",
      "epoch: 11 step: 376, loss is 1.3800348043441772\n",
      "epoch: 11 step: 377, loss is 1.4116215705871582\n",
      "epoch: 11 step: 378, loss is 1.4377838373184204\n",
      "epoch: 11 step: 379, loss is 1.3420754671096802\n",
      "epoch: 11 step: 380, loss is 1.4473668336868286\n",
      "epoch: 11 step: 381, loss is 1.3772852420806885\n",
      "epoch: 11 step: 382, loss is 1.4619741439819336\n",
      "epoch: 11 step: 383, loss is 1.30524480342865\n",
      "epoch: 11 step: 384, loss is 1.2701107263565063\n",
      "epoch: 11 step: 385, loss is 1.3822681903839111\n",
      "epoch: 11 step: 386, loss is 1.4624816179275513\n",
      "epoch: 11 step: 387, loss is 1.3806848526000977\n",
      "epoch: 11 step: 388, loss is 1.3025356531143188\n",
      "epoch: 11 step: 389, loss is 1.5739797353744507\n",
      "epoch: 11 step: 390, loss is 1.4799691438674927\n",
      "Train epoch time: 168968.614 ms, per step time: 433.253 ms\n",
      "epoch: 12 step: 1, loss is 1.4575005769729614\n",
      "epoch: 12 step: 2, loss is 1.2169957160949707\n",
      "epoch: 12 step: 3, loss is 1.398359775543213\n",
      "epoch: 12 step: 4, loss is 1.3417174816131592\n",
      "epoch: 12 step: 5, loss is 1.4462263584136963\n",
      "epoch: 12 step: 6, loss is 1.396597981452942\n",
      "epoch: 12 step: 7, loss is 1.3859400749206543\n",
      "epoch: 12 step: 8, loss is 1.4330682754516602\n",
      "epoch: 12 step: 9, loss is 1.3533565998077393\n",
      "epoch: 12 step: 10, loss is 1.5390212535858154\n",
      "epoch: 12 step: 11, loss is 1.5673198699951172\n",
      "epoch: 12 step: 12, loss is 1.340539813041687\n",
      "epoch: 12 step: 13, loss is 1.4582483768463135\n",
      "epoch: 12 step: 14, loss is 1.412503957748413\n",
      "epoch: 12 step: 15, loss is 1.465271234512329\n",
      "epoch: 12 step: 16, loss is 1.5108091831207275\n",
      "epoch: 12 step: 17, loss is 1.2804195880889893\n",
      "epoch: 12 step: 18, loss is 1.3874837160110474\n",
      "epoch: 12 step: 19, loss is 1.459375023841858\n",
      "epoch: 12 step: 20, loss is 1.4770255088806152\n",
      "epoch: 12 step: 21, loss is 1.391420602798462\n",
      "epoch: 12 step: 22, loss is 1.3302996158599854\n",
      "epoch: 12 step: 23, loss is 1.3280888795852661\n",
      "epoch: 12 step: 24, loss is 1.453094482421875\n",
      "epoch: 12 step: 25, loss is 1.4164680242538452\n",
      "epoch: 12 step: 26, loss is 1.3953096866607666\n",
      "epoch: 12 step: 27, loss is 1.4116970300674438\n",
      "epoch: 12 step: 28, loss is 1.3144400119781494\n",
      "epoch: 12 step: 29, loss is 1.2669041156768799\n",
      "epoch: 12 step: 30, loss is 1.504333257675171\n",
      "epoch: 12 step: 31, loss is 1.3141511678695679\n",
      "epoch: 12 step: 32, loss is 1.5582542419433594\n",
      "epoch: 12 step: 33, loss is 1.3605159521102905\n",
      "epoch: 12 step: 34, loss is 1.3425757884979248\n",
      "epoch: 12 step: 35, loss is 1.420456886291504\n",
      "epoch: 12 step: 36, loss is 1.3597815036773682\n",
      "epoch: 12 step: 37, loss is 1.3437135219573975\n",
      "epoch: 12 step: 38, loss is 1.35690438747406\n",
      "epoch: 12 step: 39, loss is 1.4647074937820435\n",
      "epoch: 12 step: 40, loss is 1.4102898836135864\n",
      "epoch: 12 step: 41, loss is 1.6690237522125244\n",
      "epoch: 12 step: 42, loss is 1.3959457874298096\n",
      "epoch: 12 step: 43, loss is 1.3138229846954346\n",
      "epoch: 12 step: 44, loss is 1.23500657081604\n",
      "epoch: 12 step: 45, loss is 1.3090057373046875\n",
      "epoch: 12 step: 46, loss is 1.3700908422470093\n",
      "epoch: 12 step: 47, loss is 1.3715310096740723\n",
      "epoch: 12 step: 48, loss is 1.3157665729522705\n",
      "epoch: 12 step: 49, loss is 1.4103697538375854\n",
      "epoch: 12 step: 50, loss is 1.3326597213745117\n",
      "epoch: 12 step: 51, loss is 1.354440450668335\n",
      "epoch: 12 step: 52, loss is 1.422831416130066\n",
      "epoch: 12 step: 53, loss is 1.442884922027588\n",
      "epoch: 12 step: 54, loss is 1.3325169086456299\n",
      "epoch: 12 step: 55, loss is 1.2873198986053467\n",
      "epoch: 12 step: 56, loss is 1.3892784118652344\n",
      "epoch: 12 step: 57, loss is 1.3355345726013184\n",
      "epoch: 12 step: 58, loss is 1.2711611986160278\n",
      "epoch: 12 step: 59, loss is 1.3772248029708862\n",
      "epoch: 12 step: 60, loss is 1.414976716041565\n",
      "epoch: 12 step: 61, loss is 1.392600655555725\n",
      "epoch: 12 step: 62, loss is 1.3042323589324951\n",
      "epoch: 12 step: 63, loss is 1.4420850276947021\n",
      "epoch: 12 step: 64, loss is 1.4272162914276123\n",
      "epoch: 12 step: 65, loss is 1.3376461267471313\n",
      "epoch: 12 step: 66, loss is 1.4756335020065308\n",
      "epoch: 12 step: 67, loss is 1.5138057470321655\n",
      "epoch: 12 step: 68, loss is 1.300392508506775\n",
      "epoch: 12 step: 69, loss is 1.3757517337799072\n",
      "epoch: 12 step: 70, loss is 1.329484224319458\n",
      "epoch: 12 step: 71, loss is 1.4136881828308105\n",
      "epoch: 12 step: 72, loss is 1.2871512174606323\n",
      "epoch: 12 step: 73, loss is 1.4841203689575195\n",
      "epoch: 12 step: 74, loss is 1.3080908060073853\n",
      "epoch: 12 step: 75, loss is 1.4800344705581665\n",
      "epoch: 12 step: 76, loss is 1.3472764492034912\n",
      "epoch: 12 step: 77, loss is 1.2535444498062134\n",
      "epoch: 12 step: 78, loss is 1.2417469024658203\n",
      "epoch: 12 step: 79, loss is 1.280178189277649\n",
      "epoch: 12 step: 80, loss is 1.4110898971557617\n",
      "epoch: 12 step: 81, loss is 1.38118314743042\n",
      "epoch: 12 step: 82, loss is 1.39896559715271\n",
      "epoch: 12 step: 83, loss is 1.2997838258743286\n",
      "epoch: 12 step: 84, loss is 1.3739863634109497\n",
      "epoch: 12 step: 85, loss is 1.336248517036438\n",
      "epoch: 12 step: 86, loss is 1.3940688371658325\n",
      "epoch: 12 step: 87, loss is 1.3662307262420654\n",
      "epoch: 12 step: 88, loss is 1.5120121240615845\n",
      "epoch: 12 step: 89, loss is 1.3345413208007812\n",
      "epoch: 12 step: 90, loss is 1.32352876663208\n",
      "epoch: 12 step: 91, loss is 1.4836640357971191\n",
      "epoch: 12 step: 92, loss is 1.2332793474197388\n",
      "epoch: 12 step: 93, loss is 1.356077790260315\n",
      "epoch: 12 step: 94, loss is 1.5142133235931396\n",
      "epoch: 12 step: 95, loss is 1.5169517993927002\n",
      "epoch: 12 step: 96, loss is 1.2828961610794067\n",
      "epoch: 12 step: 97, loss is 1.3393268585205078\n",
      "epoch: 12 step: 98, loss is 1.3835777044296265\n",
      "epoch: 12 step: 99, loss is 1.2933626174926758\n",
      "epoch: 12 step: 100, loss is 1.2784606218338013\n",
      "epoch: 12 step: 101, loss is 1.4304038286209106\n",
      "epoch: 12 step: 102, loss is 1.4435709714889526\n",
      "epoch: 12 step: 103, loss is 1.4697630405426025\n",
      "epoch: 12 step: 104, loss is 1.497185468673706\n",
      "epoch: 12 step: 105, loss is 1.3391826152801514\n",
      "epoch: 12 step: 106, loss is 1.3528063297271729\n",
      "epoch: 12 step: 107, loss is 1.4615089893341064\n",
      "epoch: 12 step: 108, loss is 1.3060389757156372\n",
      "epoch: 12 step: 109, loss is 1.3767505884170532\n",
      "epoch: 12 step: 110, loss is 1.302994728088379\n",
      "epoch: 12 step: 111, loss is 1.4573101997375488\n",
      "epoch: 12 step: 112, loss is 1.3355519771575928\n",
      "epoch: 12 step: 113, loss is 1.2581877708435059\n",
      "epoch: 12 step: 114, loss is 1.4319097995758057\n",
      "epoch: 12 step: 115, loss is 1.3143510818481445\n",
      "epoch: 12 step: 116, loss is 1.3621689081192017\n",
      "epoch: 12 step: 117, loss is 1.4478026628494263\n",
      "epoch: 12 step: 118, loss is 1.3628764152526855\n",
      "epoch: 12 step: 119, loss is 1.2910637855529785\n",
      "epoch: 12 step: 120, loss is 1.4248998165130615\n",
      "epoch: 12 step: 121, loss is 1.3445793390274048\n",
      "epoch: 12 step: 122, loss is 1.4503543376922607\n",
      "epoch: 12 step: 123, loss is 1.43185555934906\n",
      "epoch: 12 step: 124, loss is 1.3458086252212524\n",
      "epoch: 12 step: 125, loss is 1.3531221151351929\n",
      "epoch: 12 step: 126, loss is 1.3729525804519653\n",
      "epoch: 12 step: 127, loss is 1.3716357946395874\n",
      "epoch: 12 step: 128, loss is 1.3880500793457031\n",
      "epoch: 12 step: 129, loss is 1.4947686195373535\n",
      "epoch: 12 step: 130, loss is 1.347495198249817\n",
      "epoch: 12 step: 131, loss is 1.4594967365264893\n",
      "epoch: 12 step: 132, loss is 1.3242368698120117\n",
      "epoch: 12 step: 133, loss is 1.3741202354431152\n",
      "epoch: 12 step: 134, loss is 1.2656084299087524\n",
      "epoch: 12 step: 135, loss is 1.4563603401184082\n",
      "epoch: 12 step: 136, loss is 1.3144806623458862\n",
      "epoch: 12 step: 137, loss is 1.3952940702438354\n",
      "epoch: 12 step: 138, loss is 1.5291190147399902\n",
      "epoch: 12 step: 139, loss is 1.3607637882232666\n",
      "epoch: 12 step: 140, loss is 1.3321163654327393\n",
      "epoch: 12 step: 141, loss is 1.427079200744629\n",
      "epoch: 12 step: 142, loss is 1.3388761281967163\n",
      "epoch: 12 step: 143, loss is 1.275540828704834\n",
      "epoch: 12 step: 144, loss is 1.308158278465271\n",
      "epoch: 12 step: 145, loss is 1.3415470123291016\n",
      "epoch: 12 step: 146, loss is 1.3673211336135864\n",
      "epoch: 12 step: 147, loss is 1.2999154329299927\n",
      "epoch: 12 step: 148, loss is 1.3661785125732422\n",
      "epoch: 12 step: 149, loss is 1.3251656293869019\n",
      "epoch: 12 step: 150, loss is 1.5163675546646118\n",
      "epoch: 12 step: 151, loss is 1.4075992107391357\n",
      "epoch: 12 step: 152, loss is 1.3414099216461182\n",
      "epoch: 12 step: 153, loss is 1.3059921264648438\n",
      "epoch: 12 step: 154, loss is 1.4404704570770264\n",
      "epoch: 12 step: 155, loss is 1.4421446323394775\n",
      "epoch: 12 step: 156, loss is 1.4240370988845825\n",
      "epoch: 12 step: 157, loss is 1.4342647790908813\n",
      "epoch: 12 step: 158, loss is 1.2689944505691528\n",
      "epoch: 12 step: 159, loss is 1.2886803150177002\n",
      "epoch: 12 step: 160, loss is 1.411656141281128\n",
      "epoch: 12 step: 161, loss is 1.431640625\n",
      "epoch: 12 step: 162, loss is 1.4584934711456299\n",
      "epoch: 12 step: 163, loss is 1.3873116970062256\n",
      "epoch: 12 step: 164, loss is 1.3339544534683228\n",
      "epoch: 12 step: 165, loss is 1.4140042066574097\n",
      "epoch: 12 step: 166, loss is 1.3358639478683472\n",
      "epoch: 12 step: 167, loss is 1.451271414756775\n",
      "epoch: 12 step: 168, loss is 1.3950883150100708\n",
      "epoch: 12 step: 169, loss is 1.464303731918335\n",
      "epoch: 12 step: 170, loss is 1.4053802490234375\n",
      "epoch: 12 step: 171, loss is 1.3967323303222656\n",
      "epoch: 12 step: 172, loss is 1.3264377117156982\n",
      "epoch: 12 step: 173, loss is 1.3061527013778687\n",
      "epoch: 12 step: 174, loss is 1.4792733192443848\n",
      "epoch: 12 step: 175, loss is 1.4717388153076172\n",
      "epoch: 12 step: 176, loss is 1.3616137504577637\n",
      "epoch: 12 step: 177, loss is 1.3491920232772827\n",
      "epoch: 12 step: 178, loss is 1.3803744316101074\n",
      "epoch: 12 step: 179, loss is 1.2767155170440674\n",
      "epoch: 12 step: 180, loss is 1.3965950012207031\n",
      "epoch: 12 step: 181, loss is 1.3868718147277832\n",
      "epoch: 12 step: 182, loss is 1.3791579008102417\n",
      "epoch: 12 step: 183, loss is 1.517014980316162\n",
      "epoch: 12 step: 184, loss is 1.390669822692871\n",
      "epoch: 12 step: 185, loss is 1.365598201751709\n",
      "epoch: 12 step: 186, loss is 1.3441390991210938\n",
      "epoch: 12 step: 187, loss is 1.414841890335083\n",
      "epoch: 12 step: 188, loss is 1.3623319864273071\n",
      "epoch: 12 step: 189, loss is 1.4564409255981445\n",
      "epoch: 12 step: 190, loss is 1.329336166381836\n",
      "epoch: 12 step: 191, loss is 1.3186639547348022\n",
      "epoch: 12 step: 192, loss is 1.3575026988983154\n",
      "epoch: 12 step: 193, loss is 1.2621300220489502\n",
      "epoch: 12 step: 194, loss is 1.4083402156829834\n",
      "epoch: 12 step: 195, loss is 1.2757353782653809\n",
      "epoch: 12 step: 196, loss is 1.362163782119751\n",
      "epoch: 12 step: 197, loss is 1.4871037006378174\n",
      "epoch: 12 step: 198, loss is 1.3766902685165405\n",
      "epoch: 12 step: 199, loss is 1.4360952377319336\n",
      "epoch: 12 step: 200, loss is 1.4170920848846436\n",
      "epoch: 12 step: 201, loss is 1.4665517807006836\n",
      "epoch: 12 step: 202, loss is 1.3373783826828003\n",
      "epoch: 12 step: 203, loss is 1.3064391613006592\n",
      "epoch: 12 step: 204, loss is 1.4050259590148926\n",
      "epoch: 12 step: 205, loss is 1.3469712734222412\n",
      "epoch: 12 step: 206, loss is 1.4121686220169067\n",
      "epoch: 12 step: 207, loss is 1.449403166770935\n",
      "epoch: 12 step: 208, loss is 1.2952909469604492\n",
      "epoch: 12 step: 209, loss is 1.3678102493286133\n",
      "epoch: 12 step: 210, loss is 1.4038580656051636\n",
      "epoch: 12 step: 211, loss is 1.3285130262374878\n",
      "epoch: 12 step: 212, loss is 1.3708654642105103\n",
      "epoch: 12 step: 213, loss is 1.3416879177093506\n",
      "epoch: 12 step: 214, loss is 1.3838372230529785\n",
      "epoch: 12 step: 215, loss is 1.4524712562561035\n",
      "epoch: 12 step: 216, loss is 1.3481072187423706\n",
      "epoch: 12 step: 217, loss is 1.438492774963379\n",
      "epoch: 12 step: 218, loss is 1.4051940441131592\n",
      "epoch: 12 step: 219, loss is 1.2967711687088013\n",
      "epoch: 12 step: 220, loss is 1.3731327056884766\n",
      "epoch: 12 step: 221, loss is 1.4757063388824463\n",
      "epoch: 12 step: 222, loss is 1.4824893474578857\n",
      "epoch: 12 step: 223, loss is 1.3270410299301147\n",
      "epoch: 12 step: 224, loss is 1.3214917182922363\n",
      "epoch: 12 step: 225, loss is 1.3103123903274536\n",
      "epoch: 12 step: 226, loss is 1.2919666767120361\n",
      "epoch: 12 step: 227, loss is 1.4041459560394287\n",
      "epoch: 12 step: 228, loss is 1.3418140411376953\n",
      "epoch: 12 step: 229, loss is 1.4379417896270752\n",
      "epoch: 12 step: 230, loss is 1.4868868589401245\n",
      "epoch: 12 step: 231, loss is 1.341397762298584\n",
      "epoch: 12 step: 232, loss is 1.3187717199325562\n",
      "epoch: 12 step: 233, loss is 1.4010435342788696\n",
      "epoch: 12 step: 234, loss is 1.4496325254440308\n",
      "epoch: 12 step: 235, loss is 1.4194755554199219\n",
      "epoch: 12 step: 236, loss is 1.3924752473831177\n",
      "epoch: 12 step: 237, loss is 1.3801392316818237\n",
      "epoch: 12 step: 238, loss is 1.3744428157806396\n",
      "epoch: 12 step: 239, loss is 1.563830852508545\n",
      "epoch: 12 step: 240, loss is 1.383488416671753\n",
      "epoch: 12 step: 241, loss is 1.3727716207504272\n",
      "epoch: 12 step: 242, loss is 1.4060852527618408\n",
      "epoch: 12 step: 243, loss is 1.2934514284133911\n",
      "epoch: 12 step: 244, loss is 1.4457736015319824\n",
      "epoch: 12 step: 245, loss is 1.418215274810791\n",
      "epoch: 12 step: 246, loss is 1.5124398469924927\n",
      "epoch: 12 step: 247, loss is 1.473632574081421\n",
      "epoch: 12 step: 248, loss is 1.3808724880218506\n",
      "epoch: 12 step: 249, loss is 1.3038674592971802\n",
      "epoch: 12 step: 250, loss is 1.4182237386703491\n",
      "epoch: 12 step: 251, loss is 1.348897099494934\n",
      "epoch: 12 step: 252, loss is 1.195955753326416\n",
      "epoch: 12 step: 253, loss is 1.2581157684326172\n",
      "epoch: 12 step: 254, loss is 1.3306498527526855\n",
      "epoch: 12 step: 255, loss is 1.4584730863571167\n",
      "epoch: 12 step: 256, loss is 1.4447784423828125\n",
      "epoch: 12 step: 257, loss is 1.3449255228042603\n",
      "epoch: 12 step: 258, loss is 1.5160248279571533\n",
      "epoch: 12 step: 259, loss is 1.4237993955612183\n",
      "epoch: 12 step: 260, loss is 1.4911208152770996\n",
      "epoch: 12 step: 261, loss is 1.3190175294876099\n",
      "epoch: 12 step: 262, loss is 1.2707362174987793\n",
      "epoch: 12 step: 263, loss is 1.4377799034118652\n",
      "epoch: 12 step: 264, loss is 1.53594172000885\n",
      "epoch: 12 step: 265, loss is 1.5623704195022583\n",
      "epoch: 12 step: 266, loss is 1.221474528312683\n",
      "epoch: 12 step: 267, loss is 1.5774600505828857\n",
      "epoch: 12 step: 268, loss is 1.4774870872497559\n",
      "epoch: 12 step: 269, loss is 1.3016583919525146\n",
      "epoch: 12 step: 270, loss is 1.378337025642395\n",
      "epoch: 12 step: 271, loss is 1.4443504810333252\n",
      "epoch: 12 step: 272, loss is 1.3091847896575928\n",
      "epoch: 12 step: 273, loss is 1.4444553852081299\n",
      "epoch: 12 step: 274, loss is 1.4149279594421387\n",
      "epoch: 12 step: 275, loss is 1.4022021293640137\n",
      "epoch: 12 step: 276, loss is 1.3465325832366943\n",
      "epoch: 12 step: 277, loss is 1.41841459274292\n",
      "epoch: 12 step: 278, loss is 1.2616996765136719\n",
      "epoch: 12 step: 279, loss is 1.4674923419952393\n",
      "epoch: 12 step: 280, loss is 1.355926275253296\n",
      "epoch: 12 step: 281, loss is 1.2958030700683594\n",
      "epoch: 12 step: 282, loss is 1.380297303199768\n",
      "epoch: 12 step: 283, loss is 1.5495927333831787\n",
      "epoch: 12 step: 284, loss is 1.3202091455459595\n",
      "epoch: 12 step: 285, loss is 1.2520275115966797\n",
      "epoch: 12 step: 286, loss is 1.5055267810821533\n",
      "epoch: 12 step: 287, loss is 1.389134407043457\n",
      "epoch: 12 step: 288, loss is 1.3706386089324951\n",
      "epoch: 12 step: 289, loss is 1.3803423643112183\n",
      "epoch: 12 step: 290, loss is 1.3433113098144531\n",
      "epoch: 12 step: 291, loss is 1.3544460535049438\n",
      "epoch: 12 step: 292, loss is 1.3275408744812012\n",
      "epoch: 12 step: 293, loss is 1.3158986568450928\n",
      "epoch: 12 step: 294, loss is 1.2792519330978394\n",
      "epoch: 12 step: 295, loss is 1.4106980562210083\n",
      "epoch: 12 step: 296, loss is 1.4143273830413818\n",
      "epoch: 12 step: 297, loss is 1.5098446607589722\n",
      "epoch: 12 step: 298, loss is 1.3173761367797852\n",
      "epoch: 12 step: 299, loss is 1.2541476488113403\n",
      "epoch: 12 step: 300, loss is 1.3113584518432617\n",
      "epoch: 12 step: 301, loss is 1.3783307075500488\n",
      "epoch: 12 step: 302, loss is 1.4755009412765503\n",
      "epoch: 12 step: 303, loss is 1.4323713779449463\n",
      "epoch: 12 step: 304, loss is 1.348781943321228\n",
      "epoch: 12 step: 305, loss is 1.398331880569458\n",
      "epoch: 12 step: 306, loss is 1.2775731086730957\n",
      "epoch: 12 step: 307, loss is 1.3593051433563232\n",
      "epoch: 12 step: 308, loss is 1.5071450471878052\n",
      "epoch: 12 step: 309, loss is 1.5744606256484985\n",
      "epoch: 12 step: 310, loss is 1.3005695343017578\n",
      "epoch: 12 step: 311, loss is 1.3729565143585205\n",
      "epoch: 12 step: 312, loss is 1.3402204513549805\n",
      "epoch: 12 step: 313, loss is 1.1709668636322021\n",
      "epoch: 12 step: 314, loss is 1.260895013809204\n",
      "epoch: 12 step: 315, loss is 1.3188254833221436\n",
      "epoch: 12 step: 316, loss is 1.3649282455444336\n",
      "epoch: 12 step: 317, loss is 1.3528430461883545\n",
      "epoch: 12 step: 318, loss is 1.3005938529968262\n",
      "epoch: 12 step: 319, loss is 1.338496446609497\n",
      "epoch: 12 step: 320, loss is 1.2758145332336426\n",
      "epoch: 12 step: 321, loss is 1.3544856309890747\n",
      "epoch: 12 step: 322, loss is 1.3218134641647339\n",
      "epoch: 12 step: 323, loss is 1.2466726303100586\n",
      "epoch: 12 step: 324, loss is 1.4869632720947266\n",
      "epoch: 12 step: 325, loss is 1.2703330516815186\n",
      "epoch: 12 step: 326, loss is 1.3328006267547607\n",
      "epoch: 12 step: 327, loss is 1.3316760063171387\n",
      "epoch: 12 step: 328, loss is 1.5432043075561523\n",
      "epoch: 12 step: 329, loss is 1.4160000085830688\n",
      "epoch: 12 step: 330, loss is 1.3023513555526733\n",
      "epoch: 12 step: 331, loss is 1.3135138750076294\n",
      "epoch: 12 step: 332, loss is 1.3945001363754272\n",
      "epoch: 12 step: 333, loss is 1.3263580799102783\n",
      "epoch: 12 step: 334, loss is 1.328324556350708\n",
      "epoch: 12 step: 335, loss is 1.3791486024856567\n",
      "epoch: 12 step: 336, loss is 1.3906502723693848\n",
      "epoch: 12 step: 337, loss is 1.4556479454040527\n",
      "epoch: 12 step: 338, loss is 1.3982374668121338\n",
      "epoch: 12 step: 339, loss is 1.4005259275436401\n",
      "epoch: 12 step: 340, loss is 1.3594427108764648\n",
      "epoch: 12 step: 341, loss is 1.4073259830474854\n",
      "epoch: 12 step: 342, loss is 1.3484950065612793\n",
      "epoch: 12 step: 343, loss is 1.4147275686264038\n",
      "epoch: 12 step: 344, loss is 1.419739007949829\n",
      "epoch: 12 step: 345, loss is 1.3005547523498535\n",
      "epoch: 12 step: 346, loss is 1.321138620376587\n",
      "epoch: 12 step: 347, loss is 1.3713220357894897\n",
      "epoch: 12 step: 348, loss is 1.4168097972869873\n",
      "epoch: 12 step: 349, loss is 1.3111377954483032\n",
      "epoch: 12 step: 350, loss is 1.3590267896652222\n",
      "epoch: 12 step: 351, loss is 1.3923581838607788\n",
      "epoch: 12 step: 352, loss is 1.3154913187026978\n",
      "epoch: 12 step: 353, loss is 1.4467957019805908\n",
      "epoch: 12 step: 354, loss is 1.2797324657440186\n",
      "epoch: 12 step: 355, loss is 1.4426887035369873\n",
      "epoch: 12 step: 356, loss is 1.3148475885391235\n",
      "epoch: 12 step: 357, loss is 1.4103832244873047\n",
      "epoch: 12 step: 358, loss is 1.3824421167373657\n",
      "epoch: 12 step: 359, loss is 1.396376132965088\n",
      "epoch: 12 step: 360, loss is 1.4364761114120483\n",
      "epoch: 12 step: 361, loss is 1.4408230781555176\n",
      "epoch: 12 step: 362, loss is 1.363478183746338\n",
      "epoch: 12 step: 363, loss is 1.4100239276885986\n",
      "epoch: 12 step: 364, loss is 1.3800973892211914\n",
      "epoch: 12 step: 365, loss is 1.2415874004364014\n",
      "epoch: 12 step: 366, loss is 1.4211231470108032\n",
      "epoch: 12 step: 367, loss is 1.385695457458496\n",
      "epoch: 12 step: 368, loss is 1.31675124168396\n",
      "epoch: 12 step: 369, loss is 1.367766261100769\n",
      "epoch: 12 step: 370, loss is 1.2665719985961914\n",
      "epoch: 12 step: 371, loss is 1.4650228023529053\n",
      "epoch: 12 step: 372, loss is 1.4927170276641846\n",
      "epoch: 12 step: 373, loss is 1.292201280593872\n",
      "epoch: 12 step: 374, loss is 1.4237802028656006\n",
      "epoch: 12 step: 375, loss is 1.5392134189605713\n",
      "epoch: 12 step: 376, loss is 1.283443570137024\n",
      "epoch: 12 step: 377, loss is 1.3245651721954346\n",
      "epoch: 12 step: 378, loss is 1.3970248699188232\n",
      "epoch: 12 step: 379, loss is 1.362176775932312\n",
      "epoch: 12 step: 380, loss is 1.3143620491027832\n",
      "epoch: 12 step: 381, loss is 1.3145934343338013\n",
      "epoch: 12 step: 382, loss is 1.3505914211273193\n",
      "epoch: 12 step: 383, loss is 1.3413447141647339\n",
      "epoch: 12 step: 384, loss is 1.3001971244812012\n",
      "epoch: 12 step: 385, loss is 1.369572639465332\n",
      "epoch: 12 step: 386, loss is 1.2839242219924927\n",
      "epoch: 12 step: 387, loss is 1.1885687112808228\n",
      "epoch: 12 step: 388, loss is 1.3690110445022583\n",
      "epoch: 12 step: 389, loss is 1.4416382312774658\n",
      "epoch: 12 step: 390, loss is 1.4136388301849365\n",
      "Train epoch time: 164400.309 ms, per step time: 421.539 ms\n",
      "epoch: 13 step: 1, loss is 1.1313796043395996\n",
      "epoch: 13 step: 2, loss is 1.3777194023132324\n",
      "epoch: 13 step: 3, loss is 1.3132925033569336\n",
      "epoch: 13 step: 4, loss is 1.3480209112167358\n",
      "epoch: 13 step: 5, loss is 1.2688676118850708\n",
      "epoch: 13 step: 6, loss is 1.3092666864395142\n",
      "epoch: 13 step: 7, loss is 1.3891913890838623\n",
      "epoch: 13 step: 8, loss is 1.3165934085845947\n",
      "epoch: 13 step: 9, loss is 1.2880994081497192\n",
      "epoch: 13 step: 10, loss is 1.286957025527954\n",
      "epoch: 13 step: 11, loss is 1.305450439453125\n",
      "epoch: 13 step: 12, loss is 1.384692907333374\n",
      "epoch: 13 step: 13, loss is 1.2988693714141846\n",
      "epoch: 13 step: 14, loss is 1.3366085290908813\n",
      "epoch: 13 step: 15, loss is 1.3925701379776\n",
      "epoch: 13 step: 16, loss is 1.2376686334609985\n",
      "epoch: 13 step: 17, loss is 1.3304004669189453\n",
      "epoch: 13 step: 18, loss is 1.3015371561050415\n",
      "epoch: 13 step: 19, loss is 1.3366972208023071\n",
      "epoch: 13 step: 20, loss is 1.382440447807312\n",
      "epoch: 13 step: 21, loss is 1.2961747646331787\n",
      "epoch: 13 step: 22, loss is 1.4273227453231812\n",
      "epoch: 13 step: 23, loss is 1.334200143814087\n",
      "epoch: 13 step: 24, loss is 1.2600232362747192\n",
      "epoch: 13 step: 25, loss is 1.412683129310608\n",
      "epoch: 13 step: 26, loss is 1.404062271118164\n",
      "epoch: 13 step: 27, loss is 1.3026148080825806\n",
      "epoch: 13 step: 28, loss is 1.3204360008239746\n",
      "epoch: 13 step: 29, loss is 1.3197498321533203\n",
      "epoch: 13 step: 30, loss is 1.3600192070007324\n",
      "epoch: 13 step: 31, loss is 1.207903504371643\n",
      "epoch: 13 step: 32, loss is 1.3255062103271484\n",
      "epoch: 13 step: 33, loss is 1.3342022895812988\n",
      "epoch: 13 step: 34, loss is 1.2656807899475098\n",
      "epoch: 13 step: 35, loss is 1.2134369611740112\n",
      "epoch: 13 step: 36, loss is 1.4214228391647339\n",
      "epoch: 13 step: 37, loss is 1.3724868297576904\n",
      "epoch: 13 step: 38, loss is 1.476966142654419\n",
      "epoch: 13 step: 39, loss is 1.3639147281646729\n",
      "epoch: 13 step: 40, loss is 1.3557802438735962\n",
      "epoch: 13 step: 41, loss is 1.3500981330871582\n",
      "epoch: 13 step: 42, loss is 1.4100735187530518\n",
      "epoch: 13 step: 43, loss is 1.2934906482696533\n",
      "epoch: 13 step: 44, loss is 1.401688814163208\n",
      "epoch: 13 step: 45, loss is 1.3515598773956299\n",
      "epoch: 13 step: 46, loss is 1.37261962890625\n",
      "epoch: 13 step: 47, loss is 1.2377541065216064\n",
      "epoch: 13 step: 48, loss is 1.4350919723510742\n",
      "epoch: 13 step: 49, loss is 1.3345588445663452\n",
      "epoch: 13 step: 50, loss is 1.3932995796203613\n",
      "epoch: 13 step: 51, loss is 1.2749590873718262\n",
      "epoch: 13 step: 52, loss is 1.2872183322906494\n",
      "epoch: 13 step: 53, loss is 1.3644514083862305\n",
      "epoch: 13 step: 54, loss is 1.347597599029541\n",
      "epoch: 13 step: 55, loss is 1.3117809295654297\n",
      "epoch: 13 step: 56, loss is 1.3229491710662842\n",
      "epoch: 13 step: 57, loss is 1.2260525226593018\n",
      "epoch: 13 step: 58, loss is 1.374983787536621\n",
      "epoch: 13 step: 59, loss is 1.3691134452819824\n",
      "epoch: 13 step: 60, loss is 1.2811566591262817\n",
      "epoch: 13 step: 61, loss is 1.3171212673187256\n",
      "epoch: 13 step: 62, loss is 1.3072142601013184\n",
      "epoch: 13 step: 63, loss is 1.4012845754623413\n",
      "epoch: 13 step: 64, loss is 1.2521045207977295\n",
      "epoch: 13 step: 65, loss is 1.355018138885498\n",
      "epoch: 13 step: 66, loss is 1.3814575672149658\n",
      "epoch: 13 step: 67, loss is 1.426079511642456\n",
      "epoch: 13 step: 68, loss is 1.3361068964004517\n",
      "epoch: 13 step: 69, loss is 1.3713096380233765\n",
      "epoch: 13 step: 70, loss is 1.2667793035507202\n",
      "epoch: 13 step: 71, loss is 1.240684986114502\n",
      "epoch: 13 step: 72, loss is 1.3068690299987793\n",
      "epoch: 13 step: 73, loss is 1.286186695098877\n",
      "epoch: 13 step: 74, loss is 1.385169267654419\n",
      "epoch: 13 step: 75, loss is 1.4276155233383179\n",
      "epoch: 13 step: 76, loss is 1.3430182933807373\n",
      "epoch: 13 step: 77, loss is 1.2036211490631104\n",
      "epoch: 13 step: 78, loss is 1.4265589714050293\n",
      "epoch: 13 step: 79, loss is 1.3860275745391846\n",
      "epoch: 13 step: 80, loss is 1.3883990049362183\n",
      "epoch: 13 step: 81, loss is 1.320868730545044\n",
      "epoch: 13 step: 82, loss is 1.3053864240646362\n",
      "epoch: 13 step: 83, loss is 1.2572436332702637\n",
      "epoch: 13 step: 84, loss is 1.4699759483337402\n",
      "epoch: 13 step: 85, loss is 1.3675036430358887\n",
      "epoch: 13 step: 86, loss is 1.2641589641571045\n",
      "epoch: 13 step: 87, loss is 1.2753894329071045\n",
      "epoch: 13 step: 88, loss is 1.3795651197433472\n",
      "epoch: 13 step: 89, loss is 1.3580873012542725\n",
      "epoch: 13 step: 90, loss is 1.3609437942504883\n",
      "epoch: 13 step: 91, loss is 1.4879868030548096\n",
      "epoch: 13 step: 92, loss is 1.2811927795410156\n",
      "epoch: 13 step: 93, loss is 1.407930850982666\n",
      "epoch: 13 step: 94, loss is 1.3287272453308105\n",
      "epoch: 13 step: 95, loss is 1.321256399154663\n",
      "epoch: 13 step: 96, loss is 1.4027405977249146\n",
      "epoch: 13 step: 97, loss is 1.2851901054382324\n",
      "epoch: 13 step: 98, loss is 1.3273108005523682\n",
      "epoch: 13 step: 99, loss is 1.4240466356277466\n",
      "epoch: 13 step: 100, loss is 1.3695753812789917\n",
      "epoch: 13 step: 101, loss is 1.3315699100494385\n",
      "epoch: 13 step: 102, loss is 1.3514370918273926\n",
      "epoch: 13 step: 103, loss is 1.350149154663086\n",
      "epoch: 13 step: 104, loss is 1.3492987155914307\n",
      "epoch: 13 step: 105, loss is 1.4194622039794922\n",
      "epoch: 13 step: 106, loss is 1.328790545463562\n",
      "epoch: 13 step: 107, loss is 1.2829334735870361\n",
      "epoch: 13 step: 108, loss is 1.3187683820724487\n",
      "epoch: 13 step: 109, loss is 1.4169765710830688\n",
      "epoch: 13 step: 110, loss is 1.4913172721862793\n",
      "epoch: 13 step: 111, loss is 1.3841805458068848\n",
      "epoch: 13 step: 112, loss is 1.3091020584106445\n",
      "epoch: 13 step: 113, loss is 1.335904836654663\n",
      "epoch: 13 step: 114, loss is 1.2580149173736572\n",
      "epoch: 13 step: 115, loss is 1.2994377613067627\n",
      "epoch: 13 step: 116, loss is 1.441725254058838\n",
      "epoch: 13 step: 117, loss is 1.276145339012146\n",
      "epoch: 13 step: 118, loss is 1.2885395288467407\n",
      "epoch: 13 step: 119, loss is 1.3198317289352417\n",
      "epoch: 13 step: 120, loss is 1.3864755630493164\n",
      "epoch: 13 step: 121, loss is 1.2557088136672974\n",
      "epoch: 13 step: 122, loss is 1.444607138633728\n",
      "epoch: 13 step: 123, loss is 1.4264466762542725\n",
      "epoch: 13 step: 124, loss is 1.4350744485855103\n",
      "epoch: 13 step: 125, loss is 1.4527699947357178\n",
      "epoch: 13 step: 126, loss is 1.3718392848968506\n",
      "epoch: 13 step: 127, loss is 1.3641014099121094\n",
      "epoch: 13 step: 128, loss is 1.2796781063079834\n",
      "epoch: 13 step: 129, loss is 1.3950960636138916\n",
      "epoch: 13 step: 130, loss is 1.5875475406646729\n",
      "epoch: 13 step: 131, loss is 1.354750633239746\n",
      "epoch: 13 step: 132, loss is 1.4048960208892822\n",
      "epoch: 13 step: 133, loss is 1.5003000497817993\n",
      "epoch: 13 step: 134, loss is 1.5400362014770508\n",
      "epoch: 13 step: 135, loss is 1.3490654230117798\n",
      "epoch: 13 step: 136, loss is 1.3299142122268677\n",
      "epoch: 13 step: 137, loss is 1.4947736263275146\n",
      "epoch: 13 step: 138, loss is 1.3483797311782837\n",
      "epoch: 13 step: 139, loss is 1.31879723072052\n",
      "epoch: 13 step: 140, loss is 1.4064586162567139\n",
      "epoch: 13 step: 141, loss is 1.3602778911590576\n",
      "epoch: 13 step: 142, loss is 1.3635926246643066\n",
      "epoch: 13 step: 143, loss is 1.2582720518112183\n",
      "epoch: 13 step: 144, loss is 1.292203426361084\n",
      "epoch: 13 step: 145, loss is 1.571421504020691\n",
      "epoch: 13 step: 146, loss is 1.3387820720672607\n",
      "epoch: 13 step: 147, loss is 1.3349015712738037\n",
      "epoch: 13 step: 148, loss is 1.4762029647827148\n",
      "epoch: 13 step: 149, loss is 1.2781803607940674\n",
      "epoch: 13 step: 150, loss is 1.485748529434204\n",
      "epoch: 13 step: 151, loss is 1.3654042482376099\n",
      "epoch: 13 step: 152, loss is 1.2176282405853271\n",
      "epoch: 13 step: 153, loss is 1.4227230548858643\n",
      "epoch: 13 step: 154, loss is 1.5042189359664917\n",
      "epoch: 13 step: 155, loss is 1.4632129669189453\n",
      "epoch: 13 step: 156, loss is 1.4514423608779907\n",
      "epoch: 13 step: 157, loss is 1.359581470489502\n",
      "epoch: 13 step: 158, loss is 1.3850572109222412\n",
      "epoch: 13 step: 159, loss is 1.3759684562683105\n",
      "epoch: 13 step: 160, loss is 1.2940590381622314\n",
      "epoch: 13 step: 161, loss is 1.3391395807266235\n",
      "epoch: 13 step: 162, loss is 1.231637716293335\n",
      "epoch: 13 step: 163, loss is 1.3453739881515503\n",
      "epoch: 13 step: 164, loss is 1.2830454111099243\n",
      "epoch: 13 step: 165, loss is 1.345806360244751\n",
      "epoch: 13 step: 166, loss is 1.4185090065002441\n",
      "epoch: 13 step: 167, loss is 1.4427103996276855\n",
      "epoch: 13 step: 168, loss is 1.3340976238250732\n",
      "epoch: 13 step: 169, loss is 1.482353687286377\n",
      "epoch: 13 step: 170, loss is 1.3706550598144531\n",
      "epoch: 13 step: 171, loss is 1.4760091304779053\n",
      "epoch: 13 step: 172, loss is 1.355637788772583\n",
      "epoch: 13 step: 173, loss is 1.4514251947402954\n",
      "epoch: 13 step: 174, loss is 1.3671256303787231\n",
      "epoch: 13 step: 175, loss is 1.332965612411499\n",
      "epoch: 13 step: 176, loss is 1.3609957695007324\n",
      "epoch: 13 step: 177, loss is 1.3385403156280518\n",
      "epoch: 13 step: 178, loss is 1.30120849609375\n",
      "epoch: 13 step: 179, loss is 1.4469257593154907\n",
      "epoch: 13 step: 180, loss is 1.4133344888687134\n",
      "epoch: 13 step: 181, loss is 1.4450936317443848\n",
      "epoch: 13 step: 182, loss is 1.3827167749404907\n",
      "epoch: 13 step: 183, loss is 1.3118025064468384\n",
      "epoch: 13 step: 184, loss is 1.3163148164749146\n",
      "epoch: 13 step: 185, loss is 1.3917689323425293\n",
      "epoch: 13 step: 186, loss is 1.2691353559494019\n",
      "epoch: 13 step: 187, loss is 1.48140549659729\n",
      "epoch: 13 step: 188, loss is 1.4295026063919067\n",
      "epoch: 13 step: 189, loss is 1.3771319389343262\n",
      "epoch: 13 step: 190, loss is 1.2605063915252686\n",
      "epoch: 13 step: 191, loss is 1.448603868484497\n",
      "epoch: 13 step: 192, loss is 1.3645439147949219\n",
      "epoch: 13 step: 193, loss is 1.389114499092102\n",
      "epoch: 13 step: 194, loss is 1.4457188844680786\n",
      "epoch: 13 step: 195, loss is 1.310877799987793\n",
      "epoch: 13 step: 196, loss is 1.3684196472167969\n",
      "epoch: 13 step: 197, loss is 1.3467390537261963\n",
      "epoch: 13 step: 198, loss is 1.4606553316116333\n",
      "epoch: 13 step: 199, loss is 1.3425363302230835\n",
      "epoch: 13 step: 200, loss is 1.3505500555038452\n",
      "epoch: 13 step: 201, loss is 1.3483695983886719\n",
      "epoch: 13 step: 202, loss is 1.289661169052124\n",
      "epoch: 13 step: 203, loss is 1.4834564924240112\n",
      "epoch: 13 step: 204, loss is 1.3281831741333008\n",
      "epoch: 13 step: 205, loss is 1.2937161922454834\n",
      "epoch: 13 step: 206, loss is 1.2760108709335327\n",
      "epoch: 13 step: 207, loss is 1.3682869672775269\n",
      "epoch: 13 step: 208, loss is 1.4423075914382935\n",
      "epoch: 13 step: 209, loss is 1.2808747291564941\n",
      "epoch: 13 step: 210, loss is 1.347224235534668\n",
      "epoch: 13 step: 211, loss is 1.284347414970398\n",
      "epoch: 13 step: 212, loss is 1.3712074756622314\n",
      "epoch: 13 step: 213, loss is 1.3365623950958252\n",
      "epoch: 13 step: 214, loss is 1.2841198444366455\n",
      "epoch: 13 step: 215, loss is 1.3611571788787842\n",
      "epoch: 13 step: 216, loss is 1.2594525814056396\n",
      "epoch: 13 step: 217, loss is 1.3371738195419312\n",
      "epoch: 13 step: 218, loss is 1.3197922706604004\n",
      "epoch: 13 step: 219, loss is 1.3491398096084595\n",
      "epoch: 13 step: 220, loss is 1.4300717115402222\n",
      "epoch: 13 step: 221, loss is 1.2695294618606567\n",
      "epoch: 13 step: 222, loss is 1.3841092586517334\n",
      "epoch: 13 step: 223, loss is 1.3654413223266602\n",
      "epoch: 13 step: 224, loss is 1.378871202468872\n",
      "epoch: 13 step: 225, loss is 1.3419644832611084\n",
      "epoch: 13 step: 226, loss is 1.383886694908142\n",
      "epoch: 13 step: 227, loss is 1.4502977132797241\n",
      "epoch: 13 step: 228, loss is 1.309422254562378\n",
      "epoch: 13 step: 229, loss is 1.318457841873169\n",
      "epoch: 13 step: 230, loss is 1.4122651815414429\n",
      "epoch: 13 step: 231, loss is 1.2781833410263062\n",
      "epoch: 13 step: 232, loss is 1.3076512813568115\n",
      "epoch: 13 step: 233, loss is 1.3827688694000244\n",
      "epoch: 13 step: 234, loss is 1.422060251235962\n",
      "epoch: 13 step: 235, loss is 1.4431222677230835\n",
      "epoch: 13 step: 236, loss is 1.2541708946228027\n",
      "epoch: 13 step: 237, loss is 1.4426312446594238\n",
      "epoch: 13 step: 238, loss is 1.3558675050735474\n",
      "epoch: 13 step: 239, loss is 1.384291648864746\n",
      "epoch: 13 step: 240, loss is 1.4481226205825806\n",
      "epoch: 13 step: 241, loss is 1.3431317806243896\n",
      "epoch: 13 step: 242, loss is 1.3490207195281982\n",
      "epoch: 13 step: 243, loss is 1.3039063215255737\n",
      "epoch: 13 step: 244, loss is 1.489704966545105\n",
      "epoch: 13 step: 245, loss is 1.4863718748092651\n",
      "epoch: 13 step: 246, loss is 1.4479866027832031\n",
      "epoch: 13 step: 247, loss is 1.3400847911834717\n",
      "epoch: 13 step: 248, loss is 1.3145921230316162\n",
      "epoch: 13 step: 249, loss is 1.4613783359527588\n",
      "epoch: 13 step: 250, loss is 1.3855345249176025\n",
      "epoch: 13 step: 251, loss is 1.4234908819198608\n",
      "epoch: 13 step: 252, loss is 1.2882885932922363\n",
      "epoch: 13 step: 253, loss is 1.4269917011260986\n",
      "epoch: 13 step: 254, loss is 1.3623319864273071\n",
      "epoch: 13 step: 255, loss is 1.2474526166915894\n",
      "epoch: 13 step: 256, loss is 1.2767119407653809\n",
      "epoch: 13 step: 257, loss is 1.3972424268722534\n",
      "epoch: 13 step: 258, loss is 1.2957792282104492\n",
      "epoch: 13 step: 259, loss is 1.381443977355957\n",
      "epoch: 13 step: 260, loss is 1.4199090003967285\n",
      "epoch: 13 step: 261, loss is 1.2100369930267334\n",
      "epoch: 13 step: 262, loss is 1.3905082941055298\n",
      "epoch: 13 step: 263, loss is 1.2552077770233154\n",
      "epoch: 13 step: 264, loss is 1.301703691482544\n",
      "epoch: 13 step: 265, loss is 1.3056718111038208\n",
      "epoch: 13 step: 266, loss is 1.2914667129516602\n",
      "epoch: 13 step: 267, loss is 1.182145595550537\n",
      "epoch: 13 step: 268, loss is 1.4751548767089844\n",
      "epoch: 13 step: 269, loss is 1.3004801273345947\n",
      "epoch: 13 step: 270, loss is 1.378922462463379\n",
      "epoch: 13 step: 271, loss is 1.4138888120651245\n",
      "epoch: 13 step: 272, loss is 1.395951509475708\n",
      "epoch: 13 step: 273, loss is 1.3428568840026855\n",
      "epoch: 13 step: 274, loss is 1.264520525932312\n",
      "epoch: 13 step: 275, loss is 1.4086053371429443\n",
      "epoch: 13 step: 276, loss is 1.3097596168518066\n",
      "epoch: 13 step: 277, loss is 1.3146461248397827\n",
      "epoch: 13 step: 278, loss is 1.4444814920425415\n",
      "epoch: 13 step: 279, loss is 1.2727794647216797\n",
      "epoch: 13 step: 280, loss is 1.4707590341567993\n",
      "epoch: 13 step: 281, loss is 1.3385964632034302\n",
      "epoch: 13 step: 282, loss is 1.2965725660324097\n",
      "epoch: 13 step: 283, loss is 1.3051007986068726\n",
      "epoch: 13 step: 284, loss is 1.4677753448486328\n",
      "epoch: 13 step: 285, loss is 1.3311978578567505\n",
      "epoch: 13 step: 286, loss is 1.3438843488693237\n",
      "epoch: 13 step: 287, loss is 1.3389793634414673\n",
      "epoch: 13 step: 288, loss is 1.387879490852356\n",
      "epoch: 13 step: 289, loss is 1.3127599954605103\n",
      "epoch: 13 step: 290, loss is 1.328244924545288\n",
      "epoch: 13 step: 291, loss is 1.3820092678070068\n",
      "epoch: 13 step: 292, loss is 1.2896617650985718\n",
      "epoch: 13 step: 293, loss is 1.3299486637115479\n",
      "epoch: 13 step: 294, loss is 1.4742963314056396\n",
      "epoch: 13 step: 295, loss is 1.366229772567749\n",
      "epoch: 13 step: 296, loss is 1.3386611938476562\n",
      "epoch: 13 step: 297, loss is 1.4492952823638916\n",
      "epoch: 13 step: 298, loss is 1.2729597091674805\n",
      "epoch: 13 step: 299, loss is 1.3525216579437256\n",
      "epoch: 13 step: 300, loss is 1.3897746801376343\n",
      "epoch: 13 step: 301, loss is 1.246193289756775\n",
      "epoch: 13 step: 302, loss is 1.2498841285705566\n",
      "epoch: 13 step: 303, loss is 1.4646689891815186\n",
      "epoch: 13 step: 304, loss is 1.3852967023849487\n",
      "epoch: 13 step: 305, loss is 1.4625120162963867\n",
      "epoch: 13 step: 306, loss is 1.4135832786560059\n",
      "epoch: 13 step: 307, loss is 1.3437267541885376\n",
      "epoch: 13 step: 308, loss is 1.3297456502914429\n",
      "epoch: 13 step: 309, loss is 1.430569052696228\n",
      "epoch: 13 step: 310, loss is 1.484799861907959\n",
      "epoch: 13 step: 311, loss is 1.3706982135772705\n",
      "epoch: 13 step: 312, loss is 1.4528756141662598\n",
      "epoch: 13 step: 313, loss is 1.323481559753418\n",
      "epoch: 13 step: 314, loss is 1.3585926294326782\n",
      "epoch: 13 step: 315, loss is 1.367226243019104\n",
      "epoch: 13 step: 316, loss is 1.2600480318069458\n",
      "epoch: 13 step: 317, loss is 1.3239885568618774\n",
      "epoch: 13 step: 318, loss is 1.3151506185531616\n",
      "epoch: 13 step: 319, loss is 1.4922549724578857\n",
      "epoch: 13 step: 320, loss is 1.2975457906723022\n",
      "epoch: 13 step: 321, loss is 1.320465087890625\n",
      "epoch: 13 step: 322, loss is 1.4407814741134644\n",
      "epoch: 13 step: 323, loss is 1.311631679534912\n",
      "epoch: 13 step: 324, loss is 1.4554526805877686\n",
      "epoch: 13 step: 325, loss is 1.4704817533493042\n",
      "epoch: 13 step: 326, loss is 1.3227760791778564\n",
      "epoch: 13 step: 327, loss is 1.4474132061004639\n",
      "epoch: 13 step: 328, loss is 1.2651865482330322\n",
      "epoch: 13 step: 329, loss is 1.2624043226242065\n",
      "epoch: 13 step: 330, loss is 1.240702748298645\n",
      "epoch: 13 step: 331, loss is 1.3540045022964478\n",
      "epoch: 13 step: 332, loss is 1.2864264249801636\n",
      "epoch: 13 step: 333, loss is 1.3883100748062134\n",
      "epoch: 13 step: 334, loss is 1.2940279245376587\n",
      "epoch: 13 step: 335, loss is 1.2510480880737305\n",
      "epoch: 13 step: 336, loss is 1.4480326175689697\n",
      "epoch: 13 step: 337, loss is 1.2520272731781006\n",
      "epoch: 13 step: 338, loss is 1.402818202972412\n",
      "epoch: 13 step: 339, loss is 1.4573324918746948\n",
      "epoch: 13 step: 340, loss is 1.335496425628662\n",
      "epoch: 13 step: 341, loss is 1.4665395021438599\n",
      "epoch: 13 step: 342, loss is 1.3176965713500977\n",
      "epoch: 13 step: 343, loss is 1.325836420059204\n",
      "epoch: 13 step: 344, loss is 1.4086992740631104\n",
      "epoch: 13 step: 345, loss is 1.3610173463821411\n",
      "epoch: 13 step: 346, loss is 1.3188093900680542\n",
      "epoch: 13 step: 347, loss is 1.3317409753799438\n",
      "epoch: 13 step: 348, loss is 1.3321936130523682\n",
      "epoch: 13 step: 349, loss is 1.2423341274261475\n",
      "epoch: 13 step: 350, loss is 1.2951173782348633\n",
      "epoch: 13 step: 351, loss is 1.199411153793335\n",
      "epoch: 13 step: 352, loss is 1.3521831035614014\n",
      "epoch: 13 step: 353, loss is 1.2431930303573608\n",
      "epoch: 13 step: 354, loss is 1.3357006311416626\n",
      "epoch: 13 step: 355, loss is 1.490436315536499\n",
      "epoch: 13 step: 356, loss is 1.2619915008544922\n",
      "epoch: 13 step: 357, loss is 1.226406455039978\n",
      "epoch: 13 step: 358, loss is 1.4143495559692383\n",
      "epoch: 13 step: 359, loss is 1.41985023021698\n",
      "epoch: 13 step: 360, loss is 1.3753970861434937\n",
      "epoch: 13 step: 361, loss is 1.413233995437622\n",
      "epoch: 13 step: 362, loss is 1.2884244918823242\n",
      "epoch: 13 step: 363, loss is 1.3610293865203857\n",
      "epoch: 13 step: 364, loss is 1.371727466583252\n",
      "epoch: 13 step: 365, loss is 1.288512945175171\n",
      "epoch: 13 step: 366, loss is 1.3746721744537354\n",
      "epoch: 13 step: 367, loss is 1.2869763374328613\n",
      "epoch: 13 step: 368, loss is 1.3508414030075073\n",
      "epoch: 13 step: 369, loss is 1.3288968801498413\n",
      "epoch: 13 step: 370, loss is 1.4753375053405762\n",
      "epoch: 13 step: 371, loss is 1.373205542564392\n",
      "epoch: 13 step: 372, loss is 1.449927806854248\n",
      "epoch: 13 step: 373, loss is 1.3248317241668701\n",
      "epoch: 13 step: 374, loss is 1.344604253768921\n",
      "epoch: 13 step: 375, loss is 1.3516079187393188\n",
      "epoch: 13 step: 376, loss is 1.2383155822753906\n",
      "epoch: 13 step: 377, loss is 1.3526413440704346\n",
      "epoch: 13 step: 378, loss is 1.4081847667694092\n",
      "epoch: 13 step: 379, loss is 1.25929856300354\n",
      "epoch: 13 step: 380, loss is 1.3024091720581055\n",
      "epoch: 13 step: 381, loss is 1.4178804159164429\n",
      "epoch: 13 step: 382, loss is 1.329888105392456\n",
      "epoch: 13 step: 383, loss is 1.4211806058883667\n",
      "epoch: 13 step: 384, loss is 1.3390636444091797\n",
      "epoch: 13 step: 385, loss is 1.2948447465896606\n",
      "epoch: 13 step: 386, loss is 1.1919974088668823\n",
      "epoch: 13 step: 387, loss is 1.3913174867630005\n",
      "epoch: 13 step: 388, loss is 1.3359789848327637\n",
      "epoch: 13 step: 389, loss is 1.2566454410552979\n",
      "epoch: 13 step: 390, loss is 1.2670570611953735\n",
      "Train epoch time: 164012.789 ms, per step time: 420.546 ms\n",
      "epoch: 14 step: 1, loss is 1.3820432424545288\n",
      "epoch: 14 step: 2, loss is 1.3988544940948486\n",
      "epoch: 14 step: 3, loss is 1.44037926197052\n",
      "epoch: 14 step: 4, loss is 1.3165861368179321\n",
      "epoch: 14 step: 5, loss is 1.3309838771820068\n",
      "epoch: 14 step: 6, loss is 1.312131643295288\n",
      "epoch: 14 step: 7, loss is 1.2740932703018188\n",
      "epoch: 14 step: 8, loss is 1.444584846496582\n",
      "epoch: 14 step: 9, loss is 1.4038090705871582\n",
      "epoch: 14 step: 10, loss is 1.2803914546966553\n",
      "epoch: 14 step: 11, loss is 1.45950186252594\n",
      "epoch: 14 step: 12, loss is 1.326427698135376\n",
      "epoch: 14 step: 13, loss is 1.3256670236587524\n",
      "epoch: 14 step: 14, loss is 1.2385139465332031\n",
      "epoch: 14 step: 15, loss is 1.389953374862671\n",
      "epoch: 14 step: 16, loss is 1.5847618579864502\n",
      "epoch: 14 step: 17, loss is 1.3327291011810303\n",
      "epoch: 14 step: 18, loss is 1.3334797620773315\n",
      "epoch: 14 step: 19, loss is 1.3536601066589355\n",
      "epoch: 14 step: 20, loss is 1.2485082149505615\n",
      "epoch: 14 step: 21, loss is 1.4152452945709229\n",
      "epoch: 14 step: 22, loss is 1.2363135814666748\n",
      "epoch: 14 step: 23, loss is 1.265105128288269\n",
      "epoch: 14 step: 24, loss is 1.2803475856781006\n",
      "epoch: 14 step: 25, loss is 1.2192277908325195\n",
      "epoch: 14 step: 26, loss is 1.4072332382202148\n",
      "epoch: 14 step: 27, loss is 1.388995885848999\n",
      "epoch: 14 step: 28, loss is 1.4504945278167725\n",
      "epoch: 14 step: 29, loss is 1.2764816284179688\n",
      "epoch: 14 step: 30, loss is 1.293769359588623\n",
      "epoch: 14 step: 31, loss is 1.3116194009780884\n",
      "epoch: 14 step: 32, loss is 1.2671113014221191\n",
      "epoch: 14 step: 33, loss is 1.3612704277038574\n",
      "epoch: 14 step: 34, loss is 1.351881504058838\n",
      "epoch: 14 step: 35, loss is 1.3193250894546509\n",
      "epoch: 14 step: 36, loss is 1.567345380783081\n",
      "epoch: 14 step: 37, loss is 1.4090402126312256\n",
      "epoch: 14 step: 38, loss is 1.3462570905685425\n",
      "epoch: 14 step: 39, loss is 1.3070670366287231\n",
      "epoch: 14 step: 40, loss is 1.4266417026519775\n",
      "epoch: 14 step: 41, loss is 1.336753487586975\n",
      "epoch: 14 step: 42, loss is 1.3771753311157227\n",
      "epoch: 14 step: 43, loss is 1.4196760654449463\n",
      "epoch: 14 step: 44, loss is 1.4284844398498535\n",
      "epoch: 14 step: 45, loss is 1.3962057828903198\n",
      "epoch: 14 step: 46, loss is 1.3125652074813843\n",
      "epoch: 14 step: 47, loss is 1.3141062259674072\n",
      "epoch: 14 step: 48, loss is 1.2943758964538574\n",
      "epoch: 14 step: 49, loss is 1.3912944793701172\n",
      "epoch: 14 step: 50, loss is 1.3462424278259277\n",
      "epoch: 14 step: 51, loss is 1.3683229684829712\n",
      "epoch: 14 step: 52, loss is 1.221687912940979\n",
      "epoch: 14 step: 53, loss is 1.3189035654067993\n",
      "epoch: 14 step: 54, loss is 1.4343184232711792\n",
      "epoch: 14 step: 55, loss is 1.4071590900421143\n",
      "epoch: 14 step: 56, loss is 1.3248963356018066\n",
      "epoch: 14 step: 57, loss is 1.314147710800171\n",
      "epoch: 14 step: 58, loss is 1.3146969079971313\n",
      "epoch: 14 step: 59, loss is 1.3688913583755493\n",
      "epoch: 14 step: 60, loss is 1.3472967147827148\n",
      "epoch: 14 step: 61, loss is 1.2865488529205322\n",
      "epoch: 14 step: 62, loss is 1.4259220361709595\n",
      "epoch: 14 step: 63, loss is 1.2059632539749146\n",
      "epoch: 14 step: 64, loss is 1.3173549175262451\n",
      "epoch: 14 step: 65, loss is 1.3058218955993652\n",
      "epoch: 14 step: 66, loss is 1.3091332912445068\n",
      "epoch: 14 step: 67, loss is 1.2651675939559937\n",
      "epoch: 14 step: 68, loss is 1.2342934608459473\n",
      "epoch: 14 step: 69, loss is 1.4348223209381104\n",
      "epoch: 14 step: 70, loss is 1.3626171350479126\n",
      "epoch: 14 step: 71, loss is 1.32725191116333\n",
      "epoch: 14 step: 72, loss is 1.3325213193893433\n",
      "epoch: 14 step: 73, loss is 1.427154541015625\n",
      "epoch: 14 step: 74, loss is 1.4158718585968018\n",
      "epoch: 14 step: 75, loss is 1.3237682580947876\n",
      "epoch: 14 step: 76, loss is 1.35526704788208\n",
      "epoch: 14 step: 77, loss is 1.3755000829696655\n",
      "epoch: 14 step: 78, loss is 1.4333816766738892\n",
      "epoch: 14 step: 79, loss is 1.3537517786026\n",
      "epoch: 14 step: 80, loss is 1.3565657138824463\n",
      "epoch: 14 step: 81, loss is 1.3308887481689453\n",
      "epoch: 14 step: 82, loss is 1.37437903881073\n",
      "epoch: 14 step: 83, loss is 1.1645127534866333\n",
      "epoch: 14 step: 84, loss is 1.3659762144088745\n",
      "epoch: 14 step: 85, loss is 1.3674888610839844\n",
      "epoch: 14 step: 86, loss is 1.3828328847885132\n",
      "epoch: 14 step: 87, loss is 1.2619062662124634\n",
      "epoch: 14 step: 88, loss is 1.3203941583633423\n",
      "epoch: 14 step: 89, loss is 1.2810572385787964\n",
      "epoch: 14 step: 90, loss is 1.3943299055099487\n",
      "epoch: 14 step: 91, loss is 1.2407755851745605\n",
      "epoch: 14 step: 92, loss is 1.329437017440796\n",
      "epoch: 14 step: 93, loss is 1.3568000793457031\n",
      "epoch: 14 step: 94, loss is 1.3865987062454224\n",
      "epoch: 14 step: 95, loss is 1.403960943222046\n",
      "epoch: 14 step: 96, loss is 1.2614246606826782\n",
      "epoch: 14 step: 97, loss is 1.3357430696487427\n",
      "epoch: 14 step: 98, loss is 1.4112327098846436\n",
      "epoch: 14 step: 99, loss is 1.3679943084716797\n",
      "epoch: 14 step: 100, loss is 1.3032722473144531\n",
      "epoch: 14 step: 101, loss is 1.2956223487854004\n",
      "epoch: 14 step: 102, loss is 1.194948673248291\n",
      "epoch: 14 step: 103, loss is 1.3244378566741943\n",
      "epoch: 14 step: 104, loss is 1.4588592052459717\n",
      "epoch: 14 step: 105, loss is 1.2602591514587402\n",
      "epoch: 14 step: 106, loss is 1.311894178390503\n",
      "epoch: 14 step: 107, loss is 1.338555932044983\n",
      "epoch: 14 step: 108, loss is 1.1802493333816528\n",
      "epoch: 14 step: 109, loss is 1.3266756534576416\n",
      "epoch: 14 step: 110, loss is 1.2873892784118652\n",
      "epoch: 14 step: 111, loss is 1.4871705770492554\n",
      "epoch: 14 step: 112, loss is 1.264780879020691\n",
      "epoch: 14 step: 113, loss is 1.1624234914779663\n",
      "epoch: 14 step: 114, loss is 1.403479814529419\n",
      "epoch: 14 step: 115, loss is 1.2717348337173462\n",
      "epoch: 14 step: 116, loss is 1.3023768663406372\n",
      "epoch: 14 step: 117, loss is 1.401051640510559\n",
      "epoch: 14 step: 118, loss is 1.3821372985839844\n",
      "epoch: 14 step: 119, loss is 1.5064775943756104\n",
      "epoch: 14 step: 120, loss is 1.4430208206176758\n",
      "epoch: 14 step: 121, loss is 1.318174123764038\n",
      "epoch: 14 step: 122, loss is 1.323336124420166\n",
      "epoch: 14 step: 123, loss is 1.2534676790237427\n",
      "epoch: 14 step: 124, loss is 1.2940293550491333\n",
      "epoch: 14 step: 125, loss is 1.335729956626892\n",
      "epoch: 14 step: 126, loss is 1.3610560894012451\n",
      "epoch: 14 step: 127, loss is 1.297709584236145\n",
      "epoch: 14 step: 128, loss is 1.253353238105774\n",
      "epoch: 14 step: 129, loss is 1.3203058242797852\n",
      "epoch: 14 step: 130, loss is 1.3302401304244995\n",
      "epoch: 14 step: 131, loss is 1.4190607070922852\n",
      "epoch: 14 step: 132, loss is 1.2770761251449585\n",
      "epoch: 14 step: 133, loss is 1.2712205648422241\n",
      "epoch: 14 step: 134, loss is 1.1911165714263916\n",
      "epoch: 14 step: 135, loss is 1.4290821552276611\n",
      "epoch: 14 step: 136, loss is 1.3439866304397583\n",
      "epoch: 14 step: 137, loss is 1.3236192464828491\n",
      "epoch: 14 step: 138, loss is 1.3204154968261719\n",
      "epoch: 14 step: 139, loss is 1.517498254776001\n",
      "epoch: 14 step: 140, loss is 1.5076630115509033\n",
      "epoch: 14 step: 141, loss is 1.2937711477279663\n",
      "epoch: 14 step: 142, loss is 1.4091804027557373\n",
      "epoch: 14 step: 143, loss is 1.2833917140960693\n",
      "epoch: 14 step: 144, loss is 1.3329365253448486\n",
      "epoch: 14 step: 145, loss is 1.3095464706420898\n",
      "epoch: 14 step: 146, loss is 1.2628669738769531\n",
      "epoch: 14 step: 147, loss is 1.3097373247146606\n",
      "epoch: 14 step: 148, loss is 1.2371556758880615\n",
      "epoch: 14 step: 149, loss is 1.3604414463043213\n",
      "epoch: 14 step: 150, loss is 1.3334815502166748\n",
      "epoch: 14 step: 151, loss is 1.4597026109695435\n",
      "epoch: 14 step: 152, loss is 1.2237701416015625\n",
      "epoch: 14 step: 153, loss is 1.2066782712936401\n",
      "epoch: 14 step: 154, loss is 1.301513433456421\n",
      "epoch: 14 step: 155, loss is 1.3683602809906006\n",
      "epoch: 14 step: 156, loss is 1.259139060974121\n",
      "epoch: 14 step: 157, loss is 1.275590181350708\n",
      "epoch: 14 step: 158, loss is 1.4143133163452148\n",
      "epoch: 14 step: 159, loss is 1.3079321384429932\n",
      "epoch: 14 step: 160, loss is 1.3742742538452148\n",
      "epoch: 14 step: 161, loss is 1.3145532608032227\n",
      "epoch: 14 step: 162, loss is 1.2991304397583008\n",
      "epoch: 14 step: 163, loss is 1.3389015197753906\n",
      "epoch: 14 step: 164, loss is 1.2646232843399048\n",
      "epoch: 14 step: 165, loss is 1.3502051830291748\n",
      "epoch: 14 step: 166, loss is 1.3836538791656494\n",
      "epoch: 14 step: 167, loss is 1.3416199684143066\n",
      "epoch: 14 step: 168, loss is 1.1845409870147705\n",
      "epoch: 14 step: 169, loss is 1.4301395416259766\n",
      "epoch: 14 step: 170, loss is 1.30153226852417\n",
      "epoch: 14 step: 171, loss is 1.2869956493377686\n",
      "epoch: 14 step: 172, loss is 1.2141319513320923\n",
      "epoch: 14 step: 173, loss is 1.2882635593414307\n",
      "epoch: 14 step: 174, loss is 1.290184497833252\n",
      "epoch: 14 step: 175, loss is 1.368354320526123\n",
      "epoch: 14 step: 176, loss is 1.4250744581222534\n",
      "epoch: 14 step: 177, loss is 1.2846717834472656\n",
      "epoch: 14 step: 178, loss is 1.2918031215667725\n",
      "epoch: 14 step: 179, loss is 1.36636221408844\n",
      "epoch: 14 step: 180, loss is 1.3512439727783203\n",
      "epoch: 14 step: 181, loss is 1.3434861898422241\n",
      "epoch: 14 step: 182, loss is 1.3584818840026855\n",
      "epoch: 14 step: 183, loss is 1.3039255142211914\n",
      "epoch: 14 step: 184, loss is 1.2725200653076172\n",
      "epoch: 14 step: 185, loss is 1.3198503255844116\n",
      "epoch: 14 step: 186, loss is 1.205281376838684\n",
      "epoch: 14 step: 187, loss is 1.2965763807296753\n",
      "epoch: 14 step: 188, loss is 1.318892002105713\n",
      "epoch: 14 step: 189, loss is 1.3729712963104248\n",
      "epoch: 14 step: 190, loss is 1.2936557531356812\n",
      "epoch: 14 step: 191, loss is 1.2843492031097412\n",
      "epoch: 14 step: 192, loss is 1.3270139694213867\n",
      "epoch: 14 step: 193, loss is 1.2458996772766113\n",
      "epoch: 14 step: 194, loss is 1.311541199684143\n",
      "epoch: 14 step: 195, loss is 1.3819419145584106\n",
      "epoch: 14 step: 196, loss is 1.32559335231781\n",
      "epoch: 14 step: 197, loss is 1.2778173685073853\n",
      "epoch: 14 step: 198, loss is 1.334712028503418\n",
      "epoch: 14 step: 199, loss is 1.3644928932189941\n",
      "epoch: 14 step: 200, loss is 1.3328055143356323\n",
      "epoch: 14 step: 201, loss is 1.2643910646438599\n",
      "epoch: 14 step: 202, loss is 1.3510849475860596\n",
      "epoch: 14 step: 203, loss is 1.1813523769378662\n",
      "epoch: 14 step: 204, loss is 1.3621041774749756\n",
      "epoch: 14 step: 205, loss is 1.2436147928237915\n",
      "epoch: 14 step: 206, loss is 1.2626893520355225\n",
      "epoch: 14 step: 207, loss is 1.2106854915618896\n",
      "epoch: 14 step: 208, loss is 1.435660719871521\n",
      "epoch: 14 step: 209, loss is 1.3922826051712036\n",
      "epoch: 14 step: 210, loss is 1.3041965961456299\n",
      "epoch: 14 step: 211, loss is 1.2607917785644531\n",
      "epoch: 14 step: 212, loss is 1.270673155784607\n",
      "epoch: 14 step: 213, loss is 1.3641457557678223\n",
      "epoch: 14 step: 214, loss is 1.3840360641479492\n",
      "epoch: 14 step: 215, loss is 1.2830939292907715\n",
      "epoch: 14 step: 216, loss is 1.2449872493743896\n",
      "epoch: 14 step: 217, loss is 1.327172875404358\n",
      "epoch: 14 step: 218, loss is 1.3793716430664062\n",
      "epoch: 14 step: 219, loss is 1.3801074028015137\n",
      "epoch: 14 step: 220, loss is 1.3254469633102417\n",
      "epoch: 14 step: 221, loss is 1.3059251308441162\n",
      "epoch: 14 step: 222, loss is 1.3645546436309814\n",
      "epoch: 14 step: 223, loss is 1.445265769958496\n",
      "epoch: 14 step: 224, loss is 1.281970500946045\n",
      "epoch: 14 step: 225, loss is 1.238432765007019\n",
      "epoch: 14 step: 226, loss is 1.4846866130828857\n",
      "epoch: 14 step: 227, loss is 1.178356409072876\n",
      "epoch: 14 step: 228, loss is 1.3496383428573608\n",
      "epoch: 14 step: 229, loss is 1.2825051546096802\n",
      "epoch: 14 step: 230, loss is 1.2687373161315918\n",
      "epoch: 14 step: 231, loss is 1.3108717203140259\n",
      "epoch: 14 step: 232, loss is 1.3948638439178467\n",
      "epoch: 14 step: 233, loss is 1.2207551002502441\n",
      "epoch: 14 step: 234, loss is 1.2505606412887573\n",
      "epoch: 14 step: 235, loss is 1.2477062940597534\n",
      "epoch: 14 step: 236, loss is 1.4341553449630737\n",
      "epoch: 14 step: 237, loss is 1.316131353378296\n",
      "epoch: 14 step: 238, loss is 1.3400018215179443\n",
      "epoch: 14 step: 239, loss is 1.3541463613510132\n",
      "epoch: 14 step: 240, loss is 1.2917128801345825\n",
      "epoch: 14 step: 241, loss is 1.1819357872009277\n",
      "epoch: 14 step: 242, loss is 1.2815226316452026\n",
      "epoch: 14 step: 243, loss is 1.3485743999481201\n",
      "epoch: 14 step: 244, loss is 1.3465673923492432\n",
      "epoch: 14 step: 245, loss is 1.3365917205810547\n",
      "epoch: 14 step: 246, loss is 1.281179428100586\n",
      "epoch: 14 step: 247, loss is 1.3094648122787476\n",
      "epoch: 14 step: 248, loss is 1.2420518398284912\n",
      "epoch: 14 step: 249, loss is 1.2727433443069458\n",
      "epoch: 14 step: 250, loss is 1.3179923295974731\n",
      "epoch: 14 step: 251, loss is 1.3126215934753418\n",
      "epoch: 14 step: 252, loss is 1.329329252243042\n",
      "epoch: 14 step: 253, loss is 1.349677324295044\n",
      "epoch: 14 step: 254, loss is 1.3258042335510254\n",
      "epoch: 14 step: 255, loss is 1.390818476676941\n",
      "epoch: 14 step: 256, loss is 1.298513412475586\n",
      "epoch: 14 step: 257, loss is 1.1694215536117554\n",
      "epoch: 14 step: 258, loss is 1.2265362739562988\n",
      "epoch: 14 step: 259, loss is 1.3352991342544556\n",
      "epoch: 14 step: 260, loss is 1.2848464250564575\n",
      "epoch: 14 step: 261, loss is 1.3871345520019531\n",
      "epoch: 14 step: 262, loss is 1.2474443912506104\n",
      "epoch: 14 step: 263, loss is 1.2512059211730957\n",
      "epoch: 14 step: 264, loss is 1.2949951887130737\n",
      "epoch: 14 step: 265, loss is 1.5024014711380005\n",
      "epoch: 14 step: 266, loss is 1.1330318450927734\n",
      "epoch: 14 step: 267, loss is 1.3618507385253906\n",
      "epoch: 14 step: 268, loss is 1.2834241390228271\n",
      "epoch: 14 step: 269, loss is 1.2651212215423584\n",
      "epoch: 14 step: 270, loss is 1.4045720100402832\n",
      "epoch: 14 step: 271, loss is 1.332750678062439\n",
      "epoch: 14 step: 272, loss is 1.3089972734451294\n",
      "epoch: 14 step: 273, loss is 1.375593662261963\n",
      "epoch: 14 step: 274, loss is 1.4078805446624756\n",
      "epoch: 14 step: 275, loss is 1.3310315608978271\n",
      "epoch: 14 step: 276, loss is 1.3757506608963013\n",
      "epoch: 14 step: 277, loss is 1.2933082580566406\n",
      "epoch: 14 step: 278, loss is 1.3530322313308716\n",
      "epoch: 14 step: 279, loss is 1.3478175401687622\n",
      "epoch: 14 step: 280, loss is 1.414634108543396\n",
      "epoch: 14 step: 281, loss is 1.271622896194458\n",
      "epoch: 14 step: 282, loss is 1.3884718418121338\n",
      "epoch: 14 step: 283, loss is 1.39842689037323\n",
      "epoch: 14 step: 284, loss is 1.3656357526779175\n",
      "epoch: 14 step: 285, loss is 1.344670057296753\n",
      "epoch: 14 step: 286, loss is 1.4194122552871704\n",
      "epoch: 14 step: 287, loss is 1.278622031211853\n",
      "epoch: 14 step: 288, loss is 1.2929754257202148\n",
      "epoch: 14 step: 289, loss is 1.1950528621673584\n",
      "epoch: 14 step: 290, loss is 1.269293189048767\n",
      "epoch: 14 step: 291, loss is 1.1963787078857422\n",
      "epoch: 14 step: 292, loss is 1.2820591926574707\n",
      "epoch: 14 step: 293, loss is 1.2396966218948364\n",
      "epoch: 14 step: 294, loss is 1.2518291473388672\n",
      "epoch: 14 step: 295, loss is 1.3802796602249146\n",
      "epoch: 14 step: 296, loss is 1.3412803411483765\n",
      "epoch: 14 step: 297, loss is 1.2290488481521606\n",
      "epoch: 14 step: 298, loss is 1.3067152500152588\n",
      "epoch: 14 step: 299, loss is 1.3347361087799072\n",
      "epoch: 14 step: 300, loss is 1.3188773393630981\n",
      "epoch: 14 step: 301, loss is 1.4055564403533936\n",
      "epoch: 14 step: 302, loss is 1.3016663789749146\n",
      "epoch: 14 step: 303, loss is 1.148848533630371\n",
      "epoch: 14 step: 304, loss is 1.295877456665039\n",
      "epoch: 14 step: 305, loss is 1.3605623245239258\n",
      "epoch: 14 step: 306, loss is 1.326126217842102\n",
      "epoch: 14 step: 307, loss is 1.365221381187439\n",
      "epoch: 14 step: 308, loss is 1.2236456871032715\n",
      "epoch: 14 step: 309, loss is 1.2964234352111816\n",
      "epoch: 14 step: 310, loss is 1.228425145149231\n",
      "epoch: 14 step: 311, loss is 1.2795476913452148\n",
      "epoch: 14 step: 312, loss is 1.312882900238037\n",
      "epoch: 14 step: 313, loss is 1.444687843322754\n",
      "epoch: 14 step: 314, loss is 1.2529494762420654\n",
      "epoch: 14 step: 315, loss is 1.3206578493118286\n",
      "epoch: 14 step: 316, loss is 1.3627870082855225\n",
      "epoch: 14 step: 317, loss is 1.3490047454833984\n",
      "epoch: 14 step: 318, loss is 1.2490155696868896\n",
      "epoch: 14 step: 319, loss is 1.2738189697265625\n",
      "epoch: 14 step: 320, loss is 1.3418378829956055\n",
      "epoch: 14 step: 321, loss is 1.1854325532913208\n",
      "epoch: 14 step: 322, loss is 1.3136649131774902\n",
      "epoch: 14 step: 323, loss is 1.232125997543335\n",
      "epoch: 14 step: 324, loss is 1.3020716905593872\n",
      "epoch: 14 step: 325, loss is 1.3393293619155884\n",
      "epoch: 14 step: 326, loss is 1.373765468597412\n",
      "epoch: 14 step: 327, loss is 1.2324293851852417\n",
      "epoch: 14 step: 328, loss is 1.3639649152755737\n",
      "epoch: 14 step: 329, loss is 1.2484469413757324\n",
      "epoch: 14 step: 330, loss is 1.5103065967559814\n",
      "epoch: 14 step: 331, loss is 1.3553967475891113\n",
      "epoch: 14 step: 332, loss is 1.2550418376922607\n",
      "epoch: 14 step: 333, loss is 1.3202247619628906\n",
      "epoch: 14 step: 334, loss is 1.333214282989502\n",
      "epoch: 14 step: 335, loss is 1.280381202697754\n",
      "epoch: 14 step: 336, loss is 1.4301550388336182\n",
      "epoch: 14 step: 337, loss is 1.3960835933685303\n",
      "epoch: 14 step: 338, loss is 1.3546030521392822\n",
      "epoch: 14 step: 339, loss is 1.363744854927063\n",
      "epoch: 14 step: 340, loss is 1.3488290309906006\n",
      "epoch: 14 step: 341, loss is 1.2983479499816895\n",
      "epoch: 14 step: 342, loss is 1.244167447090149\n",
      "epoch: 14 step: 343, loss is 1.3838553428649902\n",
      "epoch: 14 step: 344, loss is 1.3306766748428345\n",
      "epoch: 14 step: 345, loss is 1.321505069732666\n",
      "epoch: 14 step: 346, loss is 1.5193493366241455\n",
      "epoch: 14 step: 347, loss is 1.353715419769287\n",
      "epoch: 14 step: 348, loss is 1.3563343286514282\n",
      "epoch: 14 step: 349, loss is 1.3194153308868408\n",
      "epoch: 14 step: 350, loss is 1.4075289964675903\n",
      "epoch: 14 step: 351, loss is 1.3665356636047363\n",
      "epoch: 14 step: 352, loss is 1.3024801015853882\n",
      "epoch: 14 step: 353, loss is 1.3858015537261963\n",
      "epoch: 14 step: 354, loss is 1.3261879682540894\n",
      "epoch: 14 step: 355, loss is 1.3382991552352905\n",
      "epoch: 14 step: 356, loss is 1.3279259204864502\n",
      "epoch: 14 step: 357, loss is 1.4396158456802368\n",
      "epoch: 14 step: 358, loss is 1.3543509244918823\n",
      "epoch: 14 step: 359, loss is 1.2216860055923462\n",
      "epoch: 14 step: 360, loss is 1.3580317497253418\n",
      "epoch: 14 step: 361, loss is 1.4056577682495117\n",
      "epoch: 14 step: 362, loss is 1.388514757156372\n",
      "epoch: 14 step: 363, loss is 1.300505518913269\n",
      "epoch: 14 step: 364, loss is 1.3412983417510986\n",
      "epoch: 14 step: 365, loss is 1.262030839920044\n",
      "epoch: 14 step: 366, loss is 1.313127040863037\n",
      "epoch: 14 step: 367, loss is 1.3061866760253906\n",
      "epoch: 14 step: 368, loss is 1.279683232307434\n",
      "epoch: 14 step: 369, loss is 1.29667329788208\n",
      "epoch: 14 step: 370, loss is 1.2581735849380493\n",
      "epoch: 14 step: 371, loss is 1.3516676425933838\n",
      "epoch: 14 step: 372, loss is 1.363648772239685\n",
      "epoch: 14 step: 373, loss is 1.3323755264282227\n",
      "epoch: 14 step: 374, loss is 1.413699746131897\n",
      "epoch: 14 step: 375, loss is 1.2947030067443848\n",
      "epoch: 14 step: 376, loss is 1.3421568870544434\n",
      "epoch: 14 step: 377, loss is 1.3673369884490967\n",
      "epoch: 14 step: 378, loss is 1.43874192237854\n",
      "epoch: 14 step: 379, loss is 1.2915805578231812\n",
      "epoch: 14 step: 380, loss is 1.382466197013855\n",
      "epoch: 14 step: 381, loss is 1.3667199611663818\n",
      "epoch: 14 step: 382, loss is 1.2697299718856812\n",
      "epoch: 14 step: 383, loss is 1.3531296253204346\n",
      "epoch: 14 step: 384, loss is 1.3972890377044678\n",
      "epoch: 14 step: 385, loss is 1.4511511325836182\n",
      "epoch: 14 step: 386, loss is 1.4228136539459229\n",
      "epoch: 14 step: 387, loss is 1.3742316961288452\n",
      "epoch: 14 step: 388, loss is 1.4357120990753174\n",
      "epoch: 14 step: 389, loss is 1.33022141456604\n",
      "epoch: 14 step: 390, loss is 1.216259241104126\n",
      "Train epoch time: 154936.760 ms, per step time: 397.274 ms\n",
      "epoch: 15 step: 1, loss is 1.3185220956802368\n",
      "epoch: 15 step: 2, loss is 1.3718132972717285\n",
      "epoch: 15 step: 3, loss is 1.4133288860321045\n",
      "epoch: 15 step: 4, loss is 1.2920836210250854\n",
      "epoch: 15 step: 5, loss is 1.2707054615020752\n",
      "epoch: 15 step: 6, loss is 1.308013916015625\n",
      "epoch: 15 step: 7, loss is 1.2908930778503418\n",
      "epoch: 15 step: 8, loss is 1.2969779968261719\n",
      "epoch: 15 step: 9, loss is 1.24564528465271\n",
      "epoch: 15 step: 10, loss is 1.3105392456054688\n",
      "epoch: 15 step: 11, loss is 1.2900470495224\n",
      "epoch: 15 step: 12, loss is 1.3784806728363037\n",
      "epoch: 15 step: 13, loss is 1.2775098085403442\n",
      "epoch: 15 step: 14, loss is 1.3354730606079102\n",
      "epoch: 15 step: 15, loss is 1.3768750429153442\n",
      "epoch: 15 step: 16, loss is 1.3532772064208984\n",
      "epoch: 15 step: 17, loss is 1.391183614730835\n",
      "epoch: 15 step: 18, loss is 1.379991888999939\n",
      "epoch: 15 step: 19, loss is 1.3182330131530762\n",
      "epoch: 15 step: 20, loss is 1.2087905406951904\n",
      "epoch: 15 step: 21, loss is 1.338545322418213\n",
      "epoch: 15 step: 22, loss is 1.3953583240509033\n",
      "epoch: 15 step: 23, loss is 1.3369604349136353\n",
      "epoch: 15 step: 24, loss is 1.391701340675354\n",
      "epoch: 15 step: 25, loss is 1.2755136489868164\n",
      "epoch: 15 step: 26, loss is 1.3066494464874268\n",
      "epoch: 15 step: 27, loss is 1.2440474033355713\n",
      "epoch: 15 step: 28, loss is 1.233601689338684\n",
      "epoch: 15 step: 29, loss is 1.3454935550689697\n",
      "epoch: 15 step: 30, loss is 1.2544476985931396\n",
      "epoch: 15 step: 31, loss is 1.3631181716918945\n",
      "epoch: 15 step: 32, loss is 1.3887758255004883\n",
      "epoch: 15 step: 33, loss is 1.262282133102417\n",
      "epoch: 15 step: 34, loss is 1.3034467697143555\n",
      "epoch: 15 step: 35, loss is 1.180389642715454\n",
      "epoch: 15 step: 36, loss is 1.3402924537658691\n",
      "epoch: 15 step: 37, loss is 1.3511910438537598\n",
      "epoch: 15 step: 38, loss is 1.2570245265960693\n",
      "epoch: 15 step: 39, loss is 1.2981783151626587\n",
      "epoch: 15 step: 40, loss is 1.3181147575378418\n",
      "epoch: 15 step: 41, loss is 1.2035614252090454\n",
      "epoch: 15 step: 42, loss is 1.266947865486145\n",
      "epoch: 15 step: 43, loss is 1.3767881393432617\n",
      "epoch: 15 step: 44, loss is 1.2466375827789307\n",
      "epoch: 15 step: 45, loss is 1.4142098426818848\n",
      "epoch: 15 step: 46, loss is 1.3081742525100708\n",
      "epoch: 15 step: 47, loss is 1.3347409963607788\n",
      "epoch: 15 step: 48, loss is 1.3824933767318726\n",
      "epoch: 15 step: 49, loss is 1.3581663370132446\n",
      "epoch: 15 step: 50, loss is 1.1915258169174194\n",
      "epoch: 15 step: 51, loss is 1.4124581813812256\n",
      "epoch: 15 step: 52, loss is 1.3016526699066162\n",
      "epoch: 15 step: 53, loss is 1.2562330961227417\n",
      "epoch: 15 step: 54, loss is 1.35615074634552\n",
      "epoch: 15 step: 55, loss is 1.241188406944275\n",
      "epoch: 15 step: 56, loss is 1.3875020742416382\n",
      "epoch: 15 step: 57, loss is 1.3917896747589111\n",
      "epoch: 15 step: 58, loss is 1.315521001815796\n",
      "epoch: 15 step: 59, loss is 1.1489337682724\n",
      "epoch: 15 step: 60, loss is 1.294872522354126\n",
      "epoch: 15 step: 61, loss is 1.2693934440612793\n",
      "epoch: 15 step: 62, loss is 1.352743148803711\n",
      "epoch: 15 step: 63, loss is 1.2878925800323486\n",
      "epoch: 15 step: 64, loss is 1.3251627683639526\n",
      "epoch: 15 step: 65, loss is 1.3354077339172363\n",
      "epoch: 15 step: 66, loss is 1.229630470275879\n",
      "epoch: 15 step: 67, loss is 1.297262191772461\n",
      "epoch: 15 step: 68, loss is 1.2726659774780273\n",
      "epoch: 15 step: 69, loss is 1.2979485988616943\n",
      "epoch: 15 step: 70, loss is 1.4514089822769165\n",
      "epoch: 15 step: 71, loss is 1.3381295204162598\n",
      "epoch: 15 step: 72, loss is 1.3555872440338135\n",
      "epoch: 15 step: 73, loss is 1.348781704902649\n",
      "epoch: 15 step: 74, loss is 1.246524691581726\n",
      "epoch: 15 step: 75, loss is 1.4120142459869385\n",
      "epoch: 15 step: 76, loss is 1.3063418865203857\n",
      "epoch: 15 step: 77, loss is 1.2970260381698608\n",
      "epoch: 15 step: 78, loss is 1.2735220193862915\n",
      "epoch: 15 step: 79, loss is 1.2033932209014893\n",
      "epoch: 15 step: 80, loss is 1.3450772762298584\n",
      "epoch: 15 step: 81, loss is 1.3544788360595703\n",
      "epoch: 15 step: 82, loss is 1.378816843032837\n",
      "epoch: 15 step: 83, loss is 1.3232290744781494\n",
      "epoch: 15 step: 84, loss is 1.2955577373504639\n",
      "epoch: 15 step: 85, loss is 1.3239589929580688\n",
      "epoch: 15 step: 86, loss is 1.3474953174591064\n",
      "epoch: 15 step: 87, loss is 1.3541253805160522\n",
      "epoch: 15 step: 88, loss is 1.3007268905639648\n",
      "epoch: 15 step: 89, loss is 1.285284399986267\n",
      "epoch: 15 step: 90, loss is 1.4182382822036743\n",
      "epoch: 15 step: 91, loss is 1.3209774494171143\n",
      "epoch: 15 step: 92, loss is 1.3924399614334106\n",
      "epoch: 15 step: 93, loss is 1.3596611022949219\n",
      "epoch: 15 step: 94, loss is 1.2706265449523926\n",
      "epoch: 15 step: 95, loss is 1.2375948429107666\n",
      "epoch: 15 step: 96, loss is 1.2922054529190063\n",
      "epoch: 15 step: 97, loss is 1.2996944189071655\n",
      "epoch: 15 step: 98, loss is 1.3117765188217163\n",
      "epoch: 15 step: 99, loss is 1.2771539688110352\n",
      "epoch: 15 step: 100, loss is 1.2637044191360474\n",
      "epoch: 15 step: 101, loss is 1.4139673709869385\n",
      "epoch: 15 step: 102, loss is 1.3912041187286377\n",
      "epoch: 15 step: 103, loss is 1.2713924646377563\n",
      "epoch: 15 step: 104, loss is 1.291444182395935\n",
      "epoch: 15 step: 105, loss is 1.4581553936004639\n",
      "epoch: 15 step: 106, loss is 1.3874549865722656\n",
      "epoch: 15 step: 107, loss is 1.4307398796081543\n",
      "epoch: 15 step: 108, loss is 1.3525068759918213\n",
      "epoch: 15 step: 109, loss is 1.3363209962844849\n",
      "epoch: 15 step: 110, loss is 1.3069143295288086\n",
      "epoch: 15 step: 111, loss is 1.2585647106170654\n",
      "epoch: 15 step: 112, loss is 1.5409170389175415\n",
      "epoch: 15 step: 113, loss is 1.2745000123977661\n",
      "epoch: 15 step: 114, loss is 1.2503793239593506\n",
      "epoch: 15 step: 115, loss is 1.3033634424209595\n",
      "epoch: 15 step: 116, loss is 1.3787314891815186\n",
      "epoch: 15 step: 117, loss is 1.296531081199646\n",
      "epoch: 15 step: 118, loss is 1.3435721397399902\n",
      "epoch: 15 step: 119, loss is 1.3919613361358643\n",
      "epoch: 15 step: 120, loss is 1.294999599456787\n",
      "epoch: 15 step: 121, loss is 1.3605186939239502\n",
      "epoch: 15 step: 122, loss is 1.3116545677185059\n",
      "epoch: 15 step: 123, loss is 1.3328372240066528\n",
      "epoch: 15 step: 124, loss is 1.3159093856811523\n",
      "epoch: 15 step: 125, loss is 1.3686180114746094\n",
      "epoch: 15 step: 126, loss is 1.283010482788086\n",
      "epoch: 15 step: 127, loss is 1.352121114730835\n",
      "epoch: 15 step: 128, loss is 1.202146291732788\n",
      "epoch: 15 step: 129, loss is 1.48191237449646\n",
      "epoch: 15 step: 130, loss is 1.3128314018249512\n",
      "epoch: 15 step: 131, loss is 1.2375216484069824\n",
      "epoch: 15 step: 132, loss is 1.3818994760513306\n",
      "epoch: 15 step: 133, loss is 1.191407561302185\n",
      "epoch: 15 step: 134, loss is 1.290122389793396\n",
      "epoch: 15 step: 135, loss is 1.3826196193695068\n",
      "epoch: 15 step: 136, loss is 1.3992997407913208\n",
      "epoch: 15 step: 137, loss is 1.1254993677139282\n",
      "epoch: 15 step: 138, loss is 1.3548712730407715\n",
      "epoch: 15 step: 139, loss is 1.278554916381836\n",
      "epoch: 15 step: 140, loss is 1.404369592666626\n",
      "epoch: 15 step: 141, loss is 1.4244900941848755\n",
      "epoch: 15 step: 142, loss is 1.3301196098327637\n",
      "epoch: 15 step: 143, loss is 1.4112539291381836\n",
      "epoch: 15 step: 144, loss is 1.2326383590698242\n",
      "epoch: 15 step: 145, loss is 1.2704870700836182\n",
      "epoch: 15 step: 146, loss is 1.277282953262329\n",
      "epoch: 15 step: 147, loss is 1.2574187517166138\n",
      "epoch: 15 step: 148, loss is 1.4491150379180908\n",
      "epoch: 15 step: 149, loss is 1.2321516275405884\n",
      "epoch: 15 step: 150, loss is 1.3241679668426514\n",
      "epoch: 15 step: 151, loss is 1.4565279483795166\n",
      "epoch: 15 step: 152, loss is 1.2299010753631592\n",
      "epoch: 15 step: 153, loss is 1.284001111984253\n",
      "epoch: 15 step: 154, loss is 1.285918951034546\n",
      "epoch: 15 step: 155, loss is 1.338294267654419\n",
      "epoch: 15 step: 156, loss is 1.2382516860961914\n",
      "epoch: 15 step: 157, loss is 1.2937593460083008\n",
      "epoch: 15 step: 158, loss is 1.2521694898605347\n",
      "epoch: 15 step: 159, loss is 1.3414567708969116\n",
      "epoch: 15 step: 160, loss is 1.3208951950073242\n",
      "epoch: 15 step: 161, loss is 1.4646775722503662\n",
      "epoch: 15 step: 162, loss is 1.2773686647415161\n",
      "epoch: 15 step: 163, loss is 1.3237378597259521\n",
      "epoch: 15 step: 164, loss is 1.2915788888931274\n",
      "epoch: 15 step: 165, loss is 1.3114038705825806\n",
      "epoch: 15 step: 166, loss is 1.3409565687179565\n",
      "epoch: 15 step: 167, loss is 1.2561161518096924\n",
      "epoch: 15 step: 168, loss is 1.4148800373077393\n",
      "epoch: 15 step: 169, loss is 1.2938109636306763\n",
      "epoch: 15 step: 170, loss is 1.2810089588165283\n",
      "epoch: 15 step: 171, loss is 1.3837147951126099\n",
      "epoch: 15 step: 172, loss is 1.3709421157836914\n",
      "epoch: 15 step: 173, loss is 1.337572693824768\n",
      "epoch: 15 step: 174, loss is 1.3677513599395752\n",
      "epoch: 15 step: 175, loss is 1.3196563720703125\n",
      "epoch: 15 step: 176, loss is 1.2183258533477783\n",
      "epoch: 15 step: 177, loss is 1.2652027606964111\n",
      "epoch: 15 step: 178, loss is 1.3334957361221313\n",
      "epoch: 15 step: 179, loss is 1.3014631271362305\n",
      "epoch: 15 step: 180, loss is 1.3626350164413452\n",
      "epoch: 15 step: 181, loss is 1.2499510049819946\n",
      "epoch: 15 step: 182, loss is 1.318432331085205\n",
      "epoch: 15 step: 183, loss is 1.2483160495758057\n",
      "epoch: 15 step: 184, loss is 1.3440488576889038\n",
      "epoch: 15 step: 185, loss is 1.362931251525879\n",
      "epoch: 15 step: 186, loss is 1.3487578630447388\n",
      "epoch: 15 step: 187, loss is 1.307734727859497\n",
      "epoch: 15 step: 188, loss is 1.3287781476974487\n",
      "epoch: 15 step: 189, loss is 1.3632469177246094\n",
      "epoch: 15 step: 190, loss is 1.2768324613571167\n",
      "epoch: 15 step: 191, loss is 1.2835136651992798\n",
      "epoch: 15 step: 192, loss is 1.2289786338806152\n",
      "epoch: 15 step: 193, loss is 1.2838716506958008\n",
      "epoch: 15 step: 194, loss is 1.2578803300857544\n",
      "epoch: 15 step: 195, loss is 1.3504598140716553\n",
      "epoch: 15 step: 196, loss is 1.2728420495986938\n",
      "epoch: 15 step: 197, loss is 1.5161454677581787\n",
      "epoch: 15 step: 198, loss is 1.2320386171340942\n",
      "epoch: 15 step: 199, loss is 1.203869104385376\n",
      "epoch: 15 step: 200, loss is 1.2849888801574707\n",
      "epoch: 15 step: 201, loss is 1.2606788873672485\n",
      "epoch: 15 step: 202, loss is 1.3716368675231934\n",
      "epoch: 15 step: 203, loss is 1.253434658050537\n",
      "epoch: 15 step: 204, loss is 1.2761807441711426\n",
      "epoch: 15 step: 205, loss is 1.1908900737762451\n",
      "epoch: 15 step: 206, loss is 1.3212707042694092\n",
      "epoch: 15 step: 207, loss is 1.442575454711914\n",
      "epoch: 15 step: 208, loss is 1.1986980438232422\n",
      "epoch: 15 step: 209, loss is 1.3760942220687866\n",
      "epoch: 15 step: 210, loss is 1.2421073913574219\n",
      "epoch: 15 step: 211, loss is 1.2317689657211304\n",
      "epoch: 15 step: 212, loss is 1.5010422468185425\n",
      "epoch: 15 step: 213, loss is 1.2805395126342773\n",
      "epoch: 15 step: 214, loss is 1.308070421218872\n",
      "epoch: 15 step: 215, loss is 1.2236039638519287\n",
      "epoch: 15 step: 216, loss is 1.3643184900283813\n",
      "epoch: 15 step: 217, loss is 1.342069149017334\n",
      "epoch: 15 step: 218, loss is 1.252162218093872\n",
      "epoch: 15 step: 219, loss is 1.1942262649536133\n",
      "epoch: 15 step: 220, loss is 1.3107887506484985\n",
      "epoch: 15 step: 221, loss is 1.369545340538025\n",
      "epoch: 15 step: 222, loss is 1.2683334350585938\n",
      "epoch: 15 step: 223, loss is 1.261537790298462\n",
      "epoch: 15 step: 224, loss is 1.3677303791046143\n",
      "epoch: 15 step: 225, loss is 1.3757641315460205\n",
      "epoch: 15 step: 226, loss is 1.244713544845581\n",
      "epoch: 15 step: 227, loss is 1.279442310333252\n",
      "epoch: 15 step: 228, loss is 1.333723545074463\n",
      "epoch: 15 step: 229, loss is 1.372757077217102\n",
      "epoch: 15 step: 230, loss is 1.3074315786361694\n",
      "epoch: 15 step: 231, loss is 1.2719943523406982\n",
      "epoch: 15 step: 232, loss is 1.3261816501617432\n",
      "epoch: 15 step: 233, loss is 1.23894464969635\n",
      "epoch: 15 step: 234, loss is 1.3456144332885742\n",
      "epoch: 15 step: 235, loss is 1.473497986793518\n",
      "epoch: 15 step: 236, loss is 1.205718994140625\n",
      "epoch: 15 step: 237, loss is 1.383063554763794\n",
      "epoch: 15 step: 238, loss is 1.251078724861145\n",
      "epoch: 15 step: 239, loss is 1.36474609375\n",
      "epoch: 15 step: 240, loss is 1.3179060220718384\n",
      "epoch: 15 step: 241, loss is 1.3594728708267212\n",
      "epoch: 15 step: 242, loss is 1.358577847480774\n",
      "epoch: 15 step: 243, loss is 1.308841586112976\n",
      "epoch: 15 step: 244, loss is 1.3097368478775024\n",
      "epoch: 15 step: 245, loss is 1.3719942569732666\n",
      "epoch: 15 step: 246, loss is 1.339890718460083\n",
      "epoch: 15 step: 247, loss is 1.2953016757965088\n",
      "epoch: 15 step: 248, loss is 1.1426552534103394\n",
      "epoch: 15 step: 249, loss is 1.2543108463287354\n",
      "epoch: 15 step: 250, loss is 1.39628005027771\n",
      "epoch: 15 step: 251, loss is 1.374617576599121\n",
      "epoch: 15 step: 252, loss is 1.2732362747192383\n",
      "epoch: 15 step: 253, loss is 1.2892104387283325\n",
      "epoch: 15 step: 254, loss is 1.272763967514038\n",
      "epoch: 15 step: 255, loss is 1.217171311378479\n",
      "epoch: 15 step: 256, loss is 1.2839552164077759\n",
      "epoch: 15 step: 257, loss is 1.2238596677780151\n",
      "epoch: 15 step: 258, loss is 1.3890793323516846\n",
      "epoch: 15 step: 259, loss is 1.2756104469299316\n",
      "epoch: 15 step: 260, loss is 1.3807841539382935\n",
      "epoch: 15 step: 261, loss is 1.3693095445632935\n",
      "epoch: 15 step: 262, loss is 1.432616949081421\n",
      "epoch: 15 step: 263, loss is 1.2581636905670166\n",
      "epoch: 15 step: 264, loss is 1.4116199016571045\n",
      "epoch: 15 step: 265, loss is 1.2901047468185425\n",
      "epoch: 15 step: 266, loss is 1.3269814252853394\n",
      "epoch: 15 step: 267, loss is 1.245314121246338\n",
      "epoch: 15 step: 268, loss is 1.311976671218872\n",
      "epoch: 15 step: 269, loss is 1.244652509689331\n",
      "epoch: 15 step: 270, loss is 1.3285906314849854\n",
      "epoch: 15 step: 271, loss is 1.3153588771820068\n",
      "epoch: 15 step: 272, loss is 1.4054408073425293\n",
      "epoch: 15 step: 273, loss is 1.2374937534332275\n",
      "epoch: 15 step: 274, loss is 1.2134650945663452\n",
      "epoch: 15 step: 275, loss is 1.3316922187805176\n",
      "epoch: 15 step: 276, loss is 1.3540480136871338\n",
      "epoch: 15 step: 277, loss is 1.4029871225357056\n",
      "epoch: 15 step: 278, loss is 1.2428820133209229\n",
      "epoch: 15 step: 279, loss is 1.366263508796692\n",
      "epoch: 15 step: 280, loss is 1.3390436172485352\n",
      "epoch: 15 step: 281, loss is 1.4437005519866943\n",
      "epoch: 15 step: 282, loss is 1.2532448768615723\n",
      "epoch: 15 step: 283, loss is 1.3608222007751465\n",
      "epoch: 15 step: 284, loss is 1.2433438301086426\n",
      "epoch: 15 step: 285, loss is 1.330042839050293\n",
      "epoch: 15 step: 286, loss is 1.4312574863433838\n",
      "epoch: 15 step: 287, loss is 1.3029954433441162\n",
      "epoch: 15 step: 288, loss is 1.2568824291229248\n",
      "epoch: 15 step: 289, loss is 1.3821197748184204\n",
      "epoch: 15 step: 290, loss is 1.3349432945251465\n",
      "epoch: 15 step: 291, loss is 1.3405165672302246\n",
      "epoch: 15 step: 292, loss is 1.3408503532409668\n",
      "epoch: 15 step: 293, loss is 1.2518752813339233\n",
      "epoch: 15 step: 294, loss is 1.254661202430725\n",
      "epoch: 15 step: 295, loss is 1.2757587432861328\n",
      "epoch: 15 step: 296, loss is 1.327907919883728\n",
      "epoch: 15 step: 297, loss is 1.3583383560180664\n",
      "epoch: 15 step: 298, loss is 1.368752121925354\n",
      "epoch: 15 step: 299, loss is 1.2563269138336182\n",
      "epoch: 15 step: 300, loss is 1.2702710628509521\n",
      "epoch: 15 step: 301, loss is 1.2467186450958252\n",
      "epoch: 15 step: 302, loss is 1.289454698562622\n",
      "epoch: 15 step: 303, loss is 1.367107629776001\n",
      "epoch: 15 step: 304, loss is 1.3607044219970703\n",
      "epoch: 15 step: 305, loss is 1.2276740074157715\n",
      "epoch: 15 step: 306, loss is 1.3959487676620483\n",
      "epoch: 15 step: 307, loss is 1.2625782489776611\n",
      "epoch: 15 step: 308, loss is 1.314075231552124\n",
      "epoch: 15 step: 309, loss is 1.3482723236083984\n",
      "epoch: 15 step: 310, loss is 1.268074870109558\n",
      "epoch: 15 step: 311, loss is 1.3256688117980957\n",
      "epoch: 15 step: 312, loss is 1.3908045291900635\n",
      "epoch: 15 step: 313, loss is 1.2719885110855103\n",
      "epoch: 15 step: 314, loss is 1.301532506942749\n",
      "epoch: 15 step: 315, loss is 1.228319525718689\n",
      "epoch: 15 step: 316, loss is 1.298280954360962\n",
      "epoch: 15 step: 317, loss is 1.3201305866241455\n",
      "epoch: 15 step: 318, loss is 1.3550207614898682\n",
      "epoch: 15 step: 319, loss is 1.2985265254974365\n",
      "epoch: 15 step: 320, loss is 1.3370742797851562\n",
      "epoch: 15 step: 321, loss is 1.3423577547073364\n",
      "epoch: 15 step: 322, loss is 1.193439245223999\n",
      "epoch: 15 step: 323, loss is 1.3355506658554077\n",
      "epoch: 15 step: 324, loss is 1.2818325757980347\n",
      "epoch: 15 step: 325, loss is 1.1936399936676025\n",
      "epoch: 15 step: 326, loss is 1.2662094831466675\n",
      "epoch: 15 step: 327, loss is 1.4311844110488892\n",
      "epoch: 15 step: 328, loss is 1.251225233078003\n",
      "epoch: 15 step: 329, loss is 1.296910285949707\n",
      "epoch: 15 step: 330, loss is 1.2827160358428955\n",
      "epoch: 15 step: 331, loss is 1.3208153247833252\n",
      "epoch: 15 step: 332, loss is 1.3517303466796875\n",
      "epoch: 15 step: 333, loss is 1.3158625364303589\n",
      "epoch: 15 step: 334, loss is 1.1546180248260498\n",
      "epoch: 15 step: 335, loss is 1.1951987743377686\n",
      "epoch: 15 step: 336, loss is 1.5089685916900635\n",
      "epoch: 15 step: 337, loss is 1.3777971267700195\n",
      "epoch: 15 step: 338, loss is 1.260854721069336\n",
      "epoch: 15 step: 339, loss is 1.2401033639907837\n",
      "epoch: 15 step: 340, loss is 1.3590039014816284\n",
      "epoch: 15 step: 341, loss is 1.3732448816299438\n",
      "epoch: 15 step: 342, loss is 1.3882513046264648\n",
      "epoch: 15 step: 343, loss is 1.2658205032348633\n",
      "epoch: 15 step: 344, loss is 1.4786310195922852\n",
      "epoch: 15 step: 345, loss is 1.2204135656356812\n",
      "epoch: 15 step: 346, loss is 1.4549394845962524\n",
      "epoch: 15 step: 347, loss is 1.3352718353271484\n",
      "epoch: 15 step: 348, loss is 1.2859606742858887\n",
      "epoch: 15 step: 349, loss is 1.4709104299545288\n",
      "epoch: 15 step: 350, loss is 1.375964641571045\n",
      "epoch: 15 step: 351, loss is 1.23580801486969\n",
      "epoch: 15 step: 352, loss is 1.4576046466827393\n",
      "epoch: 15 step: 353, loss is 1.3305339813232422\n",
      "epoch: 15 step: 354, loss is 1.3578760623931885\n",
      "epoch: 15 step: 355, loss is 1.406681776046753\n",
      "epoch: 15 step: 356, loss is 1.3001413345336914\n",
      "epoch: 15 step: 357, loss is 1.342828392982483\n",
      "epoch: 15 step: 358, loss is 1.2510833740234375\n",
      "epoch: 15 step: 359, loss is 1.4180306196212769\n",
      "epoch: 15 step: 360, loss is 1.4317739009857178\n",
      "epoch: 15 step: 361, loss is 1.3530696630477905\n",
      "epoch: 15 step: 362, loss is 1.4313223361968994\n",
      "epoch: 15 step: 363, loss is 1.3356623649597168\n",
      "epoch: 15 step: 364, loss is 1.352958083152771\n",
      "epoch: 15 step: 365, loss is 1.4138076305389404\n",
      "epoch: 15 step: 366, loss is 1.3849503993988037\n",
      "epoch: 15 step: 367, loss is 1.2818410396575928\n",
      "epoch: 15 step: 368, loss is 1.3137842416763306\n",
      "epoch: 15 step: 369, loss is 1.2578920125961304\n",
      "epoch: 15 step: 370, loss is 1.3569501638412476\n",
      "epoch: 15 step: 371, loss is 1.239445686340332\n",
      "epoch: 15 step: 372, loss is 1.1856160163879395\n",
      "epoch: 15 step: 373, loss is 1.3646267652511597\n",
      "epoch: 15 step: 374, loss is 1.3206820487976074\n",
      "epoch: 15 step: 375, loss is 1.285178542137146\n",
      "epoch: 15 step: 376, loss is 1.2771285772323608\n",
      "epoch: 15 step: 377, loss is 1.2863829135894775\n",
      "epoch: 15 step: 378, loss is 1.1666384935379028\n",
      "epoch: 15 step: 379, loss is 1.3612914085388184\n",
      "epoch: 15 step: 380, loss is 1.2683026790618896\n",
      "epoch: 15 step: 381, loss is 1.2906169891357422\n",
      "epoch: 15 step: 382, loss is 1.2965991497039795\n",
      "epoch: 15 step: 383, loss is 1.3910677433013916\n",
      "epoch: 15 step: 384, loss is 1.1946861743927002\n",
      "epoch: 15 step: 385, loss is 1.2739406824111938\n",
      "epoch: 15 step: 386, loss is 1.3365685939788818\n",
      "epoch: 15 step: 387, loss is 1.2378675937652588\n",
      "epoch: 15 step: 388, loss is 1.2978264093399048\n",
      "epoch: 15 step: 389, loss is 1.3838828802108765\n",
      "epoch: 15 step: 390, loss is 1.261654019355774\n",
      "Train epoch time: 158545.411 ms, per step time: 406.527 ms\n",
      "epoch: 16 step: 1, loss is 1.2623209953308105\n",
      "epoch: 16 step: 2, loss is 1.2128379344940186\n",
      "epoch: 16 step: 3, loss is 1.3740341663360596\n",
      "epoch: 16 step: 4, loss is 1.2504808902740479\n",
      "epoch: 16 step: 5, loss is 1.363284945487976\n",
      "epoch: 16 step: 6, loss is 1.3022751808166504\n",
      "epoch: 16 step: 7, loss is 1.3605247735977173\n",
      "epoch: 16 step: 8, loss is 1.2482513189315796\n",
      "epoch: 16 step: 9, loss is 1.2838740348815918\n",
      "epoch: 16 step: 10, loss is 1.289101243019104\n",
      "epoch: 16 step: 11, loss is 1.3126130104064941\n",
      "epoch: 16 step: 12, loss is 1.309398889541626\n",
      "epoch: 16 step: 13, loss is 1.4143693447113037\n",
      "epoch: 16 step: 14, loss is 1.3651556968688965\n",
      "epoch: 16 step: 15, loss is 1.2528750896453857\n",
      "epoch: 16 step: 16, loss is 1.2764209508895874\n",
      "epoch: 16 step: 17, loss is 1.179723858833313\n",
      "epoch: 16 step: 18, loss is 1.313732624053955\n",
      "epoch: 16 step: 19, loss is 1.3036516904830933\n",
      "epoch: 16 step: 20, loss is 1.3576622009277344\n",
      "epoch: 16 step: 21, loss is 1.3650267124176025\n",
      "epoch: 16 step: 22, loss is 1.2030786275863647\n",
      "epoch: 16 step: 23, loss is 1.2618050575256348\n",
      "epoch: 16 step: 24, loss is 1.2790038585662842\n",
      "epoch: 16 step: 25, loss is 1.3331063985824585\n",
      "epoch: 16 step: 26, loss is 1.3976110219955444\n",
      "epoch: 16 step: 27, loss is 1.2577378749847412\n",
      "epoch: 16 step: 28, loss is 1.3200048208236694\n",
      "epoch: 16 step: 29, loss is 1.324204444885254\n",
      "epoch: 16 step: 30, loss is 1.3532803058624268\n",
      "epoch: 16 step: 31, loss is 1.3836638927459717\n",
      "epoch: 16 step: 32, loss is 1.2559924125671387\n",
      "epoch: 16 step: 33, loss is 1.2702875137329102\n",
      "epoch: 16 step: 34, loss is 1.3542271852493286\n",
      "epoch: 16 step: 35, loss is 1.2489936351776123\n",
      "epoch: 16 step: 36, loss is 1.2949013710021973\n",
      "epoch: 16 step: 37, loss is 1.3043348789215088\n",
      "epoch: 16 step: 38, loss is 1.3399665355682373\n",
      "epoch: 16 step: 39, loss is 1.3572111129760742\n",
      "epoch: 16 step: 40, loss is 1.350608468055725\n",
      "epoch: 16 step: 41, loss is 1.1832423210144043\n",
      "epoch: 16 step: 42, loss is 1.3062134981155396\n",
      "epoch: 16 step: 43, loss is 1.3339805603027344\n",
      "epoch: 16 step: 44, loss is 1.249432921409607\n",
      "epoch: 16 step: 45, loss is 1.2678354978561401\n",
      "epoch: 16 step: 46, loss is 1.302646279335022\n",
      "epoch: 16 step: 47, loss is 1.3434255123138428\n",
      "epoch: 16 step: 48, loss is 1.3168554306030273\n",
      "epoch: 16 step: 49, loss is 1.249881386756897\n",
      "epoch: 16 step: 50, loss is 1.2619564533233643\n",
      "epoch: 16 step: 51, loss is 1.3637768030166626\n",
      "epoch: 16 step: 52, loss is 1.3009368181228638\n",
      "epoch: 16 step: 53, loss is 1.4215922355651855\n",
      "epoch: 16 step: 54, loss is 1.3307528495788574\n",
      "epoch: 16 step: 55, loss is 1.2112469673156738\n",
      "epoch: 16 step: 56, loss is 1.3403782844543457\n",
      "epoch: 16 step: 57, loss is 1.3146854639053345\n",
      "epoch: 16 step: 58, loss is 1.4248939752578735\n",
      "epoch: 16 step: 59, loss is 1.3379018306732178\n",
      "epoch: 16 step: 60, loss is 1.252834677696228\n",
      "epoch: 16 step: 61, loss is 1.307476282119751\n",
      "epoch: 16 step: 62, loss is 1.208288311958313\n",
      "epoch: 16 step: 63, loss is 1.32616126537323\n",
      "epoch: 16 step: 64, loss is 1.3175311088562012\n",
      "epoch: 16 step: 65, loss is 1.281423568725586\n",
      "epoch: 16 step: 66, loss is 1.4169663190841675\n",
      "epoch: 16 step: 67, loss is 1.3096116781234741\n",
      "epoch: 16 step: 68, loss is 1.3262596130371094\n",
      "epoch: 16 step: 69, loss is 1.4449481964111328\n",
      "epoch: 16 step: 70, loss is 1.2343723773956299\n",
      "epoch: 16 step: 71, loss is 1.2361571788787842\n",
      "epoch: 16 step: 72, loss is 1.430267572402954\n",
      "epoch: 16 step: 73, loss is 1.251829981803894\n",
      "epoch: 16 step: 74, loss is 1.3171852827072144\n",
      "epoch: 16 step: 75, loss is 1.202214002609253\n",
      "epoch: 16 step: 76, loss is 1.2766268253326416\n",
      "epoch: 16 step: 77, loss is 1.152397871017456\n",
      "epoch: 16 step: 78, loss is 1.286108136177063\n",
      "epoch: 16 step: 79, loss is 1.3260623216629028\n",
      "epoch: 16 step: 80, loss is 1.3881481885910034\n",
      "epoch: 16 step: 81, loss is 1.391103744506836\n",
      "epoch: 16 step: 82, loss is 1.4476453065872192\n",
      "epoch: 16 step: 83, loss is 1.268845796585083\n",
      "epoch: 16 step: 84, loss is 1.2779663801193237\n",
      "epoch: 16 step: 85, loss is 1.160872220993042\n",
      "epoch: 16 step: 86, loss is 1.3761931657791138\n",
      "epoch: 16 step: 87, loss is 1.1299588680267334\n",
      "epoch: 16 step: 88, loss is 1.2474876642227173\n",
      "epoch: 16 step: 89, loss is 1.3157200813293457\n",
      "epoch: 16 step: 90, loss is 1.2072532176971436\n",
      "epoch: 16 step: 91, loss is 1.32395601272583\n",
      "epoch: 16 step: 92, loss is 1.2480227947235107\n",
      "epoch: 16 step: 93, loss is 1.3628795146942139\n",
      "epoch: 16 step: 94, loss is 1.3369594812393188\n",
      "epoch: 16 step: 95, loss is 1.333911418914795\n",
      "epoch: 16 step: 96, loss is 1.394477367401123\n",
      "epoch: 16 step: 97, loss is 1.2571754455566406\n",
      "epoch: 16 step: 98, loss is 1.2459226846694946\n",
      "epoch: 16 step: 99, loss is 1.2120877504348755\n",
      "epoch: 16 step: 100, loss is 1.3321102857589722\n",
      "epoch: 16 step: 101, loss is 1.2738165855407715\n",
      "epoch: 16 step: 102, loss is 1.2617892026901245\n",
      "epoch: 16 step: 103, loss is 1.3563315868377686\n",
      "epoch: 16 step: 104, loss is 1.4204708337783813\n",
      "epoch: 16 step: 105, loss is 1.3269729614257812\n",
      "epoch: 16 step: 106, loss is 1.2966792583465576\n",
      "epoch: 16 step: 107, loss is 1.2424068450927734\n",
      "epoch: 16 step: 108, loss is 1.277260184288025\n",
      "epoch: 16 step: 109, loss is 1.3666449785232544\n",
      "epoch: 16 step: 110, loss is 1.2843799591064453\n",
      "epoch: 16 step: 111, loss is 1.4087201356887817\n",
      "epoch: 16 step: 112, loss is 1.381744384765625\n",
      "epoch: 16 step: 113, loss is 1.4223318099975586\n",
      "epoch: 16 step: 114, loss is 1.3396860361099243\n",
      "epoch: 16 step: 115, loss is 1.2953600883483887\n",
      "epoch: 16 step: 116, loss is 1.2882270812988281\n",
      "epoch: 16 step: 117, loss is 1.4531164169311523\n",
      "epoch: 16 step: 118, loss is 1.3362654447555542\n",
      "epoch: 16 step: 119, loss is 1.2626936435699463\n",
      "epoch: 16 step: 120, loss is 1.2504239082336426\n",
      "epoch: 16 step: 121, loss is 1.270768165588379\n",
      "epoch: 16 step: 122, loss is 1.3233091831207275\n",
      "epoch: 16 step: 123, loss is 1.3044586181640625\n",
      "epoch: 16 step: 124, loss is 1.3890318870544434\n",
      "epoch: 16 step: 125, loss is 1.3318744897842407\n",
      "epoch: 16 step: 126, loss is 1.3257980346679688\n",
      "epoch: 16 step: 127, loss is 1.1899397373199463\n",
      "epoch: 16 step: 128, loss is 1.2562305927276611\n",
      "epoch: 16 step: 129, loss is 1.2513084411621094\n",
      "epoch: 16 step: 130, loss is 1.3420690298080444\n",
      "epoch: 16 step: 131, loss is 1.2491308450698853\n",
      "epoch: 16 step: 132, loss is 1.367977499961853\n",
      "epoch: 16 step: 133, loss is 1.2394062280654907\n",
      "epoch: 16 step: 134, loss is 1.2880735397338867\n",
      "epoch: 16 step: 135, loss is 1.270825982093811\n",
      "epoch: 16 step: 136, loss is 1.209162712097168\n",
      "epoch: 16 step: 137, loss is 1.1997442245483398\n",
      "epoch: 16 step: 138, loss is 1.2429308891296387\n",
      "epoch: 16 step: 139, loss is 1.3355015516281128\n",
      "epoch: 16 step: 140, loss is 1.2760021686553955\n",
      "epoch: 16 step: 141, loss is 1.3763234615325928\n",
      "epoch: 16 step: 142, loss is 1.3732051849365234\n",
      "epoch: 16 step: 143, loss is 1.3328170776367188\n",
      "epoch: 16 step: 144, loss is 1.311214804649353\n",
      "epoch: 16 step: 145, loss is 1.3452388048171997\n",
      "epoch: 16 step: 146, loss is 1.259648323059082\n",
      "epoch: 16 step: 147, loss is 1.4446420669555664\n",
      "epoch: 16 step: 148, loss is 1.3430640697479248\n",
      "epoch: 16 step: 149, loss is 1.2015942335128784\n",
      "epoch: 16 step: 150, loss is 1.274448275566101\n",
      "epoch: 16 step: 151, loss is 1.258067011833191\n",
      "epoch: 16 step: 152, loss is 1.24289071559906\n",
      "epoch: 16 step: 153, loss is 1.238769769668579\n",
      "epoch: 16 step: 154, loss is 1.2842121124267578\n",
      "epoch: 16 step: 155, loss is 1.3417505025863647\n",
      "epoch: 16 step: 156, loss is 1.2867388725280762\n",
      "epoch: 16 step: 157, loss is 1.2815008163452148\n",
      "epoch: 16 step: 158, loss is 1.2458510398864746\n",
      "epoch: 16 step: 159, loss is 1.392712116241455\n",
      "epoch: 16 step: 160, loss is 1.3194780349731445\n",
      "epoch: 16 step: 161, loss is 1.1997146606445312\n",
      "epoch: 16 step: 162, loss is 1.287451148033142\n",
      "epoch: 16 step: 163, loss is 1.3590575456619263\n",
      "epoch: 16 step: 164, loss is 1.2377333641052246\n",
      "epoch: 16 step: 165, loss is 1.3341461420059204\n",
      "epoch: 16 step: 166, loss is 1.3288426399230957\n",
      "epoch: 16 step: 167, loss is 1.20006263256073\n",
      "epoch: 16 step: 168, loss is 1.2572826147079468\n",
      "epoch: 16 step: 169, loss is 1.4479568004608154\n",
      "epoch: 16 step: 170, loss is 1.4475131034851074\n",
      "epoch: 16 step: 171, loss is 1.351914882659912\n",
      "epoch: 16 step: 172, loss is 1.3197225332260132\n",
      "epoch: 16 step: 173, loss is 1.24617338180542\n",
      "epoch: 16 step: 174, loss is 1.2384605407714844\n",
      "epoch: 16 step: 175, loss is 1.2433409690856934\n",
      "epoch: 16 step: 176, loss is 1.2906239032745361\n",
      "epoch: 16 step: 177, loss is 1.3210591077804565\n",
      "epoch: 16 step: 178, loss is 1.358208417892456\n",
      "epoch: 16 step: 179, loss is 1.26594877243042\n",
      "epoch: 16 step: 180, loss is 1.2109971046447754\n",
      "epoch: 16 step: 181, loss is 1.30675208568573\n",
      "epoch: 16 step: 182, loss is 1.2850216627120972\n",
      "epoch: 16 step: 183, loss is 1.3494545221328735\n",
      "epoch: 16 step: 184, loss is 1.3769340515136719\n",
      "epoch: 16 step: 185, loss is 1.1327763795852661\n",
      "epoch: 16 step: 186, loss is 1.3522560596466064\n",
      "epoch: 16 step: 187, loss is 1.4491814374923706\n",
      "epoch: 16 step: 188, loss is 1.3001248836517334\n",
      "epoch: 16 step: 189, loss is 1.1775615215301514\n",
      "epoch: 16 step: 190, loss is 1.1986194849014282\n",
      "epoch: 16 step: 191, loss is 1.2954984903335571\n",
      "epoch: 16 step: 192, loss is 1.2420620918273926\n",
      "epoch: 16 step: 193, loss is 1.2637323141098022\n",
      "epoch: 16 step: 194, loss is 1.297238826751709\n",
      "epoch: 16 step: 195, loss is 1.3168586492538452\n",
      "epoch: 16 step: 196, loss is 1.2432645559310913\n",
      "epoch: 16 step: 197, loss is 1.345112919807434\n",
      "epoch: 16 step: 198, loss is 1.258817195892334\n",
      "epoch: 16 step: 199, loss is 1.3853167295455933\n",
      "epoch: 16 step: 200, loss is 1.3948339223861694\n",
      "epoch: 16 step: 201, loss is 1.2915974855422974\n",
      "epoch: 16 step: 202, loss is 1.2769397497177124\n",
      "epoch: 16 step: 203, loss is 1.2975454330444336\n",
      "epoch: 16 step: 204, loss is 1.2567775249481201\n",
      "epoch: 16 step: 205, loss is 1.2186909914016724\n",
      "epoch: 16 step: 206, loss is 1.2893743515014648\n",
      "epoch: 16 step: 207, loss is 1.2540329694747925\n",
      "epoch: 16 step: 208, loss is 1.197383165359497\n",
      "epoch: 16 step: 209, loss is 1.37493896484375\n",
      "epoch: 16 step: 210, loss is 1.2104768753051758\n",
      "epoch: 16 step: 211, loss is 1.2248002290725708\n",
      "epoch: 16 step: 212, loss is 1.3845043182373047\n",
      "epoch: 16 step: 213, loss is 1.3762717247009277\n",
      "epoch: 16 step: 214, loss is 1.2937856912612915\n",
      "epoch: 16 step: 215, loss is 1.400834560394287\n",
      "epoch: 16 step: 216, loss is 1.2980248928070068\n",
      "epoch: 16 step: 217, loss is 1.3405869007110596\n",
      "epoch: 16 step: 218, loss is 1.2550970315933228\n",
      "epoch: 16 step: 219, loss is 1.333485722541809\n",
      "epoch: 16 step: 220, loss is 1.2491743564605713\n",
      "epoch: 16 step: 221, loss is 1.2746102809906006\n",
      "epoch: 16 step: 222, loss is 1.306026816368103\n",
      "epoch: 16 step: 223, loss is 1.3830333948135376\n",
      "epoch: 16 step: 224, loss is 1.2751281261444092\n",
      "epoch: 16 step: 225, loss is 1.2623238563537598\n",
      "epoch: 16 step: 226, loss is 1.2873914241790771\n",
      "epoch: 16 step: 227, loss is 1.2640399932861328\n",
      "epoch: 16 step: 228, loss is 1.4520432949066162\n",
      "epoch: 16 step: 229, loss is 1.4942173957824707\n",
      "epoch: 16 step: 230, loss is 1.474406361579895\n",
      "epoch: 16 step: 231, loss is 1.4631710052490234\n",
      "epoch: 16 step: 232, loss is 1.3113845586776733\n",
      "epoch: 16 step: 233, loss is 1.2188023328781128\n",
      "epoch: 16 step: 234, loss is 1.235015630722046\n",
      "epoch: 16 step: 235, loss is 1.451774001121521\n",
      "epoch: 16 step: 236, loss is 1.3761779069900513\n",
      "epoch: 16 step: 237, loss is 1.108324408531189\n",
      "epoch: 16 step: 238, loss is 1.3369306325912476\n",
      "epoch: 16 step: 239, loss is 1.2333886623382568\n",
      "epoch: 16 step: 240, loss is 1.2840075492858887\n",
      "epoch: 16 step: 241, loss is 1.3558391332626343\n",
      "epoch: 16 step: 242, loss is 1.209949016571045\n",
      "epoch: 16 step: 243, loss is 1.3214459419250488\n",
      "epoch: 16 step: 244, loss is 1.2720731496810913\n",
      "epoch: 16 step: 245, loss is 1.3304722309112549\n",
      "epoch: 16 step: 246, loss is 1.3958858251571655\n",
      "epoch: 16 step: 247, loss is 1.3769705295562744\n",
      "epoch: 16 step: 248, loss is 1.2757213115692139\n",
      "epoch: 16 step: 249, loss is 1.3185203075408936\n",
      "epoch: 16 step: 250, loss is 1.2466561794281006\n",
      "epoch: 16 step: 251, loss is 1.2073277235031128\n",
      "epoch: 16 step: 252, loss is 1.3411571979522705\n",
      "epoch: 16 step: 253, loss is 1.2208608388900757\n",
      "epoch: 16 step: 254, loss is 1.2521766424179077\n",
      "epoch: 16 step: 255, loss is 1.4377790689468384\n",
      "epoch: 16 step: 256, loss is 1.309388518333435\n",
      "epoch: 16 step: 257, loss is 1.294487714767456\n",
      "epoch: 16 step: 258, loss is 1.3373725414276123\n",
      "epoch: 16 step: 259, loss is 1.3524537086486816\n",
      "epoch: 16 step: 260, loss is 1.298173427581787\n",
      "epoch: 16 step: 261, loss is 1.267491340637207\n",
      "epoch: 16 step: 262, loss is 1.4044595956802368\n",
      "epoch: 16 step: 263, loss is 1.2485281229019165\n",
      "epoch: 16 step: 264, loss is 1.274491548538208\n",
      "epoch: 16 step: 265, loss is 1.4138816595077515\n",
      "epoch: 16 step: 266, loss is 1.4095571041107178\n",
      "epoch: 16 step: 267, loss is 1.2677870988845825\n",
      "epoch: 16 step: 268, loss is 1.2465612888336182\n",
      "epoch: 16 step: 269, loss is 1.2273073196411133\n",
      "epoch: 16 step: 270, loss is 1.3580493927001953\n",
      "epoch: 16 step: 271, loss is 1.3295199871063232\n",
      "epoch: 16 step: 272, loss is 1.2743233442306519\n",
      "epoch: 16 step: 273, loss is 1.2715415954589844\n",
      "epoch: 16 step: 274, loss is 1.309531807899475\n",
      "epoch: 16 step: 275, loss is 1.2276047468185425\n",
      "epoch: 16 step: 276, loss is 1.2970184087753296\n",
      "epoch: 16 step: 277, loss is 1.2984158992767334\n",
      "epoch: 16 step: 278, loss is 1.3074352741241455\n",
      "epoch: 16 step: 279, loss is 1.3707536458969116\n",
      "epoch: 16 step: 280, loss is 1.1942834854125977\n",
      "epoch: 16 step: 281, loss is 1.3080644607543945\n",
      "epoch: 16 step: 282, loss is 1.3133494853973389\n",
      "epoch: 16 step: 283, loss is 1.2650039196014404\n",
      "epoch: 16 step: 284, loss is 1.2247414588928223\n",
      "epoch: 16 step: 285, loss is 1.2624602317810059\n",
      "epoch: 16 step: 286, loss is 1.1953582763671875\n",
      "epoch: 16 step: 287, loss is 1.3935656547546387\n",
      "epoch: 16 step: 288, loss is 1.2503219842910767\n",
      "epoch: 16 step: 289, loss is 1.3820035457611084\n",
      "epoch: 16 step: 290, loss is 1.1795002222061157\n",
      "epoch: 16 step: 291, loss is 1.3149964809417725\n",
      "epoch: 16 step: 292, loss is 1.3171873092651367\n",
      "epoch: 16 step: 293, loss is 1.241196632385254\n",
      "epoch: 16 step: 294, loss is 1.280397891998291\n",
      "epoch: 16 step: 295, loss is 1.2525522708892822\n",
      "epoch: 16 step: 296, loss is 1.199676752090454\n",
      "epoch: 16 step: 297, loss is 1.340691328048706\n",
      "epoch: 16 step: 298, loss is 1.237370491027832\n",
      "epoch: 16 step: 299, loss is 1.2031385898590088\n",
      "epoch: 16 step: 300, loss is 1.2940490245819092\n",
      "epoch: 16 step: 301, loss is 1.4875611066818237\n",
      "epoch: 16 step: 302, loss is 1.212522029876709\n",
      "epoch: 16 step: 303, loss is 1.36819589138031\n",
      "epoch: 16 step: 304, loss is 1.2224831581115723\n",
      "epoch: 16 step: 305, loss is 1.3741711378097534\n",
      "epoch: 16 step: 306, loss is 1.2745771408081055\n",
      "epoch: 16 step: 307, loss is 1.2728948593139648\n",
      "epoch: 16 step: 308, loss is 1.2988500595092773\n",
      "epoch: 16 step: 309, loss is 1.2696250677108765\n",
      "epoch: 16 step: 310, loss is 1.3740962743759155\n",
      "epoch: 16 step: 311, loss is 1.3506255149841309\n",
      "epoch: 16 step: 312, loss is 1.345147967338562\n",
      "epoch: 16 step: 313, loss is 1.3300501108169556\n",
      "epoch: 16 step: 314, loss is 1.283568024635315\n",
      "epoch: 16 step: 315, loss is 1.2476530075073242\n",
      "epoch: 16 step: 316, loss is 1.2096741199493408\n",
      "epoch: 16 step: 317, loss is 1.2855660915374756\n",
      "epoch: 16 step: 318, loss is 1.2229504585266113\n",
      "epoch: 16 step: 319, loss is 1.2855703830718994\n",
      "epoch: 16 step: 320, loss is 1.234875202178955\n",
      "epoch: 16 step: 321, loss is 1.2143000364303589\n",
      "epoch: 16 step: 322, loss is 1.2187873125076294\n",
      "epoch: 16 step: 323, loss is 1.241376280784607\n",
      "epoch: 16 step: 324, loss is 1.3604748249053955\n",
      "epoch: 16 step: 325, loss is 1.2129342555999756\n",
      "epoch: 16 step: 326, loss is 1.1392419338226318\n",
      "epoch: 16 step: 327, loss is 1.3633698225021362\n",
      "epoch: 16 step: 328, loss is 1.3900090456008911\n",
      "epoch: 16 step: 329, loss is 1.2194725275039673\n",
      "epoch: 16 step: 330, loss is 1.2662506103515625\n",
      "epoch: 16 step: 331, loss is 1.3632632493972778\n",
      "epoch: 16 step: 332, loss is 1.2675384283065796\n",
      "epoch: 16 step: 333, loss is 1.2484467029571533\n",
      "epoch: 16 step: 334, loss is 1.4452571868896484\n",
      "epoch: 16 step: 335, loss is 1.2505826950073242\n",
      "epoch: 16 step: 336, loss is 1.3683218955993652\n",
      "epoch: 16 step: 337, loss is 1.3192684650421143\n",
      "epoch: 16 step: 338, loss is 1.3175615072250366\n",
      "epoch: 16 step: 339, loss is 1.2100948095321655\n",
      "epoch: 16 step: 340, loss is 1.2422270774841309\n",
      "epoch: 16 step: 341, loss is 1.3470028638839722\n",
      "epoch: 16 step: 342, loss is 1.3496800661087036\n",
      "epoch: 16 step: 343, loss is 1.268109679222107\n",
      "epoch: 16 step: 344, loss is 1.3397935628890991\n",
      "epoch: 16 step: 345, loss is 1.2921119928359985\n",
      "epoch: 16 step: 346, loss is 1.395092248916626\n",
      "epoch: 16 step: 347, loss is 1.241277813911438\n",
      "epoch: 16 step: 348, loss is 1.2675411701202393\n",
      "epoch: 16 step: 349, loss is 1.2371431589126587\n",
      "epoch: 16 step: 350, loss is 1.3046460151672363\n",
      "epoch: 16 step: 351, loss is 1.3359887599945068\n",
      "epoch: 16 step: 352, loss is 1.2020769119262695\n",
      "epoch: 16 step: 353, loss is 1.307011604309082\n",
      "epoch: 16 step: 354, loss is 1.252579927444458\n",
      "epoch: 16 step: 355, loss is 1.1952117681503296\n",
      "epoch: 16 step: 356, loss is 1.2967877388000488\n",
      "epoch: 16 step: 357, loss is 1.2266472578048706\n",
      "epoch: 16 step: 358, loss is 1.2619049549102783\n",
      "epoch: 16 step: 359, loss is 1.2495460510253906\n",
      "epoch: 16 step: 360, loss is 1.3004270792007446\n",
      "epoch: 16 step: 361, loss is 1.1988545656204224\n",
      "epoch: 16 step: 362, loss is 1.252661943435669\n",
      "epoch: 16 step: 363, loss is 1.2074164152145386\n",
      "epoch: 16 step: 364, loss is 1.3129504919052124\n",
      "epoch: 16 step: 365, loss is 1.3173621892929077\n",
      "epoch: 16 step: 366, loss is 1.280242919921875\n",
      "epoch: 16 step: 367, loss is 1.3213449716567993\n",
      "epoch: 16 step: 368, loss is 1.2589666843414307\n",
      "epoch: 16 step: 369, loss is 1.29549241065979\n",
      "epoch: 16 step: 370, loss is 1.2043647766113281\n",
      "epoch: 16 step: 371, loss is 1.205735445022583\n",
      "epoch: 16 step: 372, loss is 1.3724133968353271\n",
      "epoch: 16 step: 373, loss is 1.4105360507965088\n",
      "epoch: 16 step: 374, loss is 1.3415188789367676\n",
      "epoch: 16 step: 375, loss is 1.2589389085769653\n",
      "epoch: 16 step: 376, loss is 1.3795737028121948\n",
      "epoch: 16 step: 377, loss is 1.3170115947723389\n",
      "epoch: 16 step: 378, loss is 1.3003252744674683\n",
      "epoch: 16 step: 379, loss is 1.1583467721939087\n",
      "epoch: 16 step: 380, loss is 1.2881090641021729\n",
      "epoch: 16 step: 381, loss is 1.252467155456543\n",
      "epoch: 16 step: 382, loss is 1.3282389640808105\n",
      "epoch: 16 step: 383, loss is 1.2265913486480713\n",
      "epoch: 16 step: 384, loss is 1.2734038829803467\n",
      "epoch: 16 step: 385, loss is 1.1914052963256836\n",
      "epoch: 16 step: 386, loss is 1.2450098991394043\n",
      "epoch: 16 step: 387, loss is 1.265929102897644\n",
      "epoch: 16 step: 388, loss is 1.260615587234497\n",
      "epoch: 16 step: 389, loss is 1.3370906114578247\n",
      "epoch: 16 step: 390, loss is 1.3260668516159058\n",
      "Train epoch time: 169103.319 ms, per step time: 433.598 ms\n",
      "epoch: 17 step: 1, loss is 1.3347898721694946\n",
      "epoch: 17 step: 2, loss is 1.2053965330123901\n",
      "epoch: 17 step: 3, loss is 1.2852733135223389\n",
      "epoch: 17 step: 4, loss is 1.3153245449066162\n",
      "epoch: 17 step: 5, loss is 1.2607505321502686\n",
      "epoch: 17 step: 6, loss is 1.172715425491333\n",
      "epoch: 17 step: 7, loss is 1.216605544090271\n",
      "epoch: 17 step: 8, loss is 1.3774631023406982\n",
      "epoch: 17 step: 9, loss is 1.2812514305114746\n",
      "epoch: 17 step: 10, loss is 1.225455641746521\n",
      "epoch: 17 step: 11, loss is 1.1877790689468384\n",
      "epoch: 17 step: 12, loss is 1.2660070657730103\n",
      "epoch: 17 step: 13, loss is 1.3207305669784546\n",
      "epoch: 17 step: 14, loss is 1.3843731880187988\n",
      "epoch: 17 step: 15, loss is 1.3177076578140259\n",
      "epoch: 17 step: 16, loss is 1.2921507358551025\n",
      "epoch: 17 step: 17, loss is 1.3695013523101807\n",
      "epoch: 17 step: 18, loss is 1.2322781085968018\n",
      "epoch: 17 step: 19, loss is 1.299051284790039\n",
      "epoch: 17 step: 20, loss is 1.2410354614257812\n",
      "epoch: 17 step: 21, loss is 1.3157376050949097\n",
      "epoch: 17 step: 22, loss is 1.2784548997879028\n",
      "epoch: 17 step: 23, loss is 1.333585262298584\n",
      "epoch: 17 step: 24, loss is 1.2475504875183105\n",
      "epoch: 17 step: 25, loss is 1.2663941383361816\n",
      "epoch: 17 step: 26, loss is 1.251211404800415\n",
      "epoch: 17 step: 27, loss is 1.3304985761642456\n",
      "epoch: 17 step: 28, loss is 1.2983418703079224\n",
      "epoch: 17 step: 29, loss is 1.2411880493164062\n",
      "epoch: 17 step: 30, loss is 1.336067795753479\n",
      "epoch: 17 step: 31, loss is 1.222684621810913\n",
      "epoch: 17 step: 32, loss is 1.2405681610107422\n",
      "epoch: 17 step: 33, loss is 1.2831904888153076\n",
      "epoch: 17 step: 34, loss is 1.3542933464050293\n",
      "epoch: 17 step: 35, loss is 1.2993193864822388\n",
      "epoch: 17 step: 36, loss is 1.215963363647461\n",
      "epoch: 17 step: 37, loss is 1.3058298826217651\n",
      "epoch: 17 step: 38, loss is 1.3148328065872192\n",
      "epoch: 17 step: 39, loss is 1.30470609664917\n",
      "epoch: 17 step: 40, loss is 1.2849091291427612\n",
      "epoch: 17 step: 41, loss is 1.2860909700393677\n",
      "epoch: 17 step: 42, loss is 1.2897158861160278\n",
      "epoch: 17 step: 43, loss is 1.3356475830078125\n",
      "epoch: 17 step: 44, loss is 1.2247209548950195\n",
      "epoch: 17 step: 45, loss is 1.1335469484329224\n",
      "epoch: 17 step: 46, loss is 1.3295316696166992\n",
      "epoch: 17 step: 47, loss is 1.2293081283569336\n",
      "epoch: 17 step: 48, loss is 1.2914835214614868\n",
      "epoch: 17 step: 49, loss is 1.3905035257339478\n",
      "epoch: 17 step: 50, loss is 1.295569896697998\n",
      "epoch: 17 step: 51, loss is 1.3401893377304077\n",
      "epoch: 17 step: 52, loss is 1.292676568031311\n",
      "epoch: 17 step: 53, loss is 1.2848809957504272\n",
      "epoch: 17 step: 54, loss is 1.2208791971206665\n",
      "epoch: 17 step: 55, loss is 1.1446528434753418\n",
      "epoch: 17 step: 56, loss is 1.2522515058517456\n",
      "epoch: 17 step: 57, loss is 1.180604338645935\n",
      "epoch: 17 step: 58, loss is 1.2770721912384033\n",
      "epoch: 17 step: 59, loss is 1.250799298286438\n",
      "epoch: 17 step: 60, loss is 1.2348783016204834\n",
      "epoch: 17 step: 61, loss is 1.1732336282730103\n",
      "epoch: 17 step: 62, loss is 1.3738899230957031\n",
      "epoch: 17 step: 63, loss is 1.3335189819335938\n",
      "epoch: 17 step: 64, loss is 1.2072373628616333\n",
      "epoch: 17 step: 65, loss is 1.2815366983413696\n",
      "epoch: 17 step: 66, loss is 1.187389850616455\n",
      "epoch: 17 step: 67, loss is 1.2579911947250366\n",
      "epoch: 17 step: 68, loss is 1.2863054275512695\n",
      "epoch: 17 step: 69, loss is 1.2807670831680298\n",
      "epoch: 17 step: 70, loss is 1.2929236888885498\n",
      "epoch: 17 step: 71, loss is 1.2995240688323975\n",
      "epoch: 17 step: 72, loss is 1.2911550998687744\n",
      "epoch: 17 step: 73, loss is 1.225509524345398\n",
      "epoch: 17 step: 74, loss is 1.245212197303772\n",
      "epoch: 17 step: 75, loss is 1.2998743057250977\n",
      "epoch: 17 step: 76, loss is 1.2599414587020874\n",
      "epoch: 17 step: 77, loss is 1.3445370197296143\n",
      "epoch: 17 step: 78, loss is 1.3349319696426392\n",
      "epoch: 17 step: 79, loss is 1.1443212032318115\n",
      "epoch: 17 step: 80, loss is 1.1841069459915161\n",
      "epoch: 17 step: 81, loss is 1.321354866027832\n",
      "epoch: 17 step: 82, loss is 1.4597135782241821\n",
      "epoch: 17 step: 83, loss is 1.249440312385559\n",
      "epoch: 17 step: 84, loss is 1.222923994064331\n",
      "epoch: 17 step: 85, loss is 1.242363691329956\n",
      "epoch: 17 step: 86, loss is 1.2932049036026\n",
      "epoch: 17 step: 87, loss is 1.2998881340026855\n",
      "epoch: 17 step: 88, loss is 1.3576773405075073\n",
      "epoch: 17 step: 89, loss is 1.1862900257110596\n",
      "epoch: 17 step: 90, loss is 1.2360827922821045\n",
      "epoch: 17 step: 91, loss is 1.3858133554458618\n",
      "epoch: 17 step: 92, loss is 1.2613673210144043\n",
      "epoch: 17 step: 93, loss is 1.1935536861419678\n",
      "epoch: 17 step: 94, loss is 1.2228751182556152\n",
      "epoch: 17 step: 95, loss is 1.2549477815628052\n",
      "epoch: 17 step: 96, loss is 1.178472638130188\n",
      "epoch: 17 step: 97, loss is 1.2146108150482178\n",
      "epoch: 17 step: 98, loss is 1.3567744493484497\n",
      "epoch: 17 step: 99, loss is 1.2340549230575562\n",
      "epoch: 17 step: 100, loss is 1.3006536960601807\n",
      "epoch: 17 step: 101, loss is 1.3031686544418335\n",
      "epoch: 17 step: 102, loss is 1.4457257986068726\n",
      "epoch: 17 step: 103, loss is 1.2568764686584473\n",
      "epoch: 17 step: 104, loss is 1.2488646507263184\n",
      "epoch: 17 step: 105, loss is 1.3203693628311157\n",
      "epoch: 17 step: 106, loss is 1.2825872898101807\n",
      "epoch: 17 step: 107, loss is 1.20609450340271\n",
      "epoch: 17 step: 108, loss is 1.1653262376785278\n",
      "epoch: 17 step: 109, loss is 1.3555785417556763\n",
      "epoch: 17 step: 110, loss is 1.2747235298156738\n",
      "epoch: 17 step: 111, loss is 1.1922621726989746\n",
      "epoch: 17 step: 112, loss is 1.2640401124954224\n",
      "epoch: 17 step: 113, loss is 1.2120879888534546\n",
      "epoch: 17 step: 114, loss is 1.265709638595581\n",
      "epoch: 17 step: 115, loss is 1.3205084800720215\n",
      "epoch: 17 step: 116, loss is 1.2610142230987549\n",
      "epoch: 17 step: 117, loss is 1.2736334800720215\n",
      "epoch: 17 step: 118, loss is 1.3047688007354736\n",
      "epoch: 17 step: 119, loss is 1.339721918106079\n",
      "epoch: 17 step: 120, loss is 1.1361781358718872\n",
      "epoch: 17 step: 121, loss is 1.275758981704712\n",
      "epoch: 17 step: 122, loss is 1.4096226692199707\n",
      "epoch: 17 step: 123, loss is 1.3717992305755615\n",
      "epoch: 17 step: 124, loss is 1.2666207551956177\n",
      "epoch: 17 step: 125, loss is 1.3071225881576538\n",
      "epoch: 17 step: 126, loss is 1.2334816455841064\n",
      "epoch: 17 step: 127, loss is 1.1718130111694336\n",
      "epoch: 17 step: 128, loss is 1.3746410608291626\n",
      "epoch: 17 step: 129, loss is 1.416361927986145\n",
      "epoch: 17 step: 130, loss is 1.2396197319030762\n",
      "epoch: 17 step: 131, loss is 1.270594596862793\n",
      "epoch: 17 step: 132, loss is 1.3428013324737549\n",
      "epoch: 17 step: 133, loss is 1.38472580909729\n",
      "epoch: 17 step: 134, loss is 1.2033638954162598\n",
      "epoch: 17 step: 135, loss is 1.3419153690338135\n",
      "epoch: 17 step: 136, loss is 1.2604929208755493\n",
      "epoch: 17 step: 137, loss is 1.2665003538131714\n",
      "epoch: 17 step: 138, loss is 1.1701079607009888\n",
      "epoch: 17 step: 139, loss is 1.0727589130401611\n",
      "epoch: 17 step: 140, loss is 1.296407699584961\n",
      "epoch: 17 step: 141, loss is 1.331585168838501\n",
      "epoch: 17 step: 142, loss is 1.2026631832122803\n",
      "epoch: 17 step: 143, loss is 1.27056086063385\n",
      "epoch: 17 step: 144, loss is 1.3096448183059692\n",
      "epoch: 17 step: 145, loss is 1.2374000549316406\n",
      "epoch: 17 step: 146, loss is 1.3447551727294922\n",
      "epoch: 17 step: 147, loss is 1.2553187608718872\n",
      "epoch: 17 step: 148, loss is 1.3874439001083374\n",
      "epoch: 17 step: 149, loss is 1.1605029106140137\n",
      "epoch: 17 step: 150, loss is 1.2886805534362793\n",
      "epoch: 17 step: 151, loss is 1.3696049451828003\n",
      "epoch: 17 step: 152, loss is 1.326798915863037\n",
      "epoch: 17 step: 153, loss is 1.2718042135238647\n",
      "epoch: 17 step: 154, loss is 1.3517992496490479\n",
      "epoch: 17 step: 155, loss is 1.2625131607055664\n",
      "epoch: 17 step: 156, loss is 1.300119400024414\n",
      "epoch: 17 step: 157, loss is 1.3354697227478027\n",
      "epoch: 17 step: 158, loss is 1.198500156402588\n",
      "epoch: 17 step: 159, loss is 1.3303905725479126\n",
      "epoch: 17 step: 160, loss is 1.340618371963501\n",
      "epoch: 17 step: 161, loss is 1.3044886589050293\n",
      "epoch: 17 step: 162, loss is 1.2919570207595825\n",
      "epoch: 17 step: 163, loss is 1.2098058462142944\n",
      "epoch: 17 step: 164, loss is 1.2184743881225586\n",
      "epoch: 17 step: 165, loss is 1.2452256679534912\n",
      "epoch: 17 step: 166, loss is 1.2418264150619507\n",
      "epoch: 17 step: 167, loss is 1.229660153388977\n",
      "epoch: 17 step: 168, loss is 1.3291709423065186\n",
      "epoch: 17 step: 169, loss is 1.1147512197494507\n",
      "epoch: 17 step: 170, loss is 1.1269927024841309\n",
      "epoch: 17 step: 171, loss is 1.2493133544921875\n",
      "epoch: 17 step: 172, loss is 1.3026455640792847\n",
      "epoch: 17 step: 173, loss is 1.2355473041534424\n",
      "epoch: 17 step: 174, loss is 1.1905839443206787\n",
      "epoch: 17 step: 175, loss is 1.1851272583007812\n",
      "epoch: 17 step: 176, loss is 1.3803367614746094\n",
      "epoch: 17 step: 177, loss is 1.3092790842056274\n",
      "epoch: 17 step: 178, loss is 1.307244062423706\n",
      "epoch: 17 step: 179, loss is 1.293893814086914\n",
      "epoch: 17 step: 180, loss is 1.180934190750122\n",
      "epoch: 17 step: 181, loss is 1.1929612159729004\n",
      "epoch: 17 step: 182, loss is 1.2309648990631104\n",
      "epoch: 17 step: 183, loss is 1.2600327730178833\n",
      "epoch: 17 step: 184, loss is 1.2619863748550415\n",
      "epoch: 17 step: 185, loss is 1.382588505744934\n",
      "epoch: 17 step: 186, loss is 1.2278698682785034\n",
      "epoch: 17 step: 187, loss is 1.235480546951294\n",
      "epoch: 17 step: 188, loss is 1.3384755849838257\n",
      "epoch: 17 step: 189, loss is 1.289669394493103\n",
      "epoch: 17 step: 190, loss is 1.198814868927002\n",
      "epoch: 17 step: 191, loss is 1.3042036294937134\n",
      "epoch: 17 step: 192, loss is 1.1871237754821777\n",
      "epoch: 17 step: 193, loss is 1.2960065603256226\n",
      "epoch: 17 step: 194, loss is 1.317857027053833\n",
      "epoch: 17 step: 195, loss is 1.2801504135131836\n",
      "epoch: 17 step: 196, loss is 1.3225985765457153\n",
      "epoch: 17 step: 197, loss is 1.283582329750061\n",
      "epoch: 17 step: 198, loss is 1.3511466979980469\n",
      "epoch: 17 step: 199, loss is 1.3006856441497803\n",
      "epoch: 17 step: 200, loss is 1.2529017925262451\n",
      "epoch: 17 step: 201, loss is 1.219931960105896\n",
      "epoch: 17 step: 202, loss is 1.2270375490188599\n",
      "epoch: 17 step: 203, loss is 1.1638860702514648\n",
      "epoch: 17 step: 204, loss is 1.1319595575332642\n",
      "epoch: 17 step: 205, loss is 1.2476813793182373\n",
      "epoch: 17 step: 206, loss is 1.2906498908996582\n",
      "epoch: 17 step: 207, loss is 1.3008493185043335\n",
      "epoch: 17 step: 208, loss is 1.278989553451538\n",
      "epoch: 17 step: 209, loss is 1.234290361404419\n",
      "epoch: 17 step: 210, loss is 1.2429001331329346\n",
      "epoch: 17 step: 211, loss is 1.2900789976119995\n",
      "epoch: 17 step: 212, loss is 1.367262601852417\n",
      "epoch: 17 step: 213, loss is 1.2100688219070435\n",
      "epoch: 17 step: 214, loss is 1.3254462480545044\n",
      "epoch: 17 step: 215, loss is 1.2815842628479004\n",
      "epoch: 17 step: 216, loss is 1.42808997631073\n",
      "epoch: 17 step: 217, loss is 1.272686243057251\n",
      "epoch: 17 step: 218, loss is 1.3761487007141113\n",
      "epoch: 17 step: 219, loss is 1.2200353145599365\n",
      "epoch: 17 step: 220, loss is 1.2757084369659424\n",
      "epoch: 17 step: 221, loss is 1.2880444526672363\n",
      "epoch: 17 step: 222, loss is 1.3304290771484375\n",
      "epoch: 17 step: 223, loss is 1.2496716976165771\n",
      "epoch: 17 step: 224, loss is 1.282301902770996\n",
      "epoch: 17 step: 225, loss is 1.361575722694397\n",
      "epoch: 17 step: 226, loss is 1.3190462589263916\n",
      "epoch: 17 step: 227, loss is 1.2249351739883423\n",
      "epoch: 17 step: 228, loss is 1.1667823791503906\n",
      "epoch: 17 step: 229, loss is 1.3362977504730225\n",
      "epoch: 17 step: 230, loss is 1.2064149379730225\n",
      "epoch: 17 step: 231, loss is 1.22005295753479\n",
      "epoch: 17 step: 232, loss is 1.2371971607208252\n",
      "epoch: 17 step: 233, loss is 1.2844072580337524\n",
      "epoch: 17 step: 234, loss is 1.3501052856445312\n",
      "epoch: 17 step: 235, loss is 1.308307409286499\n",
      "epoch: 17 step: 236, loss is 1.2784403562545776\n",
      "epoch: 17 step: 237, loss is 1.3015437126159668\n",
      "epoch: 17 step: 238, loss is 1.3146294355392456\n",
      "epoch: 17 step: 239, loss is 1.3802671432495117\n",
      "epoch: 17 step: 240, loss is 1.346983551979065\n",
      "epoch: 17 step: 241, loss is 1.33600652217865\n",
      "epoch: 17 step: 242, loss is 1.1991064548492432\n",
      "epoch: 17 step: 243, loss is 1.3115391731262207\n",
      "epoch: 17 step: 244, loss is 1.2742823362350464\n",
      "epoch: 17 step: 245, loss is 1.2519763708114624\n",
      "epoch: 17 step: 246, loss is 1.2511159181594849\n",
      "epoch: 17 step: 247, loss is 1.3113107681274414\n",
      "epoch: 17 step: 248, loss is 1.2769594192504883\n",
      "epoch: 17 step: 249, loss is 1.2413668632507324\n",
      "epoch: 17 step: 250, loss is 1.3623900413513184\n",
      "epoch: 17 step: 251, loss is 1.2336491346359253\n",
      "epoch: 17 step: 252, loss is 1.2390329837799072\n",
      "epoch: 17 step: 253, loss is 1.3072943687438965\n",
      "epoch: 17 step: 254, loss is 1.3692065477371216\n",
      "epoch: 17 step: 255, loss is 1.3874787092208862\n",
      "epoch: 17 step: 256, loss is 1.236833095550537\n",
      "epoch: 17 step: 257, loss is 1.1903314590454102\n",
      "epoch: 17 step: 258, loss is 1.2548179626464844\n",
      "epoch: 17 step: 259, loss is 1.30594801902771\n",
      "epoch: 17 step: 260, loss is 1.2593998908996582\n",
      "epoch: 17 step: 261, loss is 1.3326791524887085\n",
      "epoch: 17 step: 262, loss is 1.3799147605895996\n",
      "epoch: 17 step: 263, loss is 1.3699431419372559\n",
      "epoch: 17 step: 264, loss is 1.1887606382369995\n",
      "epoch: 17 step: 265, loss is 1.3149770498275757\n",
      "epoch: 17 step: 266, loss is 1.1459681987762451\n",
      "epoch: 17 step: 267, loss is 1.2593917846679688\n",
      "epoch: 17 step: 268, loss is 1.2892667055130005\n",
      "epoch: 17 step: 269, loss is 1.2474292516708374\n",
      "epoch: 17 step: 270, loss is 1.2806737422943115\n",
      "epoch: 17 step: 271, loss is 1.341704249382019\n",
      "epoch: 17 step: 272, loss is 1.1809715032577515\n",
      "epoch: 17 step: 273, loss is 1.3345001935958862\n",
      "epoch: 17 step: 274, loss is 1.4175667762756348\n",
      "epoch: 17 step: 275, loss is 1.2430505752563477\n",
      "epoch: 17 step: 276, loss is 1.1719796657562256\n",
      "epoch: 17 step: 277, loss is 1.4368062019348145\n",
      "epoch: 17 step: 278, loss is 1.2645026445388794\n",
      "epoch: 17 step: 279, loss is 1.3322054147720337\n",
      "epoch: 17 step: 280, loss is 1.308945894241333\n",
      "epoch: 17 step: 281, loss is 1.313809871673584\n",
      "epoch: 17 step: 282, loss is 1.3534226417541504\n",
      "epoch: 17 step: 283, loss is 1.295973300933838\n",
      "epoch: 17 step: 284, loss is 1.451543927192688\n",
      "epoch: 17 step: 285, loss is 1.2574795484542847\n",
      "epoch: 17 step: 286, loss is 1.1772189140319824\n",
      "epoch: 17 step: 287, loss is 1.21122145652771\n",
      "epoch: 17 step: 288, loss is 1.2745050191879272\n",
      "epoch: 17 step: 289, loss is 1.2259669303894043\n",
      "epoch: 17 step: 290, loss is 1.2403779029846191\n",
      "epoch: 17 step: 291, loss is 1.0442003011703491\n",
      "epoch: 17 step: 292, loss is 1.2247899770736694\n",
      "epoch: 17 step: 293, loss is 1.200964331626892\n",
      "epoch: 17 step: 294, loss is 1.3490076065063477\n",
      "epoch: 17 step: 295, loss is 1.3118712902069092\n",
      "epoch: 17 step: 296, loss is 1.3000836372375488\n",
      "epoch: 17 step: 297, loss is 1.3668462038040161\n",
      "epoch: 17 step: 298, loss is 1.2226319313049316\n",
      "epoch: 17 step: 299, loss is 1.2815325260162354\n",
      "epoch: 17 step: 300, loss is 1.2882071733474731\n",
      "epoch: 17 step: 301, loss is 1.252407193183899\n",
      "epoch: 17 step: 302, loss is 1.3424856662750244\n",
      "epoch: 17 step: 303, loss is 1.3138582706451416\n",
      "epoch: 17 step: 304, loss is 1.3135180473327637\n",
      "epoch: 17 step: 305, loss is 1.273141622543335\n",
      "epoch: 17 step: 306, loss is 1.3710347414016724\n",
      "epoch: 17 step: 307, loss is 1.2816193103790283\n",
      "epoch: 17 step: 308, loss is 1.2881743907928467\n",
      "epoch: 17 step: 309, loss is 1.2830023765563965\n",
      "epoch: 17 step: 310, loss is 1.3128790855407715\n",
      "epoch: 17 step: 311, loss is 1.27964186668396\n",
      "epoch: 17 step: 312, loss is 1.1892483234405518\n",
      "epoch: 17 step: 313, loss is 1.3186023235321045\n",
      "epoch: 17 step: 314, loss is 1.2890512943267822\n",
      "epoch: 17 step: 315, loss is 1.3952813148498535\n",
      "epoch: 17 step: 316, loss is 1.256975531578064\n",
      "epoch: 17 step: 317, loss is 1.2857143878936768\n",
      "epoch: 17 step: 318, loss is 1.2391244173049927\n",
      "epoch: 17 step: 319, loss is 1.3246235847473145\n",
      "epoch: 17 step: 320, loss is 1.211984395980835\n",
      "epoch: 17 step: 321, loss is 1.2681868076324463\n",
      "epoch: 17 step: 322, loss is 1.3291490077972412\n",
      "epoch: 17 step: 323, loss is 1.338623046875\n",
      "epoch: 17 step: 324, loss is 1.1561968326568604\n",
      "epoch: 17 step: 325, loss is 1.3585023880004883\n",
      "epoch: 17 step: 326, loss is 1.3036302328109741\n",
      "epoch: 17 step: 327, loss is 1.385136365890503\n",
      "epoch: 17 step: 328, loss is 1.2967169284820557\n",
      "epoch: 17 step: 329, loss is 1.172133207321167\n",
      "epoch: 17 step: 330, loss is 1.401888132095337\n",
      "epoch: 17 step: 331, loss is 1.3714892864227295\n",
      "epoch: 17 step: 332, loss is 1.2255687713623047\n",
      "epoch: 17 step: 333, loss is 1.3643159866333008\n",
      "epoch: 17 step: 334, loss is 1.2370178699493408\n",
      "epoch: 17 step: 335, loss is 1.3910934925079346\n",
      "epoch: 17 step: 336, loss is 1.2404625415802002\n",
      "epoch: 17 step: 337, loss is 1.2501899003982544\n",
      "epoch: 17 step: 338, loss is 1.2051712274551392\n",
      "epoch: 17 step: 339, loss is 1.2227293252944946\n",
      "epoch: 17 step: 340, loss is 1.3453137874603271\n",
      "epoch: 17 step: 341, loss is 1.3742523193359375\n",
      "epoch: 17 step: 342, loss is 1.3872482776641846\n",
      "epoch: 17 step: 343, loss is 1.2327873706817627\n",
      "epoch: 17 step: 344, loss is 1.469711422920227\n",
      "epoch: 17 step: 345, loss is 1.3275285959243774\n",
      "epoch: 17 step: 346, loss is 1.2779592275619507\n",
      "epoch: 17 step: 347, loss is 1.2227058410644531\n",
      "epoch: 17 step: 348, loss is 1.3566100597381592\n",
      "epoch: 17 step: 349, loss is 1.3099784851074219\n",
      "epoch: 17 step: 350, loss is 1.302549123764038\n",
      "epoch: 17 step: 351, loss is 1.3606468439102173\n",
      "epoch: 17 step: 352, loss is 1.3854097127914429\n",
      "epoch: 17 step: 353, loss is 1.2565358877182007\n",
      "epoch: 17 step: 354, loss is 1.263478398323059\n",
      "epoch: 17 step: 355, loss is 1.2255290746688843\n",
      "epoch: 17 step: 356, loss is 1.2052637338638306\n",
      "epoch: 17 step: 357, loss is 1.1804866790771484\n",
      "epoch: 17 step: 358, loss is 1.2780152559280396\n",
      "epoch: 17 step: 359, loss is 1.222047209739685\n",
      "epoch: 17 step: 360, loss is 1.2600409984588623\n",
      "epoch: 17 step: 361, loss is 1.2026883363723755\n",
      "epoch: 17 step: 362, loss is 1.3277300596237183\n",
      "epoch: 17 step: 363, loss is 1.2245874404907227\n",
      "epoch: 17 step: 364, loss is 1.3366135358810425\n",
      "epoch: 17 step: 365, loss is 1.285751461982727\n",
      "epoch: 17 step: 366, loss is 1.221859335899353\n",
      "epoch: 17 step: 367, loss is 1.2857786417007446\n",
      "epoch: 17 step: 368, loss is 1.2992644309997559\n",
      "epoch: 17 step: 369, loss is 1.2912085056304932\n",
      "epoch: 17 step: 370, loss is 1.1935899257659912\n",
      "epoch: 17 step: 371, loss is 1.2617518901824951\n",
      "epoch: 17 step: 372, loss is 1.3289490938186646\n",
      "epoch: 17 step: 373, loss is 1.198481798171997\n",
      "epoch: 17 step: 374, loss is 1.2807263135910034\n",
      "epoch: 17 step: 375, loss is 1.2265087366104126\n",
      "epoch: 17 step: 376, loss is 1.2973277568817139\n",
      "epoch: 17 step: 377, loss is 1.2313839197158813\n",
      "epoch: 17 step: 378, loss is 1.2102704048156738\n",
      "epoch: 17 step: 379, loss is 1.293256402015686\n",
      "epoch: 17 step: 380, loss is 1.310091257095337\n",
      "epoch: 17 step: 381, loss is 1.3114421367645264\n",
      "epoch: 17 step: 382, loss is 1.1204266548156738\n",
      "epoch: 17 step: 383, loss is 1.2242896556854248\n",
      "epoch: 17 step: 384, loss is 1.3677364587783813\n",
      "epoch: 17 step: 385, loss is 1.482177734375\n",
      "epoch: 17 step: 386, loss is 1.3005156517028809\n",
      "epoch: 17 step: 387, loss is 1.294296383857727\n",
      "epoch: 17 step: 388, loss is 1.2484052181243896\n",
      "epoch: 17 step: 389, loss is 1.3346892595291138\n",
      "epoch: 17 step: 390, loss is 1.2831881046295166\n",
      "Train epoch time: 173528.097 ms, per step time: 444.944 ms\n",
      "epoch: 18 step: 1, loss is 1.2408307790756226\n",
      "epoch: 18 step: 2, loss is 1.1631654500961304\n",
      "epoch: 18 step: 3, loss is 1.2508776187896729\n",
      "epoch: 18 step: 4, loss is 1.3215436935424805\n",
      "epoch: 18 step: 5, loss is 1.2546708583831787\n",
      "epoch: 18 step: 6, loss is 1.394094705581665\n",
      "epoch: 18 step: 7, loss is 1.3723722696304321\n",
      "epoch: 18 step: 8, loss is 1.2644695043563843\n",
      "epoch: 18 step: 9, loss is 1.2649500370025635\n",
      "epoch: 18 step: 10, loss is 1.2527767419815063\n",
      "epoch: 18 step: 11, loss is 1.2506619691848755\n",
      "epoch: 18 step: 12, loss is 1.397718906402588\n",
      "epoch: 18 step: 13, loss is 1.2815887928009033\n",
      "epoch: 18 step: 14, loss is 1.1661896705627441\n",
      "epoch: 18 step: 15, loss is 1.296334147453308\n",
      "epoch: 18 step: 16, loss is 1.269213080406189\n",
      "epoch: 18 step: 17, loss is 1.3873631954193115\n",
      "epoch: 18 step: 18, loss is 1.389634370803833\n",
      "epoch: 18 step: 19, loss is 1.373859167098999\n",
      "epoch: 18 step: 20, loss is 1.3451415300369263\n",
      "epoch: 18 step: 21, loss is 1.2998294830322266\n",
      "epoch: 18 step: 22, loss is 1.3510829210281372\n",
      "epoch: 18 step: 23, loss is 1.212601661682129\n",
      "epoch: 18 step: 24, loss is 1.2777156829833984\n",
      "epoch: 18 step: 25, loss is 1.3910162448883057\n",
      "epoch: 18 step: 26, loss is 1.2946009635925293\n",
      "epoch: 18 step: 27, loss is 1.159902572631836\n",
      "epoch: 18 step: 28, loss is 1.1535108089447021\n",
      "epoch: 18 step: 29, loss is 1.2742202281951904\n",
      "epoch: 18 step: 30, loss is 1.3785312175750732\n",
      "epoch: 18 step: 31, loss is 1.3318865299224854\n",
      "epoch: 18 step: 32, loss is 1.401235818862915\n",
      "epoch: 18 step: 33, loss is 1.3009068965911865\n",
      "epoch: 18 step: 34, loss is 1.2024548053741455\n",
      "epoch: 18 step: 35, loss is 1.2054381370544434\n",
      "epoch: 18 step: 36, loss is 1.2389249801635742\n",
      "epoch: 18 step: 37, loss is 1.3379826545715332\n",
      "epoch: 18 step: 38, loss is 1.2599505186080933\n",
      "epoch: 18 step: 39, loss is 1.2153010368347168\n",
      "epoch: 18 step: 40, loss is 1.2709927558898926\n",
      "epoch: 18 step: 41, loss is 1.2346398830413818\n",
      "epoch: 18 step: 42, loss is 1.2527607679367065\n",
      "epoch: 18 step: 43, loss is 1.3056933879852295\n",
      "epoch: 18 step: 44, loss is 1.2166435718536377\n",
      "epoch: 18 step: 45, loss is 1.205674409866333\n",
      "epoch: 18 step: 46, loss is 1.255561351776123\n",
      "epoch: 18 step: 47, loss is 1.187870740890503\n",
      "epoch: 18 step: 48, loss is 1.1850006580352783\n",
      "epoch: 18 step: 49, loss is 1.3247506618499756\n",
      "epoch: 18 step: 50, loss is 1.1785167455673218\n",
      "epoch: 18 step: 51, loss is 1.298708200454712\n",
      "epoch: 18 step: 52, loss is 1.2400895357131958\n",
      "epoch: 18 step: 53, loss is 1.1482577323913574\n",
      "epoch: 18 step: 54, loss is 1.2366845607757568\n",
      "epoch: 18 step: 55, loss is 1.2738230228424072\n",
      "epoch: 18 step: 56, loss is 1.2240009307861328\n",
      "epoch: 18 step: 57, loss is 1.2387962341308594\n",
      "epoch: 18 step: 58, loss is 1.3161710500717163\n",
      "epoch: 18 step: 59, loss is 1.2638301849365234\n",
      "epoch: 18 step: 60, loss is 1.321199655532837\n",
      "epoch: 18 step: 61, loss is 1.2677834033966064\n",
      "epoch: 18 step: 62, loss is 1.429770827293396\n",
      "epoch: 18 step: 63, loss is 1.285109519958496\n",
      "epoch: 18 step: 64, loss is 1.1874083280563354\n",
      "epoch: 18 step: 65, loss is 1.2420833110809326\n",
      "epoch: 18 step: 66, loss is 1.3374346494674683\n",
      "epoch: 18 step: 67, loss is 1.23238205909729\n",
      "epoch: 18 step: 68, loss is 1.1722311973571777\n",
      "epoch: 18 step: 69, loss is 1.2684589624404907\n",
      "epoch: 18 step: 70, loss is 1.094332218170166\n",
      "epoch: 18 step: 71, loss is 1.411271333694458\n",
      "epoch: 18 step: 72, loss is 1.2503639459609985\n",
      "epoch: 18 step: 73, loss is 1.260380744934082\n",
      "epoch: 18 step: 74, loss is 1.2549595832824707\n",
      "epoch: 18 step: 75, loss is 1.2352850437164307\n",
      "epoch: 18 step: 76, loss is 1.2738099098205566\n",
      "epoch: 18 step: 77, loss is 1.2124552726745605\n",
      "epoch: 18 step: 78, loss is 1.1744130849838257\n",
      "epoch: 18 step: 79, loss is 1.3997883796691895\n",
      "epoch: 18 step: 80, loss is 1.2910553216934204\n",
      "epoch: 18 step: 81, loss is 1.195556402206421\n",
      "epoch: 18 step: 82, loss is 1.3160736560821533\n",
      "epoch: 18 step: 83, loss is 1.2062474489212036\n",
      "epoch: 18 step: 84, loss is 1.163550853729248\n",
      "epoch: 18 step: 85, loss is 1.3720133304595947\n",
      "epoch: 18 step: 86, loss is 1.34646475315094\n",
      "epoch: 18 step: 87, loss is 1.3491789102554321\n",
      "epoch: 18 step: 88, loss is 1.2830607891082764\n",
      "epoch: 18 step: 89, loss is 1.3622148036956787\n",
      "epoch: 18 step: 90, loss is 1.447278618812561\n",
      "epoch: 18 step: 91, loss is 1.2982916831970215\n",
      "epoch: 18 step: 92, loss is 1.3275953531265259\n",
      "epoch: 18 step: 93, loss is 1.2039486169815063\n",
      "epoch: 18 step: 94, loss is 1.2259231805801392\n",
      "epoch: 18 step: 95, loss is 1.3236632347106934\n",
      "epoch: 18 step: 96, loss is 1.212672472000122\n",
      "epoch: 18 step: 97, loss is 1.1341280937194824\n",
      "epoch: 18 step: 98, loss is 1.433977723121643\n",
      "epoch: 18 step: 99, loss is 1.290435552597046\n",
      "epoch: 18 step: 100, loss is 1.3257622718811035\n",
      "epoch: 18 step: 101, loss is 1.2960487604141235\n",
      "epoch: 18 step: 102, loss is 1.103806495666504\n",
      "epoch: 18 step: 103, loss is 1.175497055053711\n",
      "epoch: 18 step: 104, loss is 1.264054298400879\n",
      "epoch: 18 step: 105, loss is 1.2803759574890137\n",
      "epoch: 18 step: 106, loss is 1.3863180875778198\n",
      "epoch: 18 step: 107, loss is 1.2417964935302734\n",
      "epoch: 18 step: 108, loss is 1.2457609176635742\n",
      "epoch: 18 step: 109, loss is 1.2825593948364258\n",
      "epoch: 18 step: 110, loss is 1.2048228979110718\n",
      "epoch: 18 step: 111, loss is 1.3289906978607178\n",
      "epoch: 18 step: 112, loss is 1.2494451999664307\n",
      "epoch: 18 step: 113, loss is 1.3406543731689453\n",
      "epoch: 18 step: 114, loss is 1.341090440750122\n",
      "epoch: 18 step: 115, loss is 1.2523081302642822\n",
      "epoch: 18 step: 116, loss is 1.32777738571167\n",
      "epoch: 18 step: 117, loss is 1.2400691509246826\n",
      "epoch: 18 step: 118, loss is 1.2756799459457397\n",
      "epoch: 18 step: 119, loss is 1.237524390220642\n",
      "epoch: 18 step: 120, loss is 1.1822166442871094\n",
      "epoch: 18 step: 121, loss is 1.112705945968628\n",
      "epoch: 18 step: 122, loss is 1.4473482370376587\n",
      "epoch: 18 step: 123, loss is 1.3096843957901\n",
      "epoch: 18 step: 124, loss is 1.142240047454834\n",
      "epoch: 18 step: 125, loss is 1.3415658473968506\n",
      "epoch: 18 step: 126, loss is 1.2950247526168823\n",
      "epoch: 18 step: 127, loss is 1.290642499923706\n",
      "epoch: 18 step: 128, loss is 1.2513409852981567\n",
      "epoch: 18 step: 129, loss is 1.190948247909546\n",
      "epoch: 18 step: 130, loss is 1.257218599319458\n",
      "epoch: 18 step: 131, loss is 1.1994636058807373\n",
      "epoch: 18 step: 132, loss is 1.229267954826355\n",
      "epoch: 18 step: 133, loss is 1.1434407234191895\n",
      "epoch: 18 step: 134, loss is 1.3151698112487793\n",
      "epoch: 18 step: 135, loss is 1.3071187734603882\n",
      "epoch: 18 step: 136, loss is 1.403760552406311\n",
      "epoch: 18 step: 137, loss is 1.1889783143997192\n",
      "epoch: 18 step: 138, loss is 1.3074626922607422\n",
      "epoch: 18 step: 139, loss is 1.3251692056655884\n",
      "epoch: 18 step: 140, loss is 1.3451709747314453\n",
      "epoch: 18 step: 141, loss is 1.3961148262023926\n",
      "epoch: 18 step: 142, loss is 1.2545212507247925\n",
      "epoch: 18 step: 143, loss is 1.1623519659042358\n",
      "epoch: 18 step: 144, loss is 1.3533824682235718\n",
      "epoch: 18 step: 145, loss is 1.1214275360107422\n",
      "epoch: 18 step: 146, loss is 1.193050503730774\n",
      "epoch: 18 step: 147, loss is 1.2078412771224976\n",
      "epoch: 18 step: 148, loss is 1.4009811878204346\n",
      "epoch: 18 step: 149, loss is 1.340140461921692\n",
      "epoch: 18 step: 150, loss is 1.2996959686279297\n",
      "epoch: 18 step: 151, loss is 1.3615777492523193\n",
      "epoch: 18 step: 152, loss is 1.224095106124878\n",
      "epoch: 18 step: 153, loss is 1.2668458223342896\n",
      "epoch: 18 step: 154, loss is 1.2781115770339966\n",
      "epoch: 18 step: 155, loss is 1.3097658157348633\n",
      "epoch: 18 step: 156, loss is 1.1893190145492554\n",
      "epoch: 18 step: 157, loss is 1.3051626682281494\n",
      "epoch: 18 step: 158, loss is 1.1551768779754639\n",
      "epoch: 18 step: 159, loss is 1.232947826385498\n",
      "epoch: 18 step: 160, loss is 1.1991558074951172\n",
      "epoch: 18 step: 161, loss is 1.1686110496520996\n",
      "epoch: 18 step: 162, loss is 1.3586561679840088\n",
      "epoch: 18 step: 163, loss is 1.2197626829147339\n",
      "epoch: 18 step: 164, loss is 1.2259737253189087\n",
      "epoch: 18 step: 165, loss is 1.3768372535705566\n",
      "epoch: 18 step: 166, loss is 1.2867240905761719\n",
      "epoch: 18 step: 167, loss is 1.3954861164093018\n",
      "epoch: 18 step: 168, loss is 1.2570277452468872\n",
      "epoch: 18 step: 169, loss is 1.2814252376556396\n",
      "epoch: 18 step: 170, loss is 1.3309948444366455\n",
      "epoch: 18 step: 171, loss is 1.3262298107147217\n",
      "epoch: 18 step: 172, loss is 1.3559606075286865\n",
      "epoch: 18 step: 173, loss is 1.3033673763275146\n",
      "epoch: 18 step: 174, loss is 1.315629482269287\n",
      "epoch: 18 step: 175, loss is 1.235244870185852\n",
      "epoch: 18 step: 176, loss is 1.1367499828338623\n",
      "epoch: 18 step: 177, loss is 1.2653999328613281\n",
      "epoch: 18 step: 178, loss is 1.2782363891601562\n",
      "epoch: 18 step: 179, loss is 1.50831139087677\n",
      "epoch: 18 step: 180, loss is 1.3561122417449951\n",
      "epoch: 18 step: 181, loss is 1.3872990608215332\n",
      "epoch: 18 step: 182, loss is 1.2995280027389526\n",
      "epoch: 18 step: 183, loss is 1.386042833328247\n",
      "epoch: 18 step: 184, loss is 1.3366293907165527\n",
      "epoch: 18 step: 185, loss is 1.1816208362579346\n",
      "epoch: 18 step: 186, loss is 1.322402000427246\n",
      "epoch: 18 step: 187, loss is 1.2758283615112305\n",
      "epoch: 18 step: 188, loss is 1.239975929260254\n",
      "epoch: 18 step: 189, loss is 1.1365861892700195\n",
      "epoch: 18 step: 190, loss is 1.2241218090057373\n",
      "epoch: 18 step: 191, loss is 1.1645605564117432\n",
      "epoch: 18 step: 192, loss is 1.2266727685928345\n",
      "epoch: 18 step: 193, loss is 1.278414249420166\n",
      "epoch: 18 step: 194, loss is 1.227318286895752\n",
      "epoch: 18 step: 195, loss is 1.2730916738510132\n",
      "epoch: 18 step: 196, loss is 1.2435685396194458\n",
      "epoch: 18 step: 197, loss is 1.1838115453720093\n",
      "epoch: 18 step: 198, loss is 1.2310142517089844\n",
      "epoch: 18 step: 199, loss is 1.2436530590057373\n",
      "epoch: 18 step: 200, loss is 1.255223035812378\n",
      "epoch: 18 step: 201, loss is 1.2575201988220215\n",
      "epoch: 18 step: 202, loss is 1.3002808094024658\n",
      "epoch: 18 step: 203, loss is 1.2392475605010986\n",
      "epoch: 18 step: 204, loss is 1.258548378944397\n",
      "epoch: 18 step: 205, loss is 1.1329553127288818\n",
      "epoch: 18 step: 206, loss is 1.2797271013259888\n",
      "epoch: 18 step: 207, loss is 1.2656276226043701\n",
      "epoch: 18 step: 208, loss is 1.3422471284866333\n",
      "epoch: 18 step: 209, loss is 1.1581379175186157\n",
      "epoch: 18 step: 210, loss is 1.2374578714370728\n",
      "epoch: 18 step: 211, loss is 1.1792701482772827\n",
      "epoch: 18 step: 212, loss is 1.3797317743301392\n",
      "epoch: 18 step: 213, loss is 1.2601816654205322\n",
      "epoch: 18 step: 214, loss is 1.3829399347305298\n",
      "epoch: 18 step: 215, loss is 1.1546679735183716\n",
      "epoch: 18 step: 216, loss is 1.2222578525543213\n",
      "epoch: 18 step: 217, loss is 1.2759662866592407\n",
      "epoch: 18 step: 218, loss is 1.2735270261764526\n",
      "epoch: 18 step: 219, loss is 1.2836666107177734\n",
      "epoch: 18 step: 220, loss is 1.298181176185608\n",
      "epoch: 18 step: 221, loss is 1.2874706983566284\n",
      "epoch: 18 step: 222, loss is 1.3864414691925049\n",
      "epoch: 18 step: 223, loss is 1.2258033752441406\n",
      "epoch: 18 step: 224, loss is 1.2604951858520508\n",
      "epoch: 18 step: 225, loss is 1.1340363025665283\n",
      "epoch: 18 step: 226, loss is 1.311391830444336\n",
      "epoch: 18 step: 227, loss is 1.2366068363189697\n",
      "epoch: 18 step: 228, loss is 1.3509578704833984\n",
      "epoch: 18 step: 229, loss is 1.3469948768615723\n",
      "epoch: 18 step: 230, loss is 1.2797743082046509\n",
      "epoch: 18 step: 231, loss is 1.2461475133895874\n",
      "epoch: 18 step: 232, loss is 1.2254245281219482\n",
      "epoch: 18 step: 233, loss is 1.2338999509811401\n",
      "epoch: 18 step: 234, loss is 1.2284393310546875\n",
      "epoch: 18 step: 235, loss is 1.2188624143600464\n",
      "epoch: 18 step: 236, loss is 1.1981627941131592\n",
      "epoch: 18 step: 237, loss is 1.289976954460144\n",
      "epoch: 18 step: 238, loss is 1.32200026512146\n",
      "epoch: 18 step: 239, loss is 1.199474811553955\n",
      "epoch: 18 step: 240, loss is 1.2911983728408813\n",
      "epoch: 18 step: 241, loss is 1.246806025505066\n",
      "epoch: 18 step: 242, loss is 1.1827571392059326\n",
      "epoch: 18 step: 243, loss is 1.2565059661865234\n",
      "epoch: 18 step: 244, loss is 1.173398733139038\n",
      "epoch: 18 step: 245, loss is 1.1042786836624146\n",
      "epoch: 18 step: 246, loss is 1.3526508808135986\n",
      "epoch: 18 step: 247, loss is 1.1906201839447021\n",
      "epoch: 18 step: 248, loss is 1.2120916843414307\n",
      "epoch: 18 step: 249, loss is 1.22701096534729\n",
      "epoch: 18 step: 250, loss is 1.1522451639175415\n",
      "epoch: 18 step: 251, loss is 1.180389642715454\n",
      "epoch: 18 step: 252, loss is 1.1943082809448242\n",
      "epoch: 18 step: 253, loss is 1.2530055046081543\n",
      "epoch: 18 step: 254, loss is 1.3444678783416748\n",
      "epoch: 18 step: 255, loss is 1.2394555807113647\n",
      "epoch: 18 step: 256, loss is 1.2467619180679321\n",
      "epoch: 18 step: 257, loss is 1.3371453285217285\n",
      "epoch: 18 step: 258, loss is 1.2141703367233276\n",
      "epoch: 18 step: 259, loss is 1.2362701892852783\n",
      "epoch: 18 step: 260, loss is 1.2596027851104736\n",
      "epoch: 18 step: 261, loss is 1.2531158924102783\n",
      "epoch: 18 step: 262, loss is 1.259497880935669\n",
      "epoch: 18 step: 263, loss is 1.2207286357879639\n",
      "epoch: 18 step: 264, loss is 1.2792558670043945\n",
      "epoch: 18 step: 265, loss is 1.1406750679016113\n",
      "epoch: 18 step: 266, loss is 1.1462721824645996\n",
      "epoch: 18 step: 267, loss is 1.2161365747451782\n",
      "epoch: 18 step: 268, loss is 1.2592756748199463\n",
      "epoch: 18 step: 269, loss is 1.2021933794021606\n",
      "epoch: 18 step: 270, loss is 1.2119475603103638\n",
      "epoch: 18 step: 271, loss is 1.3426761627197266\n",
      "epoch: 18 step: 272, loss is 1.3103561401367188\n",
      "epoch: 18 step: 273, loss is 1.190019130706787\n",
      "epoch: 18 step: 274, loss is 1.2032732963562012\n",
      "epoch: 18 step: 275, loss is 1.3687396049499512\n",
      "epoch: 18 step: 276, loss is 1.3653008937835693\n",
      "epoch: 18 step: 277, loss is 1.2169262170791626\n",
      "epoch: 18 step: 278, loss is 1.1504716873168945\n",
      "epoch: 18 step: 279, loss is 1.2782248258590698\n",
      "epoch: 18 step: 280, loss is 1.2091617584228516\n",
      "epoch: 18 step: 281, loss is 1.287479043006897\n",
      "epoch: 18 step: 282, loss is 1.44338858127594\n",
      "epoch: 18 step: 283, loss is 1.2505539655685425\n",
      "epoch: 18 step: 284, loss is 1.209262490272522\n",
      "epoch: 18 step: 285, loss is 1.306610107421875\n",
      "epoch: 18 step: 286, loss is 1.2958483695983887\n",
      "epoch: 18 step: 287, loss is 1.3734177350997925\n",
      "epoch: 18 step: 288, loss is 1.3063175678253174\n",
      "epoch: 18 step: 289, loss is 1.2905946969985962\n",
      "epoch: 18 step: 290, loss is 1.3707287311553955\n",
      "epoch: 18 step: 291, loss is 1.2031371593475342\n",
      "epoch: 18 step: 292, loss is 1.224056601524353\n",
      "epoch: 18 step: 293, loss is 1.2058342695236206\n",
      "epoch: 18 step: 294, loss is 1.3167543411254883\n",
      "epoch: 18 step: 295, loss is 1.3464583158493042\n",
      "epoch: 18 step: 296, loss is 1.2814016342163086\n",
      "epoch: 18 step: 297, loss is 1.24294114112854\n",
      "epoch: 18 step: 298, loss is 1.3411158323287964\n",
      "epoch: 18 step: 299, loss is 1.2796013355255127\n",
      "epoch: 18 step: 300, loss is 1.3203034400939941\n",
      "epoch: 18 step: 301, loss is 1.3372600078582764\n",
      "epoch: 18 step: 302, loss is 1.3061481714248657\n",
      "epoch: 18 step: 303, loss is 1.2775193452835083\n",
      "epoch: 18 step: 304, loss is 1.3021321296691895\n",
      "epoch: 18 step: 305, loss is 1.2165873050689697\n",
      "epoch: 18 step: 306, loss is 1.2892415523529053\n",
      "epoch: 18 step: 307, loss is 1.203650951385498\n",
      "epoch: 18 step: 308, loss is 1.2212783098220825\n",
      "epoch: 18 step: 309, loss is 1.2779779434204102\n",
      "epoch: 18 step: 310, loss is 1.3220016956329346\n",
      "epoch: 18 step: 311, loss is 1.2705368995666504\n",
      "epoch: 18 step: 312, loss is 1.2631011009216309\n",
      "epoch: 18 step: 313, loss is 1.215477705001831\n",
      "epoch: 18 step: 314, loss is 1.2148456573486328\n",
      "epoch: 18 step: 315, loss is 1.2699369192123413\n",
      "epoch: 18 step: 316, loss is 1.1519876718521118\n",
      "epoch: 18 step: 317, loss is 1.2830361127853394\n",
      "epoch: 18 step: 318, loss is 1.2917588949203491\n",
      "epoch: 18 step: 319, loss is 1.202256202697754\n",
      "epoch: 18 step: 320, loss is 1.3007797002792358\n",
      "epoch: 18 step: 321, loss is 1.165956735610962\n",
      "epoch: 18 step: 322, loss is 1.2434157133102417\n",
      "epoch: 18 step: 323, loss is 1.2057571411132812\n",
      "epoch: 18 step: 324, loss is 1.2862411737442017\n",
      "epoch: 18 step: 325, loss is 1.3014508485794067\n",
      "epoch: 18 step: 326, loss is 1.210720419883728\n",
      "epoch: 18 step: 327, loss is 1.1800892353057861\n",
      "epoch: 18 step: 328, loss is 1.2489469051361084\n",
      "epoch: 18 step: 329, loss is 1.251479983329773\n",
      "epoch: 18 step: 330, loss is 1.308467149734497\n",
      "epoch: 18 step: 331, loss is 1.3165773153305054\n",
      "epoch: 18 step: 332, loss is 1.3131537437438965\n",
      "epoch: 18 step: 333, loss is 1.1482688188552856\n",
      "epoch: 18 step: 334, loss is 1.3227837085723877\n",
      "epoch: 18 step: 335, loss is 1.2127957344055176\n",
      "epoch: 18 step: 336, loss is 1.2674750089645386\n",
      "epoch: 18 step: 337, loss is 1.3049931526184082\n",
      "epoch: 18 step: 338, loss is 1.2076619863510132\n",
      "epoch: 18 step: 339, loss is 1.2186959981918335\n",
      "epoch: 18 step: 340, loss is 1.1009739637374878\n",
      "epoch: 18 step: 341, loss is 1.2310254573822021\n",
      "epoch: 18 step: 342, loss is 1.3616437911987305\n",
      "epoch: 18 step: 343, loss is 1.1661268472671509\n",
      "epoch: 18 step: 344, loss is 1.233336091041565\n",
      "epoch: 18 step: 345, loss is 1.2761270999908447\n",
      "epoch: 18 step: 346, loss is 1.2608637809753418\n",
      "epoch: 18 step: 347, loss is 1.1595410108566284\n",
      "epoch: 18 step: 348, loss is 1.3041821718215942\n",
      "epoch: 18 step: 349, loss is 1.3392540216445923\n",
      "epoch: 18 step: 350, loss is 1.3197031021118164\n",
      "epoch: 18 step: 351, loss is 1.1354902982711792\n",
      "epoch: 18 step: 352, loss is 1.1690433025360107\n",
      "epoch: 18 step: 353, loss is 1.364578127861023\n",
      "epoch: 18 step: 354, loss is 1.2666820287704468\n",
      "epoch: 18 step: 355, loss is 1.2176129817962646\n",
      "epoch: 18 step: 356, loss is 1.4017055034637451\n",
      "epoch: 18 step: 357, loss is 1.3278132677078247\n",
      "epoch: 18 step: 358, loss is 1.261662244796753\n",
      "epoch: 18 step: 359, loss is 1.3867642879486084\n",
      "epoch: 18 step: 360, loss is 1.2417798042297363\n",
      "epoch: 18 step: 361, loss is 1.3095788955688477\n",
      "epoch: 18 step: 362, loss is 1.2731595039367676\n",
      "epoch: 18 step: 363, loss is 1.271857738494873\n",
      "epoch: 18 step: 364, loss is 1.2924326658248901\n",
      "epoch: 18 step: 365, loss is 1.2241945266723633\n",
      "epoch: 18 step: 366, loss is 1.271817922592163\n",
      "epoch: 18 step: 367, loss is 1.333160161972046\n",
      "epoch: 18 step: 368, loss is 1.4300614595413208\n",
      "epoch: 18 step: 369, loss is 1.1999680995941162\n",
      "epoch: 18 step: 370, loss is 1.26017427444458\n",
      "epoch: 18 step: 371, loss is 1.278554916381836\n",
      "epoch: 18 step: 372, loss is 1.3502068519592285\n",
      "epoch: 18 step: 373, loss is 1.2744922637939453\n",
      "epoch: 18 step: 374, loss is 1.3065853118896484\n",
      "epoch: 18 step: 375, loss is 1.2810032367706299\n",
      "epoch: 18 step: 376, loss is 1.2976787090301514\n",
      "epoch: 18 step: 377, loss is 1.2381786108016968\n",
      "epoch: 18 step: 378, loss is 1.2582179307937622\n",
      "epoch: 18 step: 379, loss is 1.370807409286499\n",
      "epoch: 18 step: 380, loss is 1.1763852834701538\n",
      "epoch: 18 step: 381, loss is 1.203294038772583\n",
      "epoch: 18 step: 382, loss is 1.2204780578613281\n",
      "epoch: 18 step: 383, loss is 1.2748467922210693\n",
      "epoch: 18 step: 384, loss is 1.329321026802063\n",
      "epoch: 18 step: 385, loss is 1.1877288818359375\n",
      "epoch: 18 step: 386, loss is 1.2016034126281738\n",
      "epoch: 18 step: 387, loss is 1.2249866724014282\n",
      "epoch: 18 step: 388, loss is 1.3475239276885986\n",
      "epoch: 18 step: 389, loss is 1.1403566598892212\n",
      "epoch: 18 step: 390, loss is 1.2192060947418213\n",
      "Train epoch time: 166754.063 ms, per step time: 427.575 ms\n",
      "epoch: 19 step: 1, loss is 1.1571192741394043\n",
      "epoch: 19 step: 2, loss is 1.3018152713775635\n",
      "epoch: 19 step: 3, loss is 1.417311668395996\n",
      "epoch: 19 step: 4, loss is 1.29036545753479\n",
      "epoch: 19 step: 5, loss is 1.3087507486343384\n",
      "epoch: 19 step: 6, loss is 1.2259279489517212\n",
      "epoch: 19 step: 7, loss is 1.2368073463439941\n",
      "epoch: 19 step: 8, loss is 1.256930947303772\n",
      "epoch: 19 step: 9, loss is 1.3803741931915283\n",
      "epoch: 19 step: 10, loss is 1.2651050090789795\n",
      "epoch: 19 step: 11, loss is 1.1550512313842773\n",
      "epoch: 19 step: 12, loss is 1.3004356622695923\n",
      "epoch: 19 step: 13, loss is 1.2313997745513916\n",
      "epoch: 19 step: 14, loss is 1.3288365602493286\n",
      "epoch: 19 step: 15, loss is 1.1413860321044922\n",
      "epoch: 19 step: 16, loss is 1.259682536125183\n",
      "epoch: 19 step: 17, loss is 1.2007008790969849\n",
      "epoch: 19 step: 18, loss is 1.2584697008132935\n",
      "epoch: 19 step: 19, loss is 1.3168666362762451\n",
      "epoch: 19 step: 20, loss is 1.3369873762130737\n",
      "epoch: 19 step: 21, loss is 1.2681804895401\n",
      "epoch: 19 step: 22, loss is 1.1593093872070312\n",
      "epoch: 19 step: 23, loss is 1.2112013101577759\n",
      "epoch: 19 step: 24, loss is 1.3045576810836792\n",
      "epoch: 19 step: 25, loss is 1.253093957901001\n",
      "epoch: 19 step: 26, loss is 1.256956696510315\n",
      "epoch: 19 step: 27, loss is 1.192374348640442\n",
      "epoch: 19 step: 28, loss is 1.2644758224487305\n",
      "epoch: 19 step: 29, loss is 1.3127388954162598\n",
      "epoch: 19 step: 30, loss is 1.2435096502304077\n",
      "epoch: 19 step: 31, loss is 1.261054277420044\n",
      "epoch: 19 step: 32, loss is 1.2385309934616089\n",
      "epoch: 19 step: 33, loss is 1.082090973854065\n",
      "epoch: 19 step: 34, loss is 1.195878267288208\n",
      "epoch: 19 step: 35, loss is 1.2517796754837036\n",
      "epoch: 19 step: 36, loss is 1.3034214973449707\n",
      "epoch: 19 step: 37, loss is 1.234901785850525\n",
      "epoch: 19 step: 38, loss is 1.2529656887054443\n",
      "epoch: 19 step: 39, loss is 1.1876275539398193\n",
      "epoch: 19 step: 40, loss is 1.1815646886825562\n",
      "epoch: 19 step: 41, loss is 1.4026751518249512\n",
      "epoch: 19 step: 42, loss is 1.3854331970214844\n",
      "epoch: 19 step: 43, loss is 1.3248919248580933\n",
      "epoch: 19 step: 44, loss is 1.1951920986175537\n",
      "epoch: 19 step: 45, loss is 1.1990950107574463\n",
      "epoch: 19 step: 46, loss is 1.1397156715393066\n",
      "epoch: 19 step: 47, loss is 1.2491933107376099\n",
      "epoch: 19 step: 48, loss is 1.3431791067123413\n",
      "epoch: 19 step: 49, loss is 1.2624794244766235\n",
      "epoch: 19 step: 50, loss is 1.1634695529937744\n",
      "epoch: 19 step: 51, loss is 1.114998698234558\n",
      "epoch: 19 step: 52, loss is 1.3217648267745972\n",
      "epoch: 19 step: 53, loss is 1.2382584810256958\n",
      "epoch: 19 step: 54, loss is 1.3006823062896729\n",
      "epoch: 19 step: 55, loss is 1.2783434391021729\n",
      "epoch: 19 step: 56, loss is 1.2888381481170654\n",
      "epoch: 19 step: 57, loss is 1.290712833404541\n",
      "epoch: 19 step: 58, loss is 1.3719426393508911\n",
      "epoch: 19 step: 59, loss is 1.325284481048584\n",
      "epoch: 19 step: 60, loss is 1.3332334756851196\n",
      "epoch: 19 step: 61, loss is 1.249886155128479\n",
      "epoch: 19 step: 62, loss is 1.2357901334762573\n",
      "epoch: 19 step: 63, loss is 1.2468678951263428\n",
      "epoch: 19 step: 64, loss is 1.3507694005966187\n",
      "epoch: 19 step: 65, loss is 1.3077702522277832\n",
      "epoch: 19 step: 66, loss is 1.277274250984192\n",
      "epoch: 19 step: 67, loss is 1.2289921045303345\n",
      "epoch: 19 step: 68, loss is 1.2602825164794922\n",
      "epoch: 19 step: 69, loss is 1.2677255868911743\n",
      "epoch: 19 step: 70, loss is 1.231101393699646\n",
      "epoch: 19 step: 71, loss is 1.323051929473877\n",
      "epoch: 19 step: 72, loss is 1.2871673107147217\n",
      "epoch: 19 step: 73, loss is 1.189720869064331\n",
      "epoch: 19 step: 74, loss is 1.4338269233703613\n",
      "epoch: 19 step: 75, loss is 1.2684552669525146\n",
      "epoch: 19 step: 76, loss is 1.3301386833190918\n",
      "epoch: 19 step: 77, loss is 1.2762985229492188\n",
      "epoch: 19 step: 78, loss is 1.304665207862854\n",
      "epoch: 19 step: 79, loss is 1.1853647232055664\n",
      "epoch: 19 step: 80, loss is 1.1930651664733887\n",
      "epoch: 19 step: 81, loss is 1.2449712753295898\n",
      "epoch: 19 step: 82, loss is 1.2371337413787842\n",
      "epoch: 19 step: 83, loss is 1.266878604888916\n",
      "epoch: 19 step: 84, loss is 1.2930662631988525\n",
      "epoch: 19 step: 85, loss is 1.1808568239212036\n",
      "epoch: 19 step: 86, loss is 1.2872169017791748\n",
      "epoch: 19 step: 87, loss is 1.3326871395111084\n",
      "epoch: 19 step: 88, loss is 1.2389378547668457\n",
      "epoch: 19 step: 89, loss is 1.1439483165740967\n",
      "epoch: 19 step: 90, loss is 1.2356855869293213\n",
      "epoch: 19 step: 91, loss is 1.2478965520858765\n",
      "epoch: 19 step: 92, loss is 1.1989901065826416\n",
      "epoch: 19 step: 93, loss is 1.3758251667022705\n",
      "epoch: 19 step: 94, loss is 1.2634074687957764\n",
      "epoch: 19 step: 95, loss is 1.3823597431182861\n",
      "epoch: 19 step: 96, loss is 1.2586528062820435\n",
      "epoch: 19 step: 97, loss is 1.230586051940918\n",
      "epoch: 19 step: 98, loss is 1.30308198928833\n",
      "epoch: 19 step: 99, loss is 1.2832896709442139\n",
      "epoch: 19 step: 100, loss is 1.2451674938201904\n",
      "epoch: 19 step: 101, loss is 1.140499472618103\n",
      "epoch: 19 step: 102, loss is 1.2704999446868896\n",
      "epoch: 19 step: 103, loss is 1.2931233644485474\n",
      "epoch: 19 step: 104, loss is 1.159803867340088\n",
      "epoch: 19 step: 105, loss is 1.2276933193206787\n",
      "epoch: 19 step: 106, loss is 1.2992204427719116\n",
      "epoch: 19 step: 107, loss is 1.166528344154358\n",
      "epoch: 19 step: 108, loss is 1.2340764999389648\n",
      "epoch: 19 step: 109, loss is 1.2384064197540283\n",
      "epoch: 19 step: 110, loss is 1.2302297353744507\n",
      "epoch: 19 step: 111, loss is 1.3426765203475952\n",
      "epoch: 19 step: 112, loss is 1.3000624179840088\n",
      "epoch: 19 step: 113, loss is 1.3379542827606201\n",
      "epoch: 19 step: 114, loss is 1.271637201309204\n",
      "epoch: 19 step: 115, loss is 1.1457175016403198\n",
      "epoch: 19 step: 116, loss is 1.2521064281463623\n",
      "epoch: 19 step: 117, loss is 1.2357683181762695\n",
      "epoch: 19 step: 118, loss is 1.294014811515808\n",
      "epoch: 19 step: 119, loss is 1.2499679327011108\n",
      "epoch: 19 step: 120, loss is 1.2780169248580933\n",
      "epoch: 19 step: 121, loss is 1.2275532484054565\n",
      "epoch: 19 step: 122, loss is 1.086137056350708\n",
      "epoch: 19 step: 123, loss is 1.3581304550170898\n",
      "epoch: 19 step: 124, loss is 1.2526664733886719\n",
      "epoch: 19 step: 125, loss is 1.1136568784713745\n",
      "epoch: 19 step: 126, loss is 1.130285382270813\n",
      "epoch: 19 step: 127, loss is 1.3023267984390259\n",
      "epoch: 19 step: 128, loss is 1.249549150466919\n",
      "epoch: 19 step: 129, loss is 1.0480942726135254\n",
      "epoch: 19 step: 130, loss is 1.1672507524490356\n",
      "epoch: 19 step: 131, loss is 1.2579829692840576\n",
      "epoch: 19 step: 132, loss is 1.2131481170654297\n",
      "epoch: 19 step: 133, loss is 1.337471842765808\n",
      "epoch: 19 step: 134, loss is 1.346435546875\n",
      "epoch: 19 step: 135, loss is 1.2786890268325806\n",
      "epoch: 19 step: 136, loss is 1.1515913009643555\n",
      "epoch: 19 step: 137, loss is 1.296823501586914\n",
      "epoch: 19 step: 138, loss is 1.233733057975769\n",
      "epoch: 19 step: 139, loss is 1.1701099872589111\n",
      "epoch: 19 step: 140, loss is 1.307328224182129\n",
      "epoch: 19 step: 141, loss is 1.227055311203003\n",
      "epoch: 19 step: 142, loss is 1.330544352531433\n",
      "epoch: 19 step: 143, loss is 1.3856385946273804\n",
      "epoch: 19 step: 144, loss is 1.1970326900482178\n",
      "epoch: 19 step: 145, loss is 1.0469098091125488\n",
      "epoch: 19 step: 146, loss is 1.146632194519043\n",
      "epoch: 19 step: 147, loss is 1.3608322143554688\n",
      "epoch: 19 step: 148, loss is 1.198267936706543\n",
      "epoch: 19 step: 149, loss is 1.0918573141098022\n",
      "epoch: 19 step: 150, loss is 1.15444815158844\n",
      "epoch: 19 step: 151, loss is 1.3355088233947754\n",
      "epoch: 19 step: 152, loss is 1.19523024559021\n",
      "epoch: 19 step: 153, loss is 1.4090862274169922\n",
      "epoch: 19 step: 154, loss is 1.374210238456726\n",
      "epoch: 19 step: 155, loss is 1.1676076650619507\n",
      "epoch: 19 step: 156, loss is 1.2412644624710083\n",
      "epoch: 19 step: 157, loss is 1.335054874420166\n",
      "epoch: 19 step: 158, loss is 1.1749980449676514\n",
      "epoch: 19 step: 159, loss is 1.1486632823944092\n",
      "epoch: 19 step: 160, loss is 1.284940481185913\n",
      "epoch: 19 step: 161, loss is 1.2563796043395996\n",
      "epoch: 19 step: 162, loss is 1.2258206605911255\n",
      "epoch: 19 step: 163, loss is 1.3069164752960205\n",
      "epoch: 19 step: 164, loss is 1.2125461101531982\n",
      "epoch: 19 step: 165, loss is 1.2909409999847412\n",
      "epoch: 19 step: 166, loss is 1.3447988033294678\n",
      "epoch: 19 step: 167, loss is 1.1570183038711548\n",
      "epoch: 19 step: 168, loss is 1.2817645072937012\n",
      "epoch: 19 step: 169, loss is 1.3139322996139526\n",
      "epoch: 19 step: 170, loss is 1.2076959609985352\n",
      "epoch: 19 step: 171, loss is 1.313258409500122\n",
      "epoch: 19 step: 172, loss is 1.2852559089660645\n",
      "epoch: 19 step: 173, loss is 1.1222028732299805\n",
      "epoch: 19 step: 174, loss is 1.1729739904403687\n",
      "epoch: 19 step: 175, loss is 1.279350996017456\n",
      "epoch: 19 step: 176, loss is 1.2186200618743896\n",
      "epoch: 19 step: 177, loss is 1.257918119430542\n",
      "epoch: 19 step: 178, loss is 1.269060492515564\n",
      "epoch: 19 step: 179, loss is 1.1462465524673462\n",
      "epoch: 19 step: 180, loss is 1.2326247692108154\n",
      "epoch: 19 step: 181, loss is 1.2971532344818115\n",
      "epoch: 19 step: 182, loss is 1.2188827991485596\n",
      "epoch: 19 step: 183, loss is 1.0655921697616577\n",
      "epoch: 19 step: 184, loss is 1.1711533069610596\n",
      "epoch: 19 step: 185, loss is 1.1846405267715454\n",
      "epoch: 19 step: 186, loss is 1.1888545751571655\n",
      "epoch: 19 step: 187, loss is 1.1622838973999023\n",
      "epoch: 19 step: 188, loss is 1.1736618280410767\n",
      "epoch: 19 step: 189, loss is 1.3097220659255981\n",
      "epoch: 19 step: 190, loss is 1.1272637844085693\n",
      "epoch: 19 step: 191, loss is 1.256526231765747\n",
      "epoch: 19 step: 192, loss is 1.1652913093566895\n",
      "epoch: 19 step: 193, loss is 1.1278657913208008\n",
      "epoch: 19 step: 194, loss is 1.3245590925216675\n",
      "epoch: 19 step: 195, loss is 1.2749693393707275\n",
      "epoch: 19 step: 196, loss is 1.1539710760116577\n",
      "epoch: 19 step: 197, loss is 1.3209466934204102\n",
      "epoch: 19 step: 198, loss is 1.1826634407043457\n",
      "epoch: 19 step: 199, loss is 1.2431573867797852\n",
      "epoch: 19 step: 200, loss is 1.2964141368865967\n",
      "epoch: 19 step: 201, loss is 1.19877028465271\n",
      "epoch: 19 step: 202, loss is 1.1318930387496948\n",
      "epoch: 19 step: 203, loss is 1.4447407722473145\n",
      "epoch: 19 step: 204, loss is 1.29729163646698\n",
      "epoch: 19 step: 205, loss is 1.1488302946090698\n",
      "epoch: 19 step: 206, loss is 1.2782615423202515\n",
      "epoch: 19 step: 207, loss is 1.2408299446105957\n",
      "epoch: 19 step: 208, loss is 1.180931568145752\n",
      "epoch: 19 step: 209, loss is 1.383570671081543\n",
      "epoch: 19 step: 210, loss is 1.1607271432876587\n",
      "epoch: 19 step: 211, loss is 1.1919664144515991\n",
      "epoch: 19 step: 212, loss is 1.2791510820388794\n",
      "epoch: 19 step: 213, loss is 1.1901907920837402\n",
      "epoch: 19 step: 214, loss is 1.250763177871704\n",
      "epoch: 19 step: 215, loss is 1.171726942062378\n",
      "epoch: 19 step: 216, loss is 1.1978760957717896\n",
      "epoch: 19 step: 217, loss is 1.3478834629058838\n",
      "epoch: 19 step: 218, loss is 1.3576148748397827\n",
      "epoch: 19 step: 219, loss is 1.2893004417419434\n",
      "epoch: 19 step: 220, loss is 1.3532969951629639\n",
      "epoch: 19 step: 221, loss is 1.3548228740692139\n",
      "epoch: 19 step: 222, loss is 1.2772302627563477\n",
      "epoch: 19 step: 223, loss is 1.3402087688446045\n",
      "epoch: 19 step: 224, loss is 1.2301369905471802\n",
      "epoch: 19 step: 225, loss is 1.275805950164795\n",
      "epoch: 19 step: 226, loss is 1.325966477394104\n",
      "epoch: 19 step: 227, loss is 1.1454639434814453\n",
      "epoch: 19 step: 228, loss is 1.2684738636016846\n",
      "epoch: 19 step: 229, loss is 1.3122855424880981\n",
      "epoch: 19 step: 230, loss is 1.2422311305999756\n",
      "epoch: 19 step: 231, loss is 1.2703922986984253\n",
      "epoch: 19 step: 232, loss is 1.29654860496521\n",
      "epoch: 19 step: 233, loss is 1.2180168628692627\n",
      "epoch: 19 step: 234, loss is 1.090483546257019\n",
      "epoch: 19 step: 235, loss is 1.311023235321045\n",
      "epoch: 19 step: 236, loss is 1.148746371269226\n",
      "epoch: 19 step: 237, loss is 1.257711410522461\n",
      "epoch: 19 step: 238, loss is 1.2451852560043335\n",
      "epoch: 19 step: 239, loss is 1.2435365915298462\n",
      "epoch: 19 step: 240, loss is 1.1491018533706665\n",
      "epoch: 19 step: 241, loss is 1.3113512992858887\n",
      "epoch: 19 step: 242, loss is 1.4058253765106201\n",
      "epoch: 19 step: 243, loss is 1.275842547416687\n",
      "epoch: 19 step: 244, loss is 1.1640870571136475\n",
      "epoch: 19 step: 245, loss is 1.1400872468948364\n",
      "epoch: 19 step: 246, loss is 1.216071367263794\n",
      "epoch: 19 step: 247, loss is 1.2706536054611206\n",
      "epoch: 19 step: 248, loss is 1.274778127670288\n",
      "epoch: 19 step: 249, loss is 1.2176203727722168\n",
      "epoch: 19 step: 250, loss is 1.1978780031204224\n",
      "epoch: 19 step: 251, loss is 1.221265196800232\n",
      "epoch: 19 step: 252, loss is 1.2830712795257568\n",
      "epoch: 19 step: 253, loss is 1.1629939079284668\n",
      "epoch: 19 step: 254, loss is 1.22537100315094\n",
      "epoch: 19 step: 255, loss is 1.2220875024795532\n",
      "epoch: 19 step: 256, loss is 1.2681543827056885\n",
      "epoch: 19 step: 257, loss is 1.198846459388733\n",
      "epoch: 19 step: 258, loss is 1.1247644424438477\n",
      "epoch: 19 step: 259, loss is 1.2101410627365112\n",
      "epoch: 19 step: 260, loss is 1.3712717294692993\n",
      "epoch: 19 step: 261, loss is 1.1964203119277954\n",
      "epoch: 19 step: 262, loss is 1.2123273611068726\n",
      "epoch: 19 step: 263, loss is 1.4231802225112915\n",
      "epoch: 19 step: 264, loss is 1.25104558467865\n",
      "epoch: 19 step: 265, loss is 1.2173962593078613\n",
      "epoch: 19 step: 266, loss is 1.2826526165008545\n",
      "epoch: 19 step: 267, loss is 1.1273561716079712\n",
      "epoch: 19 step: 268, loss is 1.2797179222106934\n",
      "epoch: 19 step: 269, loss is 1.1418102979660034\n",
      "epoch: 19 step: 270, loss is 1.3650009632110596\n",
      "epoch: 19 step: 271, loss is 1.2477885484695435\n",
      "epoch: 19 step: 272, loss is 1.2550933361053467\n",
      "epoch: 19 step: 273, loss is 1.252050518989563\n",
      "epoch: 19 step: 274, loss is 1.2377417087554932\n",
      "epoch: 19 step: 275, loss is 1.1867297887802124\n",
      "epoch: 19 step: 276, loss is 1.27466881275177\n",
      "epoch: 19 step: 277, loss is 1.2309889793395996\n",
      "epoch: 19 step: 278, loss is 1.1884195804595947\n",
      "epoch: 19 step: 279, loss is 1.2431600093841553\n",
      "epoch: 19 step: 280, loss is 1.206491231918335\n",
      "epoch: 19 step: 281, loss is 1.1592214107513428\n",
      "epoch: 19 step: 282, loss is 1.200244665145874\n",
      "epoch: 19 step: 283, loss is 1.1693764925003052\n",
      "epoch: 19 step: 284, loss is 1.3159308433532715\n",
      "epoch: 19 step: 285, loss is 1.2791436910629272\n",
      "epoch: 19 step: 286, loss is 1.301742434501648\n",
      "epoch: 19 step: 287, loss is 1.3527685403823853\n",
      "epoch: 19 step: 288, loss is 1.3076132535934448\n",
      "epoch: 19 step: 289, loss is 1.2098515033721924\n",
      "epoch: 19 step: 290, loss is 1.2107572555541992\n",
      "epoch: 19 step: 291, loss is 1.286595106124878\n",
      "epoch: 19 step: 292, loss is 1.2192758321762085\n",
      "epoch: 19 step: 293, loss is 1.2207541465759277\n",
      "epoch: 19 step: 294, loss is 1.2715137004852295\n",
      "epoch: 19 step: 295, loss is 1.372401237487793\n",
      "epoch: 19 step: 296, loss is 1.262516975402832\n",
      "epoch: 19 step: 297, loss is 1.2971665859222412\n",
      "epoch: 19 step: 298, loss is 1.2809861898422241\n",
      "epoch: 19 step: 299, loss is 1.1765782833099365\n",
      "epoch: 19 step: 300, loss is 1.2084715366363525\n",
      "epoch: 19 step: 301, loss is 1.2790005207061768\n",
      "epoch: 19 step: 302, loss is 1.3938932418823242\n",
      "epoch: 19 step: 303, loss is 1.2817955017089844\n",
      "epoch: 19 step: 304, loss is 1.2991702556610107\n",
      "epoch: 19 step: 305, loss is 1.4025146961212158\n",
      "epoch: 19 step: 306, loss is 1.2454867362976074\n",
      "epoch: 19 step: 307, loss is 1.1752642393112183\n",
      "epoch: 19 step: 308, loss is 1.257241129875183\n",
      "epoch: 19 step: 309, loss is 1.2472178936004639\n",
      "epoch: 19 step: 310, loss is 1.3898632526397705\n",
      "epoch: 19 step: 311, loss is 1.2765672206878662\n",
      "epoch: 19 step: 312, loss is 1.2380094528198242\n",
      "epoch: 19 step: 313, loss is 1.2423027753829956\n",
      "epoch: 19 step: 314, loss is 1.1590207815170288\n",
      "epoch: 19 step: 315, loss is 1.3271429538726807\n",
      "epoch: 19 step: 316, loss is 1.2320200204849243\n",
      "epoch: 19 step: 317, loss is 1.28823983669281\n",
      "epoch: 19 step: 318, loss is 1.2691816091537476\n",
      "epoch: 19 step: 319, loss is 1.258131504058838\n",
      "epoch: 19 step: 320, loss is 1.2743427753448486\n",
      "epoch: 19 step: 321, loss is 1.3876168727874756\n",
      "epoch: 19 step: 322, loss is 1.2812578678131104\n",
      "epoch: 19 step: 323, loss is 1.2470871210098267\n",
      "epoch: 19 step: 324, loss is 1.2716410160064697\n",
      "epoch: 19 step: 325, loss is 1.3297210931777954\n",
      "epoch: 19 step: 326, loss is 1.2494899034500122\n",
      "epoch: 19 step: 327, loss is 1.269239902496338\n",
      "epoch: 19 step: 328, loss is 1.2345049381256104\n",
      "epoch: 19 step: 329, loss is 1.1862833499908447\n",
      "epoch: 19 step: 330, loss is 1.2486112117767334\n",
      "epoch: 19 step: 331, loss is 1.2511705160140991\n",
      "epoch: 19 step: 332, loss is 1.2749948501586914\n",
      "epoch: 19 step: 333, loss is 1.286163330078125\n",
      "epoch: 19 step: 334, loss is 1.2380597591400146\n",
      "epoch: 19 step: 335, loss is 1.2500145435333252\n",
      "epoch: 19 step: 336, loss is 1.1656582355499268\n",
      "epoch: 19 step: 337, loss is 1.124942421913147\n",
      "epoch: 19 step: 338, loss is 1.296471118927002\n",
      "epoch: 19 step: 339, loss is 1.241979718208313\n",
      "epoch: 19 step: 340, loss is 1.1837997436523438\n",
      "epoch: 19 step: 341, loss is 1.1690869331359863\n",
      "epoch: 19 step: 342, loss is 1.326719880104065\n",
      "epoch: 19 step: 343, loss is 1.2496376037597656\n",
      "epoch: 19 step: 344, loss is 1.2871484756469727\n",
      "epoch: 19 step: 345, loss is 1.267042636871338\n",
      "epoch: 19 step: 346, loss is 1.3019424676895142\n",
      "epoch: 19 step: 347, loss is 1.3454205989837646\n",
      "epoch: 19 step: 348, loss is 1.2614572048187256\n",
      "epoch: 19 step: 349, loss is 1.133711338043213\n",
      "epoch: 19 step: 350, loss is 1.3164207935333252\n",
      "epoch: 19 step: 351, loss is 1.2623231410980225\n",
      "epoch: 19 step: 352, loss is 1.2589396238327026\n",
      "epoch: 19 step: 353, loss is 1.2407844066619873\n",
      "epoch: 19 step: 354, loss is 1.2476599216461182\n",
      "epoch: 19 step: 355, loss is 1.1982697248458862\n",
      "epoch: 19 step: 356, loss is 1.1899397373199463\n",
      "epoch: 19 step: 357, loss is 1.2041347026824951\n",
      "epoch: 19 step: 358, loss is 1.3240282535552979\n",
      "epoch: 19 step: 359, loss is 1.1814793348312378\n",
      "epoch: 19 step: 360, loss is 1.2126513719558716\n",
      "epoch: 19 step: 361, loss is 1.2833902835845947\n",
      "epoch: 19 step: 362, loss is 1.2316713333129883\n",
      "epoch: 19 step: 363, loss is 1.179283857345581\n",
      "epoch: 19 step: 364, loss is 1.2410740852355957\n",
      "epoch: 19 step: 365, loss is 1.1672874689102173\n",
      "epoch: 19 step: 366, loss is 1.2991812229156494\n",
      "epoch: 19 step: 367, loss is 1.3246769905090332\n",
      "epoch: 19 step: 368, loss is 1.1122252941131592\n",
      "epoch: 19 step: 369, loss is 1.238835334777832\n",
      "epoch: 19 step: 370, loss is 1.2160773277282715\n",
      "epoch: 19 step: 371, loss is 1.18985116481781\n",
      "epoch: 19 step: 372, loss is 1.2138497829437256\n",
      "epoch: 19 step: 373, loss is 1.2084240913391113\n",
      "epoch: 19 step: 374, loss is 1.255854845046997\n",
      "epoch: 19 step: 375, loss is 1.3282759189605713\n",
      "epoch: 19 step: 376, loss is 1.238401174545288\n",
      "epoch: 19 step: 377, loss is 1.3513778448104858\n",
      "epoch: 19 step: 378, loss is 1.2531793117523193\n",
      "epoch: 19 step: 379, loss is 1.374362587928772\n",
      "epoch: 19 step: 380, loss is 1.2505404949188232\n",
      "epoch: 19 step: 381, loss is 1.1615098714828491\n",
      "epoch: 19 step: 382, loss is 1.3335922956466675\n",
      "epoch: 19 step: 383, loss is 1.2069859504699707\n",
      "epoch: 19 step: 384, loss is 1.2778081893920898\n",
      "epoch: 19 step: 385, loss is 1.2768895626068115\n",
      "epoch: 19 step: 386, loss is 1.0350100994110107\n",
      "epoch: 19 step: 387, loss is 1.3078234195709229\n",
      "epoch: 19 step: 388, loss is 1.2603235244750977\n",
      "epoch: 19 step: 389, loss is 1.2149488925933838\n",
      "epoch: 19 step: 390, loss is 1.267446517944336\n",
      "Train epoch time: 154990.999 ms, per step time: 397.413 ms\n",
      "epoch: 20 step: 1, loss is 1.197556972503662\n",
      "epoch: 20 step: 2, loss is 1.389183759689331\n",
      "epoch: 20 step: 3, loss is 1.166422724723816\n",
      "epoch: 20 step: 4, loss is 1.2699060440063477\n",
      "epoch: 20 step: 5, loss is 1.1450998783111572\n",
      "epoch: 20 step: 6, loss is 1.239898681640625\n",
      "epoch: 20 step: 7, loss is 1.185147762298584\n",
      "epoch: 20 step: 8, loss is 1.2297102212905884\n",
      "epoch: 20 step: 9, loss is 1.4370856285095215\n",
      "epoch: 20 step: 10, loss is 1.2735464572906494\n",
      "epoch: 20 step: 11, loss is 1.2306537628173828\n",
      "epoch: 20 step: 12, loss is 1.1941375732421875\n",
      "epoch: 20 step: 13, loss is 1.2113935947418213\n",
      "epoch: 20 step: 14, loss is 1.1178401708602905\n",
      "epoch: 20 step: 15, loss is 1.2034715414047241\n",
      "epoch: 20 step: 16, loss is 1.2487064599990845\n",
      "epoch: 20 step: 17, loss is 1.1000794172286987\n",
      "epoch: 20 step: 18, loss is 1.2652137279510498\n",
      "epoch: 20 step: 19, loss is 1.3090734481811523\n",
      "epoch: 20 step: 20, loss is 1.3001289367675781\n",
      "epoch: 20 step: 21, loss is 1.2733631134033203\n",
      "epoch: 20 step: 22, loss is 1.1629890203475952\n",
      "epoch: 20 step: 23, loss is 1.172607660293579\n",
      "epoch: 20 step: 24, loss is 1.3644932508468628\n",
      "epoch: 20 step: 25, loss is 1.1768889427185059\n",
      "epoch: 20 step: 26, loss is 1.3341679573059082\n",
      "epoch: 20 step: 27, loss is 1.321631908416748\n",
      "epoch: 20 step: 28, loss is 1.2755260467529297\n",
      "epoch: 20 step: 29, loss is 1.3136781454086304\n",
      "epoch: 20 step: 30, loss is 1.3157976865768433\n",
      "epoch: 20 step: 31, loss is 1.2205078601837158\n",
      "epoch: 20 step: 32, loss is 1.2891911268234253\n",
      "epoch: 20 step: 33, loss is 1.2469595670700073\n",
      "epoch: 20 step: 34, loss is 1.1952838897705078\n",
      "epoch: 20 step: 35, loss is 1.138488531112671\n",
      "epoch: 20 step: 36, loss is 1.214637279510498\n",
      "epoch: 20 step: 37, loss is 1.162651777267456\n",
      "epoch: 20 step: 38, loss is 1.226750135421753\n",
      "epoch: 20 step: 39, loss is 1.2463994026184082\n",
      "epoch: 20 step: 40, loss is 1.1797946691513062\n",
      "epoch: 20 step: 41, loss is 1.1017000675201416\n",
      "epoch: 20 step: 42, loss is 1.2615044116973877\n",
      "epoch: 20 step: 43, loss is 1.0984840393066406\n",
      "epoch: 20 step: 44, loss is 1.2849140167236328\n",
      "epoch: 20 step: 45, loss is 1.190547227859497\n",
      "epoch: 20 step: 46, loss is 1.235239028930664\n",
      "epoch: 20 step: 47, loss is 1.1865785121917725\n",
      "epoch: 20 step: 48, loss is 1.24590003490448\n",
      "epoch: 20 step: 49, loss is 1.185233473777771\n",
      "epoch: 20 step: 50, loss is 1.1646113395690918\n",
      "epoch: 20 step: 51, loss is 1.221154808998108\n",
      "epoch: 20 step: 52, loss is 1.2303260564804077\n",
      "epoch: 20 step: 53, loss is 1.2390905618667603\n",
      "epoch: 20 step: 54, loss is 1.1528395414352417\n",
      "epoch: 20 step: 55, loss is 1.2375341653823853\n",
      "epoch: 20 step: 56, loss is 1.1628379821777344\n",
      "epoch: 20 step: 57, loss is 1.1410186290740967\n",
      "epoch: 20 step: 58, loss is 1.224947452545166\n",
      "epoch: 20 step: 59, loss is 1.240738034248352\n",
      "epoch: 20 step: 60, loss is 1.1113231182098389\n",
      "epoch: 20 step: 61, loss is 1.3667035102844238\n",
      "epoch: 20 step: 62, loss is 1.252026915550232\n",
      "epoch: 20 step: 63, loss is 1.2766807079315186\n",
      "epoch: 20 step: 64, loss is 1.269879937171936\n",
      "epoch: 20 step: 65, loss is 1.3319385051727295\n",
      "epoch: 20 step: 66, loss is 1.290205955505371\n",
      "epoch: 20 step: 67, loss is 1.1426464319229126\n",
      "epoch: 20 step: 68, loss is 1.3478502035140991\n",
      "epoch: 20 step: 69, loss is 1.1110044717788696\n",
      "epoch: 20 step: 70, loss is 1.2997112274169922\n",
      "epoch: 20 step: 71, loss is 1.226818561553955\n",
      "epoch: 20 step: 72, loss is 1.2558202743530273\n",
      "epoch: 20 step: 73, loss is 1.1865713596343994\n",
      "epoch: 20 step: 74, loss is 1.220354676246643\n",
      "epoch: 20 step: 75, loss is 1.2220889329910278\n",
      "epoch: 20 step: 76, loss is 1.278118371963501\n",
      "epoch: 20 step: 77, loss is 1.3038808107376099\n",
      "epoch: 20 step: 78, loss is 1.1737768650054932\n",
      "epoch: 20 step: 79, loss is 1.200420618057251\n",
      "epoch: 20 step: 80, loss is 1.1950438022613525\n",
      "epoch: 20 step: 81, loss is 1.3326632976531982\n",
      "epoch: 20 step: 82, loss is 1.1160014867782593\n",
      "epoch: 20 step: 83, loss is 1.188245415687561\n",
      "epoch: 20 step: 84, loss is 1.3270821571350098\n",
      "epoch: 20 step: 85, loss is 1.2818032503128052\n",
      "epoch: 20 step: 86, loss is 1.1121509075164795\n",
      "epoch: 20 step: 87, loss is 1.2520675659179688\n",
      "epoch: 20 step: 88, loss is 1.1826547384262085\n",
      "epoch: 20 step: 89, loss is 1.171878457069397\n",
      "epoch: 20 step: 90, loss is 1.2113234996795654\n",
      "epoch: 20 step: 91, loss is 1.0996912717819214\n",
      "epoch: 20 step: 92, loss is 1.2579352855682373\n",
      "epoch: 20 step: 93, loss is 1.2368903160095215\n",
      "epoch: 20 step: 94, loss is 1.0853163003921509\n",
      "epoch: 20 step: 95, loss is 1.247185468673706\n",
      "epoch: 20 step: 96, loss is 1.1993343830108643\n",
      "epoch: 20 step: 97, loss is 1.1546097993850708\n",
      "epoch: 20 step: 98, loss is 1.2260456085205078\n",
      "epoch: 20 step: 99, loss is 1.2948459386825562\n",
      "epoch: 20 step: 100, loss is 1.2775516510009766\n",
      "epoch: 20 step: 101, loss is 1.3167275190353394\n",
      "epoch: 20 step: 102, loss is 1.1921861171722412\n",
      "epoch: 20 step: 103, loss is 1.1850922107696533\n",
      "epoch: 20 step: 104, loss is 1.3176419734954834\n",
      "epoch: 20 step: 105, loss is 1.2637492418289185\n",
      "epoch: 20 step: 106, loss is 1.3051173686981201\n",
      "epoch: 20 step: 107, loss is 1.2036219835281372\n",
      "epoch: 20 step: 108, loss is 1.3243831396102905\n",
      "epoch: 20 step: 109, loss is 1.294742465019226\n",
      "epoch: 20 step: 110, loss is 1.207282304763794\n",
      "epoch: 20 step: 111, loss is 1.2533514499664307\n",
      "epoch: 20 step: 112, loss is 1.287611722946167\n",
      "epoch: 20 step: 113, loss is 1.226453423500061\n",
      "epoch: 20 step: 114, loss is 1.1815662384033203\n",
      "epoch: 20 step: 115, loss is 1.3019702434539795\n",
      "epoch: 20 step: 116, loss is 1.2419085502624512\n",
      "epoch: 20 step: 117, loss is 1.1965887546539307\n",
      "epoch: 20 step: 118, loss is 1.2047531604766846\n",
      "epoch: 20 step: 119, loss is 1.3486053943634033\n",
      "epoch: 20 step: 120, loss is 1.2897441387176514\n",
      "epoch: 20 step: 121, loss is 1.251727819442749\n",
      "epoch: 20 step: 122, loss is 1.1260316371917725\n",
      "epoch: 20 step: 123, loss is 1.1936216354370117\n",
      "epoch: 20 step: 124, loss is 1.2134320735931396\n",
      "epoch: 20 step: 125, loss is 1.2591583728790283\n",
      "epoch: 20 step: 126, loss is 1.216981291770935\n",
      "epoch: 20 step: 127, loss is 1.2930017709732056\n",
      "epoch: 20 step: 128, loss is 1.2371585369110107\n",
      "epoch: 20 step: 129, loss is 1.2798019647598267\n",
      "epoch: 20 step: 130, loss is 1.1591821908950806\n",
      "epoch: 20 step: 131, loss is 1.1191426515579224\n",
      "epoch: 20 step: 132, loss is 1.203589916229248\n",
      "epoch: 20 step: 133, loss is 1.2936183214187622\n",
      "epoch: 20 step: 134, loss is 1.2348637580871582\n",
      "epoch: 20 step: 135, loss is 1.2420222759246826\n",
      "epoch: 20 step: 136, loss is 1.2688586711883545\n",
      "epoch: 20 step: 137, loss is 1.318023920059204\n",
      "epoch: 20 step: 138, loss is 1.2086608409881592\n",
      "epoch: 20 step: 139, loss is 1.2972315549850464\n",
      "epoch: 20 step: 140, loss is 1.224470615386963\n",
      "epoch: 20 step: 141, loss is 1.2636746168136597\n",
      "epoch: 20 step: 142, loss is 1.165405035018921\n",
      "epoch: 20 step: 143, loss is 1.3150711059570312\n",
      "epoch: 20 step: 144, loss is 1.2137681245803833\n",
      "epoch: 20 step: 145, loss is 1.1571409702301025\n",
      "epoch: 20 step: 146, loss is 1.1180195808410645\n",
      "epoch: 20 step: 147, loss is 1.2829701900482178\n",
      "epoch: 20 step: 148, loss is 1.2578635215759277\n",
      "epoch: 20 step: 149, loss is 1.2186647653579712\n",
      "epoch: 20 step: 150, loss is 1.1533688306808472\n",
      "epoch: 20 step: 151, loss is 1.1924257278442383\n",
      "epoch: 20 step: 152, loss is 1.2960214614868164\n",
      "epoch: 20 step: 153, loss is 1.2640340328216553\n",
      "epoch: 20 step: 154, loss is 1.2766039371490479\n",
      "epoch: 20 step: 155, loss is 1.340808629989624\n",
      "epoch: 20 step: 156, loss is 1.1450920104980469\n",
      "epoch: 20 step: 157, loss is 1.2307569980621338\n",
      "epoch: 20 step: 158, loss is 1.1663376092910767\n",
      "epoch: 20 step: 159, loss is 1.2722055912017822\n",
      "epoch: 20 step: 160, loss is 1.3669984340667725\n",
      "epoch: 20 step: 161, loss is 1.2392840385437012\n",
      "epoch: 20 step: 162, loss is 1.304880976676941\n",
      "epoch: 20 step: 163, loss is 1.3490225076675415\n",
      "epoch: 20 step: 164, loss is 1.264035701751709\n",
      "epoch: 20 step: 165, loss is 1.224332571029663\n",
      "epoch: 20 step: 166, loss is 1.1070141792297363\n",
      "epoch: 20 step: 167, loss is 1.1798268556594849\n",
      "epoch: 20 step: 168, loss is 1.1481359004974365\n",
      "epoch: 20 step: 169, loss is 1.0855939388275146\n",
      "epoch: 20 step: 170, loss is 1.2457213401794434\n",
      "epoch: 20 step: 171, loss is 1.2207927703857422\n",
      "epoch: 20 step: 172, loss is 1.208216905593872\n",
      "epoch: 20 step: 173, loss is 1.2089239358901978\n",
      "epoch: 20 step: 174, loss is 1.2956469058990479\n",
      "epoch: 20 step: 175, loss is 1.2927305698394775\n",
      "epoch: 20 step: 176, loss is 1.2265841960906982\n",
      "epoch: 20 step: 177, loss is 1.1524945497512817\n",
      "epoch: 20 step: 178, loss is 1.161374807357788\n",
      "epoch: 20 step: 179, loss is 1.2409486770629883\n",
      "epoch: 20 step: 180, loss is 1.197866439819336\n",
      "epoch: 20 step: 181, loss is 1.183764934539795\n",
      "epoch: 20 step: 182, loss is 1.3626569509506226\n",
      "epoch: 20 step: 183, loss is 1.2829912900924683\n",
      "epoch: 20 step: 184, loss is 1.1850665807724\n",
      "epoch: 20 step: 185, loss is 1.1669025421142578\n",
      "epoch: 20 step: 186, loss is 1.3716366291046143\n",
      "epoch: 20 step: 187, loss is 1.156614899635315\n",
      "epoch: 20 step: 188, loss is 1.220583438873291\n",
      "epoch: 20 step: 189, loss is 1.2179847955703735\n",
      "epoch: 20 step: 190, loss is 1.2634044885635376\n",
      "epoch: 20 step: 191, loss is 1.2037254571914673\n",
      "epoch: 20 step: 192, loss is 1.1909422874450684\n",
      "epoch: 20 step: 193, loss is 1.1314406394958496\n",
      "epoch: 20 step: 194, loss is 1.280731439590454\n",
      "epoch: 20 step: 195, loss is 1.2459959983825684\n",
      "epoch: 20 step: 196, loss is 1.2293071746826172\n",
      "epoch: 20 step: 197, loss is 1.352856159210205\n",
      "epoch: 20 step: 198, loss is 1.2987720966339111\n",
      "epoch: 20 step: 199, loss is 1.268446922302246\n",
      "epoch: 20 step: 200, loss is 1.2943611145019531\n",
      "epoch: 20 step: 201, loss is 1.2295958995819092\n",
      "epoch: 20 step: 202, loss is 1.1987035274505615\n",
      "epoch: 20 step: 203, loss is 1.2268882989883423\n",
      "epoch: 20 step: 204, loss is 1.3312546014785767\n",
      "epoch: 20 step: 205, loss is 1.194422721862793\n",
      "epoch: 20 step: 206, loss is 1.2074064016342163\n",
      "epoch: 20 step: 207, loss is 1.2509839534759521\n",
      "epoch: 20 step: 208, loss is 1.316085696220398\n",
      "epoch: 20 step: 209, loss is 1.1716859340667725\n",
      "epoch: 20 step: 210, loss is 1.1963887214660645\n",
      "epoch: 20 step: 211, loss is 1.141488790512085\n",
      "epoch: 20 step: 212, loss is 1.2751461267471313\n",
      "epoch: 20 step: 213, loss is 1.2893162965774536\n",
      "epoch: 20 step: 214, loss is 1.3541374206542969\n",
      "epoch: 20 step: 215, loss is 1.163397192955017\n",
      "epoch: 20 step: 216, loss is 1.13347327709198\n",
      "epoch: 20 step: 217, loss is 1.2444424629211426\n",
      "epoch: 20 step: 218, loss is 1.1923720836639404\n",
      "epoch: 20 step: 219, loss is 1.2153860330581665\n",
      "epoch: 20 step: 220, loss is 1.2253296375274658\n",
      "epoch: 20 step: 221, loss is 1.294002890586853\n",
      "epoch: 20 step: 222, loss is 1.1548446416854858\n",
      "epoch: 20 step: 223, loss is 1.2537407875061035\n",
      "epoch: 20 step: 224, loss is 1.2103140354156494\n",
      "epoch: 20 step: 225, loss is 1.2454626560211182\n",
      "epoch: 20 step: 226, loss is 1.0852956771850586\n",
      "epoch: 20 step: 227, loss is 1.2291189432144165\n",
      "epoch: 20 step: 228, loss is 1.137430191040039\n",
      "epoch: 20 step: 229, loss is 1.1975185871124268\n",
      "epoch: 20 step: 230, loss is 1.2827889919281006\n",
      "epoch: 20 step: 231, loss is 1.3349640369415283\n",
      "epoch: 20 step: 232, loss is 1.3770318031311035\n",
      "epoch: 20 step: 233, loss is 1.1973485946655273\n",
      "epoch: 20 step: 234, loss is 1.3396482467651367\n",
      "epoch: 20 step: 235, loss is 1.3121848106384277\n",
      "epoch: 20 step: 236, loss is 1.1714434623718262\n",
      "epoch: 20 step: 237, loss is 1.1908199787139893\n",
      "epoch: 20 step: 238, loss is 1.2612555027008057\n",
      "epoch: 20 step: 239, loss is 1.1013786792755127\n",
      "epoch: 20 step: 240, loss is 1.3332849740982056\n",
      "epoch: 20 step: 241, loss is 1.189979076385498\n",
      "epoch: 20 step: 242, loss is 1.2249242067337036\n",
      "epoch: 20 step: 243, loss is 1.1063212156295776\n",
      "epoch: 20 step: 244, loss is 1.2314562797546387\n",
      "epoch: 20 step: 245, loss is 1.2060474157333374\n",
      "epoch: 20 step: 246, loss is 1.3422013521194458\n",
      "epoch: 20 step: 247, loss is 1.2330951690673828\n",
      "epoch: 20 step: 248, loss is 1.2953530550003052\n",
      "epoch: 20 step: 249, loss is 1.260798692703247\n",
      "epoch: 20 step: 250, loss is 1.281907558441162\n",
      "epoch: 20 step: 251, loss is 1.3707468509674072\n",
      "epoch: 20 step: 252, loss is 1.2504725456237793\n",
      "epoch: 20 step: 253, loss is 1.2398579120635986\n",
      "epoch: 20 step: 254, loss is 1.1650879383087158\n",
      "epoch: 20 step: 255, loss is 1.2216486930847168\n",
      "epoch: 20 step: 256, loss is 1.1326109170913696\n",
      "epoch: 20 step: 257, loss is 1.1536017656326294\n",
      "epoch: 20 step: 258, loss is 1.2047028541564941\n",
      "epoch: 20 step: 259, loss is 1.1769514083862305\n",
      "epoch: 20 step: 260, loss is 1.2950222492218018\n",
      "epoch: 20 step: 261, loss is 1.1869674921035767\n",
      "epoch: 20 step: 262, loss is 1.2311809062957764\n",
      "epoch: 20 step: 263, loss is 1.262503981590271\n",
      "epoch: 20 step: 264, loss is 1.2545640468597412\n",
      "epoch: 20 step: 265, loss is 1.3508014678955078\n",
      "epoch: 20 step: 266, loss is 1.190416932106018\n",
      "epoch: 20 step: 267, loss is 1.2179359197616577\n",
      "epoch: 20 step: 268, loss is 1.2878785133361816\n",
      "epoch: 20 step: 269, loss is 1.2366784811019897\n",
      "epoch: 20 step: 270, loss is 1.2893686294555664\n",
      "epoch: 20 step: 271, loss is 1.3233864307403564\n",
      "epoch: 20 step: 272, loss is 1.333669900894165\n",
      "epoch: 20 step: 273, loss is 1.3431434631347656\n",
      "epoch: 20 step: 274, loss is 1.2265419960021973\n",
      "epoch: 20 step: 275, loss is 1.2026820182800293\n",
      "epoch: 20 step: 276, loss is 1.2068098783493042\n",
      "epoch: 20 step: 277, loss is 1.2159349918365479\n",
      "epoch: 20 step: 278, loss is 1.2727667093276978\n",
      "epoch: 20 step: 279, loss is 1.353980302810669\n",
      "epoch: 20 step: 280, loss is 1.256213665008545\n",
      "epoch: 20 step: 281, loss is 1.2362723350524902\n",
      "epoch: 20 step: 282, loss is 1.2737009525299072\n",
      "epoch: 20 step: 283, loss is 1.2896981239318848\n",
      "epoch: 20 step: 284, loss is 1.222170114517212\n",
      "epoch: 20 step: 285, loss is 1.2730722427368164\n",
      "epoch: 20 step: 286, loss is 1.2753636837005615\n",
      "epoch: 20 step: 287, loss is 1.2350330352783203\n",
      "epoch: 20 step: 288, loss is 1.3407853841781616\n",
      "epoch: 20 step: 289, loss is 1.1739298105239868\n",
      "epoch: 20 step: 290, loss is 1.1391953229904175\n",
      "epoch: 20 step: 291, loss is 1.1845166683197021\n",
      "epoch: 20 step: 292, loss is 1.264927864074707\n",
      "epoch: 20 step: 293, loss is 1.1297565698623657\n",
      "epoch: 20 step: 294, loss is 1.3883031606674194\n",
      "epoch: 20 step: 295, loss is 1.2475148439407349\n",
      "epoch: 20 step: 296, loss is 1.3029162883758545\n",
      "epoch: 20 step: 297, loss is 1.172546148300171\n",
      "epoch: 20 step: 298, loss is 1.2564995288848877\n",
      "epoch: 20 step: 299, loss is 1.1711851358413696\n",
      "epoch: 20 step: 300, loss is 1.220129370689392\n",
      "epoch: 20 step: 301, loss is 1.2576204538345337\n",
      "epoch: 20 step: 302, loss is 1.2064214944839478\n",
      "epoch: 20 step: 303, loss is 1.2175047397613525\n",
      "epoch: 20 step: 304, loss is 1.154371976852417\n",
      "epoch: 20 step: 305, loss is 1.2331469058990479\n",
      "epoch: 20 step: 306, loss is 1.2691091299057007\n",
      "epoch: 20 step: 307, loss is 1.2594242095947266\n",
      "epoch: 20 step: 308, loss is 1.3016440868377686\n",
      "epoch: 20 step: 309, loss is 1.178559422492981\n",
      "epoch: 20 step: 310, loss is 1.2586344480514526\n",
      "epoch: 20 step: 311, loss is 1.231146216392517\n",
      "epoch: 20 step: 312, loss is 1.2463293075561523\n",
      "epoch: 20 step: 313, loss is 1.2080556154251099\n",
      "epoch: 20 step: 314, loss is 1.3404113054275513\n",
      "epoch: 20 step: 315, loss is 1.0988454818725586\n",
      "epoch: 20 step: 316, loss is 1.2567620277404785\n",
      "epoch: 20 step: 317, loss is 1.1916524171829224\n",
      "epoch: 20 step: 318, loss is 1.189435601234436\n",
      "epoch: 20 step: 319, loss is 1.2979004383087158\n",
      "epoch: 20 step: 320, loss is 1.3790260553359985\n",
      "epoch: 20 step: 321, loss is 1.202498197555542\n",
      "epoch: 20 step: 322, loss is 1.2249139547348022\n",
      "epoch: 20 step: 323, loss is 1.373388648033142\n",
      "epoch: 20 step: 324, loss is 1.2126511335372925\n",
      "epoch: 20 step: 325, loss is 1.246138095855713\n",
      "epoch: 20 step: 326, loss is 1.1686595678329468\n",
      "epoch: 20 step: 327, loss is 1.258760929107666\n",
      "epoch: 20 step: 328, loss is 1.191119909286499\n",
      "epoch: 20 step: 329, loss is 1.2924623489379883\n",
      "epoch: 20 step: 330, loss is 1.1824407577514648\n",
      "epoch: 20 step: 331, loss is 1.3224905729293823\n",
      "epoch: 20 step: 332, loss is 1.354811191558838\n",
      "epoch: 20 step: 333, loss is 1.2132937908172607\n",
      "epoch: 20 step: 334, loss is 1.2021316289901733\n",
      "epoch: 20 step: 335, loss is 1.2165236473083496\n",
      "epoch: 20 step: 336, loss is 1.296247959136963\n",
      "epoch: 20 step: 337, loss is 1.2110549211502075\n",
      "epoch: 20 step: 338, loss is 1.1918374300003052\n",
      "epoch: 20 step: 339, loss is 1.2736217975616455\n",
      "epoch: 20 step: 340, loss is 1.13703191280365\n",
      "epoch: 20 step: 341, loss is 1.4097265005111694\n",
      "epoch: 20 step: 342, loss is 1.202958345413208\n",
      "epoch: 20 step: 343, loss is 1.2356860637664795\n",
      "epoch: 20 step: 344, loss is 1.0204918384552002\n",
      "epoch: 20 step: 345, loss is 1.2624269723892212\n",
      "epoch: 20 step: 346, loss is 1.317805528640747\n",
      "epoch: 20 step: 347, loss is 1.2078654766082764\n",
      "epoch: 20 step: 348, loss is 1.213521122932434\n",
      "epoch: 20 step: 349, loss is 1.3085002899169922\n",
      "epoch: 20 step: 350, loss is 1.1478341817855835\n",
      "epoch: 20 step: 351, loss is 1.2284778356552124\n",
      "epoch: 20 step: 352, loss is 1.2073756456375122\n",
      "epoch: 20 step: 353, loss is 1.1922107934951782\n",
      "epoch: 20 step: 354, loss is 1.18558669090271\n",
      "epoch: 20 step: 355, loss is 1.2228925228118896\n",
      "epoch: 20 step: 356, loss is 1.2373785972595215\n",
      "epoch: 20 step: 357, loss is 1.1995676755905151\n",
      "epoch: 20 step: 358, loss is 1.2597603797912598\n",
      "epoch: 20 step: 359, loss is 1.1970210075378418\n",
      "epoch: 20 step: 360, loss is 1.0435912609100342\n",
      "epoch: 20 step: 361, loss is 1.1464519500732422\n",
      "epoch: 20 step: 362, loss is 1.169013261795044\n",
      "epoch: 20 step: 363, loss is 1.387771487236023\n",
      "epoch: 20 step: 364, loss is 1.388826608657837\n",
      "epoch: 20 step: 365, loss is 1.2592004537582397\n",
      "epoch: 20 step: 366, loss is 1.2960729598999023\n",
      "epoch: 20 step: 367, loss is 1.1470458507537842\n",
      "epoch: 20 step: 368, loss is 1.1990410089492798\n",
      "epoch: 20 step: 369, loss is 1.1522057056427002\n",
      "epoch: 20 step: 370, loss is 1.3439254760742188\n",
      "epoch: 20 step: 371, loss is 1.206378698348999\n",
      "epoch: 20 step: 372, loss is 1.1382468938827515\n",
      "epoch: 20 step: 373, loss is 1.1712162494659424\n",
      "epoch: 20 step: 374, loss is 1.2724783420562744\n",
      "epoch: 20 step: 375, loss is 1.2530593872070312\n",
      "epoch: 20 step: 376, loss is 1.243129849433899\n",
      "epoch: 20 step: 377, loss is 1.2489368915557861\n",
      "epoch: 20 step: 378, loss is 1.2367819547653198\n",
      "epoch: 20 step: 379, loss is 1.3074487447738647\n",
      "epoch: 20 step: 380, loss is 1.3524625301361084\n",
      "epoch: 20 step: 381, loss is 1.072824478149414\n",
      "epoch: 20 step: 382, loss is 1.3751952648162842\n",
      "epoch: 20 step: 383, loss is 1.277267575263977\n",
      "epoch: 20 step: 384, loss is 1.3184386491775513\n",
      "epoch: 20 step: 385, loss is 1.2498992681503296\n",
      "epoch: 20 step: 386, loss is 1.4109116792678833\n",
      "epoch: 20 step: 387, loss is 1.270542860031128\n",
      "epoch: 20 step: 388, loss is 1.312660813331604\n",
      "epoch: 20 step: 389, loss is 1.238967776298523\n",
      "epoch: 20 step: 390, loss is 1.1755846738815308\n",
      "Train epoch time: 152536.469 ms, per step time: 391.119 ms\n",
      "epoch: 21 step: 1, loss is 1.2348674535751343\n",
      "epoch: 21 step: 2, loss is 1.0952279567718506\n",
      "epoch: 21 step: 3, loss is 1.2730932235717773\n",
      "epoch: 21 step: 4, loss is 1.2922210693359375\n",
      "epoch: 21 step: 5, loss is 1.2111479043960571\n",
      "epoch: 21 step: 6, loss is 1.246249794960022\n",
      "epoch: 21 step: 7, loss is 1.1332900524139404\n",
      "epoch: 21 step: 8, loss is 1.284751057624817\n",
      "epoch: 21 step: 9, loss is 1.2269785404205322\n",
      "epoch: 21 step: 10, loss is 1.270702838897705\n",
      "epoch: 21 step: 11, loss is 1.2004224061965942\n",
      "epoch: 21 step: 12, loss is 1.2212352752685547\n",
      "epoch: 21 step: 13, loss is 1.3073492050170898\n",
      "epoch: 21 step: 14, loss is 1.2914563417434692\n",
      "epoch: 21 step: 15, loss is 1.1910967826843262\n",
      "epoch: 21 step: 16, loss is 1.078718900680542\n",
      "epoch: 21 step: 17, loss is 1.2137423753738403\n",
      "epoch: 21 step: 18, loss is 1.1556098461151123\n",
      "epoch: 21 step: 19, loss is 1.1864385604858398\n",
      "epoch: 21 step: 20, loss is 1.165450096130371\n",
      "epoch: 21 step: 21, loss is 1.3818473815917969\n",
      "epoch: 21 step: 22, loss is 1.3220829963684082\n",
      "epoch: 21 step: 23, loss is 1.236673355102539\n",
      "epoch: 21 step: 24, loss is 1.2148929834365845\n",
      "epoch: 21 step: 25, loss is 1.1255438327789307\n",
      "epoch: 21 step: 26, loss is 1.1505910158157349\n",
      "epoch: 21 step: 27, loss is 1.2568581104278564\n",
      "epoch: 21 step: 28, loss is 1.114612340927124\n",
      "epoch: 21 step: 29, loss is 1.1188454627990723\n",
      "epoch: 21 step: 30, loss is 1.246419906616211\n",
      "epoch: 21 step: 31, loss is 1.298232913017273\n",
      "epoch: 21 step: 32, loss is 1.2310080528259277\n",
      "epoch: 21 step: 33, loss is 1.2063202857971191\n",
      "epoch: 21 step: 34, loss is 1.1251440048217773\n",
      "epoch: 21 step: 35, loss is 1.2959468364715576\n",
      "epoch: 21 step: 36, loss is 1.2805039882659912\n",
      "epoch: 21 step: 37, loss is 1.247409462928772\n",
      "epoch: 21 step: 38, loss is 1.2585680484771729\n",
      "epoch: 21 step: 39, loss is 1.2349491119384766\n",
      "epoch: 21 step: 40, loss is 1.1464354991912842\n",
      "epoch: 21 step: 41, loss is 1.1644048690795898\n",
      "epoch: 21 step: 42, loss is 1.2160874605178833\n",
      "epoch: 21 step: 43, loss is 1.1854361295700073\n",
      "epoch: 21 step: 44, loss is 1.3060516119003296\n",
      "epoch: 21 step: 45, loss is 1.1907234191894531\n",
      "epoch: 21 step: 46, loss is 1.2542061805725098\n",
      "epoch: 21 step: 47, loss is 1.2187825441360474\n",
      "epoch: 21 step: 48, loss is 1.124962329864502\n",
      "epoch: 21 step: 49, loss is 1.2124483585357666\n",
      "epoch: 21 step: 50, loss is 1.3526349067687988\n",
      "epoch: 21 step: 51, loss is 1.2455432415008545\n",
      "epoch: 21 step: 52, loss is 1.2332618236541748\n",
      "epoch: 21 step: 53, loss is 1.311389684677124\n",
      "epoch: 21 step: 54, loss is 1.3095455169677734\n",
      "epoch: 21 step: 55, loss is 1.3056809902191162\n",
      "epoch: 21 step: 56, loss is 1.258806824684143\n",
      "epoch: 21 step: 57, loss is 1.2485365867614746\n",
      "epoch: 21 step: 58, loss is 1.2217332124710083\n",
      "epoch: 21 step: 59, loss is 1.2462360858917236\n",
      "epoch: 21 step: 60, loss is 1.170924186706543\n",
      "epoch: 21 step: 61, loss is 1.2848358154296875\n",
      "epoch: 21 step: 62, loss is 1.3142054080963135\n",
      "epoch: 21 step: 63, loss is 1.262374997138977\n",
      "epoch: 21 step: 64, loss is 1.1923134326934814\n",
      "epoch: 21 step: 65, loss is 1.1463724374771118\n",
      "epoch: 21 step: 66, loss is 1.1171154975891113\n",
      "epoch: 21 step: 67, loss is 1.178549885749817\n",
      "epoch: 21 step: 68, loss is 1.26799476146698\n",
      "epoch: 21 step: 69, loss is 1.2382456064224243\n",
      "epoch: 21 step: 70, loss is 1.1629129648208618\n",
      "epoch: 21 step: 71, loss is 1.378380537033081\n",
      "epoch: 21 step: 72, loss is 1.1888396739959717\n",
      "epoch: 21 step: 73, loss is 1.2273694276809692\n",
      "epoch: 21 step: 74, loss is 1.2036888599395752\n",
      "epoch: 21 step: 75, loss is 1.1018717288970947\n",
      "epoch: 21 step: 76, loss is 1.358462929725647\n",
      "epoch: 21 step: 77, loss is 1.1866027116775513\n",
      "epoch: 21 step: 78, loss is 1.3101325035095215\n",
      "epoch: 21 step: 79, loss is 1.1520743370056152\n",
      "epoch: 21 step: 80, loss is 1.2111055850982666\n",
      "epoch: 21 step: 81, loss is 1.1736831665039062\n",
      "epoch: 21 step: 82, loss is 1.0910906791687012\n",
      "epoch: 21 step: 83, loss is 1.1522328853607178\n",
      "epoch: 21 step: 84, loss is 1.2964918613433838\n",
      "epoch: 21 step: 85, loss is 1.20089590549469\n",
      "epoch: 21 step: 86, loss is 1.2227849960327148\n",
      "epoch: 21 step: 87, loss is 1.1504758596420288\n",
      "epoch: 21 step: 88, loss is 1.311632752418518\n",
      "epoch: 21 step: 89, loss is 1.28671395778656\n",
      "epoch: 21 step: 90, loss is 1.1900701522827148\n",
      "epoch: 21 step: 91, loss is 1.385035753250122\n",
      "epoch: 21 step: 92, loss is 1.2954820394515991\n",
      "epoch: 21 step: 93, loss is 1.3201452493667603\n",
      "epoch: 21 step: 94, loss is 1.2768564224243164\n",
      "epoch: 21 step: 95, loss is 1.1947026252746582\n",
      "epoch: 21 step: 96, loss is 1.1703661680221558\n",
      "epoch: 21 step: 97, loss is 1.2523577213287354\n",
      "epoch: 21 step: 98, loss is 1.184714436531067\n",
      "epoch: 21 step: 99, loss is 1.2077562808990479\n",
      "epoch: 21 step: 100, loss is 1.2739125490188599\n",
      "epoch: 21 step: 101, loss is 1.3030776977539062\n",
      "epoch: 21 step: 102, loss is 1.2287976741790771\n",
      "epoch: 21 step: 103, loss is 1.2400588989257812\n",
      "epoch: 21 step: 104, loss is 1.122629165649414\n",
      "epoch: 21 step: 105, loss is 1.3091330528259277\n",
      "epoch: 21 step: 106, loss is 1.2551624774932861\n",
      "epoch: 21 step: 107, loss is 1.2148585319519043\n",
      "epoch: 21 step: 108, loss is 1.1591109037399292\n",
      "epoch: 21 step: 109, loss is 1.1383874416351318\n",
      "epoch: 21 step: 110, loss is 1.3003603219985962\n",
      "epoch: 21 step: 111, loss is 1.1695345640182495\n",
      "epoch: 21 step: 112, loss is 1.1327769756317139\n",
      "epoch: 21 step: 113, loss is 1.1897084712982178\n",
      "epoch: 21 step: 114, loss is 1.296822190284729\n",
      "epoch: 21 step: 115, loss is 1.3049877882003784\n",
      "epoch: 21 step: 116, loss is 1.2192260026931763\n",
      "epoch: 21 step: 117, loss is 1.2590235471725464\n",
      "epoch: 21 step: 118, loss is 1.2340890169143677\n",
      "epoch: 21 step: 119, loss is 1.2353240251541138\n",
      "epoch: 21 step: 120, loss is 1.2516366243362427\n",
      "epoch: 21 step: 121, loss is 1.075684666633606\n",
      "epoch: 21 step: 122, loss is 1.2703872919082642\n",
      "epoch: 21 step: 123, loss is 1.2291933298110962\n",
      "epoch: 21 step: 124, loss is 1.1884536743164062\n",
      "epoch: 21 step: 125, loss is 1.263223648071289\n",
      "epoch: 21 step: 126, loss is 1.2101075649261475\n",
      "epoch: 21 step: 127, loss is 1.214843988418579\n",
      "epoch: 21 step: 128, loss is 1.2473669052124023\n",
      "epoch: 21 step: 129, loss is 1.195798397064209\n",
      "epoch: 21 step: 130, loss is 1.1453715562820435\n",
      "epoch: 21 step: 131, loss is 1.1725050210952759\n",
      "epoch: 21 step: 132, loss is 1.215622901916504\n",
      "epoch: 21 step: 133, loss is 1.3415968418121338\n",
      "epoch: 21 step: 134, loss is 1.203373670578003\n",
      "epoch: 21 step: 135, loss is 1.2395775318145752\n",
      "epoch: 21 step: 136, loss is 1.274935245513916\n",
      "epoch: 21 step: 137, loss is 1.2422921657562256\n",
      "epoch: 21 step: 138, loss is 1.3507742881774902\n",
      "epoch: 21 step: 139, loss is 1.2604472637176514\n",
      "epoch: 21 step: 140, loss is 1.2103936672210693\n",
      "epoch: 21 step: 141, loss is 1.205899953842163\n",
      "epoch: 21 step: 142, loss is 1.1813068389892578\n",
      "epoch: 21 step: 143, loss is 1.1961500644683838\n",
      "epoch: 21 step: 144, loss is 1.140228033065796\n",
      "epoch: 21 step: 145, loss is 1.147475004196167\n",
      "epoch: 21 step: 146, loss is 1.1309611797332764\n",
      "epoch: 21 step: 147, loss is 1.1112803220748901\n",
      "epoch: 21 step: 148, loss is 1.1407980918884277\n",
      "epoch: 21 step: 149, loss is 1.2802574634552002\n",
      "epoch: 21 step: 150, loss is 1.1602221727371216\n",
      "epoch: 21 step: 151, loss is 1.2529444694519043\n",
      "epoch: 21 step: 152, loss is 1.13972008228302\n",
      "epoch: 21 step: 153, loss is 1.2260246276855469\n",
      "epoch: 21 step: 154, loss is 1.2710590362548828\n",
      "epoch: 21 step: 155, loss is 1.1109527349472046\n",
      "epoch: 21 step: 156, loss is 1.135871171951294\n",
      "epoch: 21 step: 157, loss is 1.1928032636642456\n",
      "epoch: 21 step: 158, loss is 1.2056978940963745\n",
      "epoch: 21 step: 159, loss is 1.1262805461883545\n",
      "epoch: 21 step: 160, loss is 1.2467927932739258\n",
      "epoch: 21 step: 161, loss is 1.1255288124084473\n",
      "epoch: 21 step: 162, loss is 1.2237108945846558\n",
      "epoch: 21 step: 163, loss is 1.1842710971832275\n",
      "epoch: 21 step: 164, loss is 1.2686667442321777\n",
      "epoch: 21 step: 165, loss is 1.1135128736495972\n",
      "epoch: 21 step: 166, loss is 1.2281818389892578\n",
      "epoch: 21 step: 167, loss is 1.356857419013977\n",
      "epoch: 21 step: 168, loss is 1.2103383541107178\n",
      "epoch: 21 step: 169, loss is 1.2400434017181396\n",
      "epoch: 21 step: 170, loss is 1.132602334022522\n",
      "epoch: 21 step: 171, loss is 1.3267313241958618\n",
      "epoch: 21 step: 172, loss is 1.2022550106048584\n",
      "epoch: 21 step: 173, loss is 1.1345690488815308\n",
      "epoch: 21 step: 174, loss is 1.2356369495391846\n",
      "epoch: 21 step: 175, loss is 1.2376070022583008\n",
      "epoch: 21 step: 176, loss is 1.2389329671859741\n",
      "epoch: 21 step: 177, loss is 1.1094130277633667\n",
      "epoch: 21 step: 178, loss is 1.2466402053833008\n",
      "epoch: 21 step: 179, loss is 1.1411726474761963\n",
      "epoch: 21 step: 180, loss is 1.1547420024871826\n",
      "epoch: 21 step: 181, loss is 1.2782690525054932\n",
      "epoch: 21 step: 182, loss is 1.2864198684692383\n",
      "epoch: 21 step: 183, loss is 1.3028761148452759\n",
      "epoch: 21 step: 184, loss is 1.172422170639038\n",
      "epoch: 21 step: 185, loss is 1.3244884014129639\n",
      "epoch: 21 step: 186, loss is 1.182878851890564\n",
      "epoch: 21 step: 187, loss is 1.1044723987579346\n",
      "epoch: 21 step: 188, loss is 1.2080539464950562\n",
      "epoch: 21 step: 189, loss is 1.1361266374588013\n",
      "epoch: 21 step: 190, loss is 1.0888797044754028\n",
      "epoch: 21 step: 191, loss is 1.194122552871704\n",
      "epoch: 21 step: 192, loss is 1.222778081893921\n",
      "epoch: 21 step: 193, loss is 1.1208940744400024\n",
      "epoch: 21 step: 194, loss is 1.2023911476135254\n",
      "epoch: 21 step: 195, loss is 1.1374180316925049\n",
      "epoch: 21 step: 196, loss is 1.234015703201294\n",
      "epoch: 21 step: 197, loss is 1.2483210563659668\n",
      "epoch: 21 step: 198, loss is 1.2910112142562866\n",
      "epoch: 21 step: 199, loss is 1.3186969757080078\n",
      "epoch: 21 step: 200, loss is 1.2400057315826416\n",
      "epoch: 21 step: 201, loss is 1.2608875036239624\n",
      "epoch: 21 step: 202, loss is 1.1429038047790527\n",
      "epoch: 21 step: 203, loss is 1.265782117843628\n",
      "epoch: 21 step: 204, loss is 1.1821001768112183\n",
      "epoch: 21 step: 205, loss is 1.1724251508712769\n",
      "epoch: 21 step: 206, loss is 1.1994096040725708\n",
      "epoch: 21 step: 207, loss is 1.244287371635437\n",
      "epoch: 21 step: 208, loss is 1.1996631622314453\n",
      "epoch: 21 step: 209, loss is 1.1456413269042969\n",
      "epoch: 21 step: 210, loss is 1.2304978370666504\n",
      "epoch: 21 step: 211, loss is 1.2021207809448242\n",
      "epoch: 21 step: 212, loss is 1.1523791551589966\n",
      "epoch: 21 step: 213, loss is 1.2179924249649048\n",
      "epoch: 21 step: 214, loss is 1.1896378993988037\n",
      "epoch: 21 step: 215, loss is 1.262809157371521\n",
      "epoch: 21 step: 216, loss is 1.2810685634613037\n",
      "epoch: 21 step: 217, loss is 1.2284505367279053\n",
      "epoch: 21 step: 218, loss is 1.2900147438049316\n",
      "epoch: 21 step: 219, loss is 1.0893325805664062\n",
      "epoch: 21 step: 220, loss is 1.1816822290420532\n",
      "epoch: 21 step: 221, loss is 1.165593147277832\n",
      "epoch: 21 step: 222, loss is 1.2395480871200562\n",
      "epoch: 21 step: 223, loss is 1.1219631433486938\n",
      "epoch: 21 step: 224, loss is 1.2161519527435303\n",
      "epoch: 21 step: 225, loss is 1.1514852046966553\n",
      "epoch: 21 step: 226, loss is 1.2925573587417603\n",
      "epoch: 21 step: 227, loss is 1.169454574584961\n",
      "epoch: 21 step: 228, loss is 1.2869188785552979\n",
      "epoch: 21 step: 229, loss is 1.2994863986968994\n",
      "epoch: 21 step: 230, loss is 1.186095118522644\n",
      "epoch: 21 step: 231, loss is 1.2019087076187134\n",
      "epoch: 21 step: 232, loss is 1.4115395545959473\n",
      "epoch: 21 step: 233, loss is 1.2625601291656494\n",
      "epoch: 21 step: 234, loss is 1.1604719161987305\n",
      "epoch: 21 step: 235, loss is 1.2226773500442505\n",
      "epoch: 21 step: 236, loss is 1.1612441539764404\n",
      "epoch: 21 step: 237, loss is 1.1996150016784668\n",
      "epoch: 21 step: 238, loss is 1.2134114503860474\n",
      "epoch: 21 step: 239, loss is 1.234945297241211\n",
      "epoch: 21 step: 240, loss is 1.1926320791244507\n",
      "epoch: 21 step: 241, loss is 1.2286957502365112\n",
      "epoch: 21 step: 242, loss is 1.1943359375\n",
      "epoch: 21 step: 243, loss is 1.1168508529663086\n",
      "epoch: 21 step: 244, loss is 1.249754548072815\n",
      "epoch: 21 step: 245, loss is 1.1470119953155518\n",
      "epoch: 21 step: 246, loss is 1.198516845703125\n",
      "epoch: 21 step: 247, loss is 1.2081916332244873\n",
      "epoch: 21 step: 248, loss is 1.1315999031066895\n",
      "epoch: 21 step: 249, loss is 1.3453948497772217\n",
      "epoch: 21 step: 250, loss is 1.0640411376953125\n",
      "epoch: 21 step: 251, loss is 1.1470601558685303\n",
      "epoch: 21 step: 252, loss is 1.2843947410583496\n",
      "epoch: 21 step: 253, loss is 1.238511085510254\n",
      "epoch: 21 step: 254, loss is 1.3432122468948364\n",
      "epoch: 21 step: 255, loss is 1.27750825881958\n",
      "epoch: 21 step: 256, loss is 1.2722740173339844\n",
      "epoch: 21 step: 257, loss is 1.1981682777404785\n",
      "epoch: 21 step: 258, loss is 1.3230966329574585\n",
      "epoch: 21 step: 259, loss is 1.2787028551101685\n",
      "epoch: 21 step: 260, loss is 1.3069584369659424\n",
      "epoch: 21 step: 261, loss is 1.3288236856460571\n",
      "epoch: 21 step: 262, loss is 1.3759911060333252\n",
      "epoch: 21 step: 263, loss is 1.2316700220108032\n",
      "epoch: 21 step: 264, loss is 1.3424043655395508\n",
      "epoch: 21 step: 265, loss is 1.2646148204803467\n",
      "epoch: 21 step: 266, loss is 1.362971305847168\n",
      "epoch: 21 step: 267, loss is 1.1707950830459595\n",
      "epoch: 21 step: 268, loss is 1.3083221912384033\n",
      "epoch: 21 step: 269, loss is 1.2034786939620972\n",
      "epoch: 21 step: 270, loss is 1.2916382551193237\n",
      "epoch: 21 step: 271, loss is 1.3215136528015137\n",
      "epoch: 21 step: 272, loss is 1.3770501613616943\n",
      "epoch: 21 step: 273, loss is 1.3022494316101074\n",
      "epoch: 21 step: 274, loss is 1.167525053024292\n",
      "epoch: 21 step: 275, loss is 1.2149349451065063\n",
      "epoch: 21 step: 276, loss is 1.243471622467041\n",
      "epoch: 21 step: 277, loss is 1.2103101015090942\n",
      "epoch: 21 step: 278, loss is 1.285531759262085\n",
      "epoch: 21 step: 279, loss is 1.2708266973495483\n",
      "epoch: 21 step: 280, loss is 1.3666027784347534\n",
      "epoch: 21 step: 281, loss is 1.3116674423217773\n",
      "epoch: 21 step: 282, loss is 1.256005883216858\n",
      "epoch: 21 step: 283, loss is 1.1518465280532837\n",
      "epoch: 21 step: 284, loss is 1.176817774772644\n",
      "epoch: 21 step: 285, loss is 1.2540239095687866\n",
      "epoch: 21 step: 286, loss is 1.2000977993011475\n",
      "epoch: 21 step: 287, loss is 1.3029913902282715\n",
      "epoch: 21 step: 288, loss is 1.1423282623291016\n",
      "epoch: 21 step: 289, loss is 1.242989420890808\n",
      "epoch: 21 step: 290, loss is 1.2434946298599243\n",
      "epoch: 21 step: 291, loss is 1.1960572004318237\n",
      "epoch: 21 step: 292, loss is 1.1750985383987427\n",
      "epoch: 21 step: 293, loss is 1.1933690309524536\n",
      "epoch: 21 step: 294, loss is 1.255394458770752\n",
      "epoch: 21 step: 295, loss is 1.2562947273254395\n",
      "epoch: 21 step: 296, loss is 1.3434221744537354\n",
      "epoch: 21 step: 297, loss is 1.2500889301300049\n",
      "epoch: 21 step: 298, loss is 1.254473090171814\n",
      "epoch: 21 step: 299, loss is 1.2575652599334717\n",
      "epoch: 21 step: 300, loss is 1.2851117849349976\n",
      "epoch: 21 step: 301, loss is 1.2421964406967163\n",
      "epoch: 21 step: 302, loss is 1.155226469039917\n",
      "epoch: 21 step: 303, loss is 1.219355821609497\n",
      "epoch: 21 step: 304, loss is 1.2633590698242188\n",
      "epoch: 21 step: 305, loss is 1.2752434015274048\n",
      "epoch: 21 step: 306, loss is 1.2561076879501343\n",
      "epoch: 21 step: 307, loss is 1.2450840473175049\n",
      "epoch: 21 step: 308, loss is 1.1122004985809326\n",
      "epoch: 21 step: 309, loss is 1.2581286430358887\n",
      "epoch: 21 step: 310, loss is 1.2537355422973633\n",
      "epoch: 21 step: 311, loss is 1.3951294422149658\n",
      "epoch: 21 step: 312, loss is 1.2617425918579102\n",
      "epoch: 21 step: 313, loss is 1.1419329643249512\n",
      "epoch: 21 step: 314, loss is 1.2185128927230835\n",
      "epoch: 21 step: 315, loss is 1.2462372779846191\n",
      "epoch: 21 step: 316, loss is 1.3384387493133545\n",
      "epoch: 21 step: 317, loss is 1.271424651145935\n",
      "epoch: 21 step: 318, loss is 1.2360097169876099\n",
      "epoch: 21 step: 319, loss is 1.2672388553619385\n",
      "epoch: 21 step: 320, loss is 1.2499114274978638\n",
      "epoch: 21 step: 321, loss is 1.134010910987854\n",
      "epoch: 21 step: 322, loss is 1.2260406017303467\n",
      "epoch: 21 step: 323, loss is 1.2410449981689453\n",
      "epoch: 21 step: 324, loss is 1.207275629043579\n",
      "epoch: 21 step: 325, loss is 1.2405757904052734\n",
      "epoch: 21 step: 326, loss is 1.2868430614471436\n",
      "epoch: 21 step: 327, loss is 1.1938527822494507\n",
      "epoch: 21 step: 328, loss is 1.1179931163787842\n",
      "epoch: 21 step: 329, loss is 1.1727352142333984\n",
      "epoch: 21 step: 330, loss is 1.281166911125183\n",
      "epoch: 21 step: 331, loss is 1.0966370105743408\n",
      "epoch: 21 step: 332, loss is 1.219882845878601\n",
      "epoch: 21 step: 333, loss is 1.1120532751083374\n",
      "epoch: 21 step: 334, loss is 1.1468589305877686\n",
      "epoch: 21 step: 335, loss is 1.185915470123291\n",
      "epoch: 21 step: 336, loss is 1.1339616775512695\n",
      "epoch: 21 step: 337, loss is 1.2760154008865356\n",
      "epoch: 21 step: 338, loss is 1.1706678867340088\n",
      "epoch: 21 step: 339, loss is 1.2723309993743896\n",
      "epoch: 21 step: 340, loss is 1.171268343925476\n",
      "epoch: 21 step: 341, loss is 1.106117844581604\n",
      "epoch: 21 step: 342, loss is 1.163185715675354\n",
      "epoch: 21 step: 343, loss is 1.2677465677261353\n",
      "epoch: 21 step: 344, loss is 1.2241452932357788\n",
      "epoch: 21 step: 345, loss is 1.2869422435760498\n",
      "epoch: 21 step: 346, loss is 1.2170685529708862\n",
      "epoch: 21 step: 347, loss is 1.1170823574066162\n",
      "epoch: 21 step: 348, loss is 1.2502357959747314\n",
      "epoch: 21 step: 349, loss is 1.2753503322601318\n",
      "epoch: 21 step: 350, loss is 1.3136649131774902\n",
      "epoch: 21 step: 351, loss is 1.212615728378296\n",
      "epoch: 21 step: 352, loss is 1.1486151218414307\n",
      "epoch: 21 step: 353, loss is 1.3341282606124878\n",
      "epoch: 21 step: 354, loss is 1.2293241024017334\n",
      "epoch: 21 step: 355, loss is 1.2654683589935303\n",
      "epoch: 21 step: 356, loss is 1.2332017421722412\n",
      "epoch: 21 step: 357, loss is 1.140330195426941\n",
      "epoch: 21 step: 358, loss is 1.3241527080535889\n",
      "epoch: 21 step: 359, loss is 1.1891944408416748\n",
      "epoch: 21 step: 360, loss is 1.272446870803833\n",
      "epoch: 21 step: 361, loss is 1.2996734380722046\n",
      "epoch: 21 step: 362, loss is 1.269547700881958\n",
      "epoch: 21 step: 363, loss is 1.4189624786376953\n",
      "epoch: 21 step: 364, loss is 1.2812986373901367\n",
      "epoch: 21 step: 365, loss is 1.1368916034698486\n",
      "epoch: 21 step: 366, loss is 1.306788682937622\n",
      "epoch: 21 step: 367, loss is 1.3058085441589355\n",
      "epoch: 21 step: 368, loss is 1.1625902652740479\n",
      "epoch: 21 step: 369, loss is 1.2091530561447144\n",
      "epoch: 21 step: 370, loss is 1.2341059446334839\n",
      "epoch: 21 step: 371, loss is 1.1860178709030151\n",
      "epoch: 21 step: 372, loss is 1.196275234222412\n",
      "epoch: 21 step: 373, loss is 1.1368287801742554\n",
      "epoch: 21 step: 374, loss is 1.0521256923675537\n",
      "epoch: 21 step: 375, loss is 1.22911536693573\n",
      "epoch: 21 step: 376, loss is 1.224898338317871\n",
      "epoch: 21 step: 377, loss is 1.1969563961029053\n",
      "epoch: 21 step: 378, loss is 1.2351130247116089\n",
      "epoch: 21 step: 379, loss is 1.341909646987915\n",
      "epoch: 21 step: 380, loss is 1.1748933792114258\n",
      "epoch: 21 step: 381, loss is 1.0861399173736572\n",
      "epoch: 21 step: 382, loss is 1.297091007232666\n",
      "epoch: 21 step: 383, loss is 1.1231567859649658\n",
      "epoch: 21 step: 384, loss is 1.388903021812439\n",
      "epoch: 21 step: 385, loss is 1.2206048965454102\n",
      "epoch: 21 step: 386, loss is 1.2124245166778564\n",
      "epoch: 21 step: 387, loss is 1.1211059093475342\n",
      "epoch: 21 step: 388, loss is 1.205702543258667\n",
      "epoch: 21 step: 389, loss is 1.2159204483032227\n",
      "epoch: 21 step: 390, loss is 1.304290533065796\n",
      "Train epoch time: 159604.982 ms, per step time: 409.244 ms\n",
      "epoch: 22 step: 1, loss is 1.114069938659668\n",
      "epoch: 22 step: 2, loss is 1.1762428283691406\n",
      "epoch: 22 step: 3, loss is 1.0918591022491455\n",
      "epoch: 22 step: 4, loss is 1.1218067407608032\n",
      "epoch: 22 step: 5, loss is 1.2431871891021729\n",
      "epoch: 22 step: 6, loss is 1.1528429985046387\n",
      "epoch: 22 step: 7, loss is 1.1558952331542969\n",
      "epoch: 22 step: 8, loss is 1.242392897605896\n",
      "epoch: 22 step: 9, loss is 1.3158013820648193\n",
      "epoch: 22 step: 10, loss is 1.1638773679733276\n",
      "epoch: 22 step: 11, loss is 1.2038897275924683\n",
      "epoch: 22 step: 12, loss is 1.2046068906784058\n",
      "epoch: 22 step: 13, loss is 1.2450149059295654\n",
      "epoch: 22 step: 14, loss is 1.2178075313568115\n",
      "epoch: 22 step: 15, loss is 1.21919846534729\n",
      "epoch: 22 step: 16, loss is 1.2081927061080933\n",
      "epoch: 22 step: 17, loss is 1.187069058418274\n",
      "epoch: 22 step: 18, loss is 1.3229693174362183\n",
      "epoch: 22 step: 19, loss is 1.2105482816696167\n",
      "epoch: 22 step: 20, loss is 1.2138283252716064\n",
      "epoch: 22 step: 21, loss is 1.2532916069030762\n",
      "epoch: 22 step: 22, loss is 1.2689673900604248\n",
      "epoch: 22 step: 23, loss is 1.2634532451629639\n",
      "epoch: 22 step: 24, loss is 1.1219120025634766\n",
      "epoch: 22 step: 25, loss is 1.3057277202606201\n",
      "epoch: 22 step: 26, loss is 1.0943692922592163\n",
      "epoch: 22 step: 27, loss is 1.2581760883331299\n",
      "epoch: 22 step: 28, loss is 1.1235203742980957\n",
      "epoch: 22 step: 29, loss is 1.181718111038208\n",
      "epoch: 22 step: 30, loss is 1.24735689163208\n",
      "epoch: 22 step: 31, loss is 1.2572718858718872\n",
      "epoch: 22 step: 32, loss is 1.1626710891723633\n",
      "epoch: 22 step: 33, loss is 1.219564437866211\n",
      "epoch: 22 step: 34, loss is 1.1137639284133911\n",
      "epoch: 22 step: 35, loss is 1.2760069370269775\n",
      "epoch: 22 step: 36, loss is 1.2278642654418945\n",
      "epoch: 22 step: 37, loss is 1.0744175910949707\n",
      "epoch: 22 step: 38, loss is 1.1804496049880981\n",
      "epoch: 22 step: 39, loss is 1.1819937229156494\n",
      "epoch: 22 step: 40, loss is 1.1791620254516602\n",
      "epoch: 22 step: 41, loss is 1.1671651601791382\n",
      "epoch: 22 step: 42, loss is 1.122753620147705\n",
      "epoch: 22 step: 43, loss is 1.194036602973938\n",
      "epoch: 22 step: 44, loss is 1.143357276916504\n",
      "epoch: 22 step: 45, loss is 1.2013362646102905\n",
      "epoch: 22 step: 46, loss is 1.2700965404510498\n",
      "epoch: 22 step: 47, loss is 1.130150318145752\n",
      "epoch: 22 step: 48, loss is 1.232823371887207\n",
      "epoch: 22 step: 49, loss is 1.3323894739151\n",
      "epoch: 22 step: 50, loss is 1.0476195812225342\n",
      "epoch: 22 step: 51, loss is 1.1992233991622925\n",
      "epoch: 22 step: 52, loss is 1.1241704225540161\n",
      "epoch: 22 step: 53, loss is 1.3678929805755615\n",
      "epoch: 22 step: 54, loss is 1.1109492778778076\n",
      "epoch: 22 step: 55, loss is 1.117936134338379\n",
      "epoch: 22 step: 56, loss is 1.4297336339950562\n",
      "epoch: 22 step: 57, loss is 1.141024112701416\n",
      "epoch: 22 step: 58, loss is 1.2110989093780518\n",
      "epoch: 22 step: 59, loss is 1.178731918334961\n",
      "epoch: 22 step: 60, loss is 1.3250477313995361\n",
      "epoch: 22 step: 61, loss is 1.1611260175704956\n",
      "epoch: 22 step: 62, loss is 1.3169797658920288\n",
      "epoch: 22 step: 63, loss is 1.3554075956344604\n",
      "epoch: 22 step: 64, loss is 1.2140851020812988\n",
      "epoch: 22 step: 65, loss is 1.2967100143432617\n",
      "epoch: 22 step: 66, loss is 1.3416262865066528\n",
      "epoch: 22 step: 67, loss is 1.2080763578414917\n",
      "epoch: 22 step: 68, loss is 1.0841525793075562\n",
      "epoch: 22 step: 69, loss is 1.2041354179382324\n",
      "epoch: 22 step: 70, loss is 1.2109750509262085\n",
      "epoch: 22 step: 71, loss is 1.1192173957824707\n",
      "epoch: 22 step: 72, loss is 1.1472653150558472\n",
      "epoch: 22 step: 73, loss is 1.28310227394104\n",
      "epoch: 22 step: 74, loss is 1.2670924663543701\n",
      "epoch: 22 step: 75, loss is 1.2286326885223389\n",
      "epoch: 22 step: 76, loss is 1.2364920377731323\n",
      "epoch: 22 step: 77, loss is 1.2048065662384033\n",
      "epoch: 22 step: 78, loss is 1.1759164333343506\n",
      "epoch: 22 step: 79, loss is 1.1801435947418213\n",
      "epoch: 22 step: 80, loss is 1.2193386554718018\n",
      "epoch: 22 step: 81, loss is 1.0277256965637207\n",
      "epoch: 22 step: 82, loss is 1.194731593132019\n",
      "epoch: 22 step: 83, loss is 1.2568680047988892\n",
      "epoch: 22 step: 84, loss is 1.1217964887619019\n",
      "epoch: 22 step: 85, loss is 1.1299335956573486\n",
      "epoch: 22 step: 86, loss is 1.2387014627456665\n",
      "epoch: 22 step: 87, loss is 1.155701756477356\n",
      "epoch: 22 step: 88, loss is 1.2543970346450806\n",
      "epoch: 22 step: 89, loss is 1.1686817407608032\n",
      "epoch: 22 step: 90, loss is 1.1539167165756226\n",
      "epoch: 22 step: 91, loss is 1.1604377031326294\n",
      "epoch: 22 step: 92, loss is 1.0978548526763916\n",
      "epoch: 22 step: 93, loss is 1.2340264320373535\n",
      "epoch: 22 step: 94, loss is 1.2495760917663574\n",
      "epoch: 22 step: 95, loss is 1.2432303428649902\n",
      "epoch: 22 step: 96, loss is 1.1433550119400024\n",
      "epoch: 22 step: 97, loss is 1.1074129343032837\n",
      "epoch: 22 step: 98, loss is 1.2292721271514893\n",
      "epoch: 22 step: 99, loss is 1.2512491941452026\n",
      "epoch: 22 step: 100, loss is 1.1598347425460815\n",
      "epoch: 22 step: 101, loss is 1.19309663772583\n",
      "epoch: 22 step: 102, loss is 1.274191975593567\n",
      "epoch: 22 step: 103, loss is 1.2008616924285889\n",
      "epoch: 22 step: 104, loss is 1.2206299304962158\n",
      "epoch: 22 step: 105, loss is 1.1387734413146973\n",
      "epoch: 22 step: 106, loss is 1.2486460208892822\n",
      "epoch: 22 step: 107, loss is 1.1766316890716553\n",
      "epoch: 22 step: 108, loss is 1.0855472087860107\n",
      "epoch: 22 step: 109, loss is 1.1363286972045898\n",
      "epoch: 22 step: 110, loss is 1.2283577919006348\n",
      "epoch: 22 step: 111, loss is 1.2348437309265137\n",
      "epoch: 22 step: 112, loss is 1.1199814081192017\n",
      "epoch: 22 step: 113, loss is 1.3351435661315918\n",
      "epoch: 22 step: 114, loss is 1.266234278678894\n",
      "epoch: 22 step: 115, loss is 1.3175950050354004\n",
      "epoch: 22 step: 116, loss is 1.0836329460144043\n",
      "epoch: 22 step: 117, loss is 1.2414205074310303\n",
      "epoch: 22 step: 118, loss is 1.2394587993621826\n",
      "epoch: 22 step: 119, loss is 1.3142132759094238\n",
      "epoch: 22 step: 120, loss is 1.3061684370040894\n",
      "epoch: 22 step: 121, loss is 1.2301808595657349\n",
      "epoch: 22 step: 122, loss is 1.2333585023880005\n",
      "epoch: 22 step: 123, loss is 1.179466962814331\n",
      "epoch: 22 step: 124, loss is 1.3226839303970337\n",
      "epoch: 22 step: 125, loss is 1.1243513822555542\n",
      "epoch: 22 step: 126, loss is 1.3291049003601074\n",
      "epoch: 22 step: 127, loss is 1.1490155458450317\n",
      "epoch: 22 step: 128, loss is 1.2945456504821777\n",
      "epoch: 22 step: 129, loss is 1.1335194110870361\n",
      "epoch: 22 step: 130, loss is 1.1183078289031982\n",
      "epoch: 22 step: 131, loss is 1.2185481786727905\n",
      "epoch: 22 step: 132, loss is 1.1875255107879639\n",
      "epoch: 22 step: 133, loss is 1.2852814197540283\n",
      "epoch: 22 step: 134, loss is 1.3553006649017334\n",
      "epoch: 22 step: 135, loss is 1.2811617851257324\n",
      "epoch: 22 step: 136, loss is 1.1772874593734741\n",
      "epoch: 22 step: 137, loss is 1.1700093746185303\n",
      "epoch: 22 step: 138, loss is 1.1590850353240967\n",
      "epoch: 22 step: 139, loss is 1.1494412422180176\n",
      "epoch: 22 step: 140, loss is 1.1756232976913452\n",
      "epoch: 22 step: 141, loss is 1.1322643756866455\n",
      "epoch: 22 step: 142, loss is 1.2272229194641113\n",
      "epoch: 22 step: 143, loss is 1.230161190032959\n",
      "epoch: 22 step: 144, loss is 1.1999139785766602\n",
      "epoch: 22 step: 145, loss is 1.2468714714050293\n",
      "epoch: 22 step: 146, loss is 1.2983002662658691\n",
      "epoch: 22 step: 147, loss is 1.3184194564819336\n",
      "epoch: 22 step: 148, loss is 1.2265002727508545\n",
      "epoch: 22 step: 149, loss is 1.1301223039627075\n",
      "epoch: 22 step: 150, loss is 1.210692286491394\n",
      "epoch: 22 step: 151, loss is 1.2014970779418945\n",
      "epoch: 22 step: 152, loss is 1.2846218347549438\n",
      "epoch: 22 step: 153, loss is 1.1444858312606812\n",
      "epoch: 22 step: 154, loss is 1.1814013719558716\n",
      "epoch: 22 step: 155, loss is 1.277574896812439\n",
      "epoch: 22 step: 156, loss is 1.2767293453216553\n",
      "epoch: 22 step: 157, loss is 1.2475546598434448\n",
      "epoch: 22 step: 158, loss is 1.283220887184143\n",
      "epoch: 22 step: 159, loss is 1.1665420532226562\n",
      "epoch: 22 step: 160, loss is 1.3448970317840576\n",
      "epoch: 22 step: 161, loss is 1.2334994077682495\n",
      "epoch: 22 step: 162, loss is 1.2554317712783813\n",
      "epoch: 22 step: 163, loss is 1.2866947650909424\n",
      "epoch: 22 step: 164, loss is 1.1448653936386108\n",
      "epoch: 22 step: 165, loss is 1.1597528457641602\n",
      "epoch: 22 step: 166, loss is 1.324694275856018\n",
      "epoch: 22 step: 167, loss is 1.3971877098083496\n",
      "epoch: 22 step: 168, loss is 1.1879819631576538\n",
      "epoch: 22 step: 169, loss is 1.1499574184417725\n",
      "epoch: 22 step: 170, loss is 1.317596197128296\n",
      "epoch: 22 step: 171, loss is 1.1717548370361328\n",
      "epoch: 22 step: 172, loss is 1.181182861328125\n",
      "epoch: 22 step: 173, loss is 1.207060694694519\n",
      "epoch: 22 step: 174, loss is 1.1986212730407715\n",
      "epoch: 22 step: 175, loss is 1.1398048400878906\n",
      "epoch: 22 step: 176, loss is 1.219564437866211\n",
      "epoch: 22 step: 177, loss is 1.13076651096344\n",
      "epoch: 22 step: 178, loss is 1.2896807193756104\n",
      "epoch: 22 step: 179, loss is 1.2473658323287964\n",
      "epoch: 22 step: 180, loss is 1.1230777502059937\n",
      "epoch: 22 step: 181, loss is 1.234462857246399\n",
      "epoch: 22 step: 182, loss is 1.2268264293670654\n",
      "epoch: 22 step: 183, loss is 1.2434332370758057\n",
      "epoch: 22 step: 184, loss is 1.205217719078064\n",
      "epoch: 22 step: 185, loss is 1.1761231422424316\n",
      "epoch: 22 step: 186, loss is 1.1283022165298462\n",
      "epoch: 22 step: 187, loss is 1.3710548877716064\n",
      "epoch: 22 step: 188, loss is 1.111041784286499\n",
      "epoch: 22 step: 189, loss is 1.2167110443115234\n",
      "epoch: 22 step: 190, loss is 1.245553970336914\n",
      "epoch: 22 step: 191, loss is 1.096959114074707\n",
      "epoch: 22 step: 192, loss is 1.401504635810852\n",
      "epoch: 22 step: 193, loss is 1.1169971227645874\n",
      "epoch: 22 step: 194, loss is 1.157245397567749\n",
      "epoch: 22 step: 195, loss is 1.1929004192352295\n",
      "epoch: 22 step: 196, loss is 1.1818914413452148\n",
      "epoch: 22 step: 197, loss is 1.2763044834136963\n",
      "epoch: 22 step: 198, loss is 1.244083285331726\n",
      "epoch: 22 step: 199, loss is 1.1967260837554932\n",
      "epoch: 22 step: 200, loss is 1.2545576095581055\n",
      "epoch: 22 step: 201, loss is 1.2225381135940552\n",
      "epoch: 22 step: 202, loss is 1.2310911417007446\n",
      "epoch: 22 step: 203, loss is 1.376629114151001\n",
      "epoch: 22 step: 204, loss is 1.2744637727737427\n",
      "epoch: 22 step: 205, loss is 1.0813047885894775\n",
      "epoch: 22 step: 206, loss is 1.2825671434402466\n",
      "epoch: 22 step: 207, loss is 1.1883347034454346\n",
      "epoch: 22 step: 208, loss is 1.1197937726974487\n",
      "epoch: 22 step: 209, loss is 1.1272304058074951\n",
      "epoch: 22 step: 210, loss is 1.2690470218658447\n",
      "epoch: 22 step: 211, loss is 1.185225009918213\n",
      "epoch: 22 step: 212, loss is 1.3083142042160034\n",
      "epoch: 22 step: 213, loss is 1.2370673418045044\n",
      "epoch: 22 step: 214, loss is 1.2334330081939697\n",
      "epoch: 22 step: 215, loss is 1.156526803970337\n",
      "epoch: 22 step: 216, loss is 1.0961825847625732\n",
      "epoch: 22 step: 217, loss is 1.221296787261963\n",
      "epoch: 22 step: 218, loss is 1.1235395669937134\n",
      "epoch: 22 step: 219, loss is 1.164186716079712\n",
      "epoch: 22 step: 220, loss is 1.164849877357483\n",
      "epoch: 22 step: 221, loss is 1.2344701290130615\n",
      "epoch: 22 step: 222, loss is 1.155430555343628\n",
      "epoch: 22 step: 223, loss is 1.2991247177124023\n",
      "epoch: 22 step: 224, loss is 1.1518186330795288\n",
      "epoch: 22 step: 225, loss is 1.1968687772750854\n",
      "epoch: 22 step: 226, loss is 1.1938766241073608\n",
      "epoch: 22 step: 227, loss is 1.1903437376022339\n",
      "epoch: 22 step: 228, loss is 1.1462546586990356\n",
      "epoch: 22 step: 229, loss is 1.2536190748214722\n",
      "epoch: 22 step: 230, loss is 1.2195653915405273\n",
      "epoch: 22 step: 231, loss is 1.3214534521102905\n",
      "epoch: 22 step: 232, loss is 1.2776784896850586\n",
      "epoch: 22 step: 233, loss is 1.134865403175354\n",
      "epoch: 22 step: 234, loss is 1.2346529960632324\n",
      "epoch: 22 step: 235, loss is 1.1608917713165283\n",
      "epoch: 22 step: 236, loss is 1.1050307750701904\n",
      "epoch: 22 step: 237, loss is 1.1935606002807617\n",
      "epoch: 22 step: 238, loss is 1.1890532970428467\n",
      "epoch: 22 step: 239, loss is 1.2765071392059326\n",
      "epoch: 22 step: 240, loss is 1.1772361993789673\n",
      "epoch: 22 step: 241, loss is 1.2414464950561523\n",
      "epoch: 22 step: 242, loss is 1.1954007148742676\n",
      "epoch: 22 step: 243, loss is 1.240378737449646\n",
      "epoch: 22 step: 244, loss is 1.0755383968353271\n",
      "epoch: 22 step: 245, loss is 1.1886203289031982\n",
      "epoch: 22 step: 246, loss is 1.160918116569519\n",
      "epoch: 22 step: 247, loss is 1.260208010673523\n",
      "epoch: 22 step: 248, loss is 1.0826016664505005\n",
      "epoch: 22 step: 249, loss is 1.2069966793060303\n",
      "epoch: 22 step: 250, loss is 1.1532971858978271\n",
      "epoch: 22 step: 251, loss is 1.2113993167877197\n",
      "epoch: 22 step: 252, loss is 1.1135756969451904\n",
      "epoch: 22 step: 253, loss is 1.2696974277496338\n",
      "epoch: 22 step: 254, loss is 1.1629273891448975\n",
      "epoch: 22 step: 255, loss is 1.2077817916870117\n",
      "epoch: 22 step: 256, loss is 1.213813304901123\n",
      "epoch: 22 step: 257, loss is 1.10919189453125\n",
      "epoch: 22 step: 258, loss is 1.2866235971450806\n",
      "epoch: 22 step: 259, loss is 1.2954537868499756\n",
      "epoch: 22 step: 260, loss is 1.2297443151474\n",
      "epoch: 22 step: 261, loss is 1.1476190090179443\n",
      "epoch: 22 step: 262, loss is 1.221277117729187\n",
      "epoch: 22 step: 263, loss is 1.266815185546875\n",
      "epoch: 22 step: 264, loss is 1.243274450302124\n",
      "epoch: 22 step: 265, loss is 1.1359456777572632\n",
      "epoch: 22 step: 266, loss is 1.0679492950439453\n",
      "epoch: 22 step: 267, loss is 1.287003517150879\n",
      "epoch: 22 step: 268, loss is 1.2437986135482788\n",
      "epoch: 22 step: 269, loss is 1.202545404434204\n",
      "epoch: 22 step: 270, loss is 1.1708455085754395\n",
      "epoch: 22 step: 271, loss is 1.2290903329849243\n",
      "epoch: 22 step: 272, loss is 1.1621718406677246\n",
      "epoch: 22 step: 273, loss is 1.2275108098983765\n",
      "epoch: 22 step: 274, loss is 1.2035943269729614\n",
      "epoch: 22 step: 275, loss is 1.2253293991088867\n",
      "epoch: 22 step: 276, loss is 1.1207809448242188\n",
      "epoch: 22 step: 277, loss is 1.2187252044677734\n",
      "epoch: 22 step: 278, loss is 1.120834231376648\n",
      "epoch: 22 step: 279, loss is 1.196164846420288\n",
      "epoch: 22 step: 280, loss is 1.1205309629440308\n",
      "epoch: 22 step: 281, loss is 1.1885433197021484\n",
      "epoch: 22 step: 282, loss is 1.2084789276123047\n",
      "epoch: 22 step: 283, loss is 1.1169307231903076\n",
      "epoch: 22 step: 284, loss is 1.0810325145721436\n",
      "epoch: 22 step: 285, loss is 1.269601821899414\n",
      "epoch: 22 step: 286, loss is 1.177978754043579\n",
      "epoch: 22 step: 287, loss is 1.2777117490768433\n",
      "epoch: 22 step: 288, loss is 1.0601332187652588\n",
      "epoch: 22 step: 289, loss is 1.175809383392334\n",
      "epoch: 22 step: 290, loss is 1.2293579578399658\n",
      "epoch: 22 step: 291, loss is 1.1811589002609253\n",
      "epoch: 22 step: 292, loss is 1.2688024044036865\n",
      "epoch: 22 step: 293, loss is 1.1125246286392212\n",
      "epoch: 22 step: 294, loss is 1.1855181455612183\n",
      "epoch: 22 step: 295, loss is 1.268799066543579\n",
      "epoch: 22 step: 296, loss is 1.0965471267700195\n",
      "epoch: 22 step: 297, loss is 1.1837656497955322\n",
      "epoch: 22 step: 298, loss is 1.3752007484436035\n",
      "epoch: 22 step: 299, loss is 1.2556582689285278\n",
      "epoch: 22 step: 300, loss is 1.1585357189178467\n",
      "epoch: 22 step: 301, loss is 1.1896770000457764\n",
      "epoch: 22 step: 302, loss is 1.291536569595337\n",
      "epoch: 22 step: 303, loss is 1.194380521774292\n",
      "epoch: 22 step: 304, loss is 1.285902738571167\n",
      "epoch: 22 step: 305, loss is 1.1851080656051636\n",
      "epoch: 22 step: 306, loss is 1.1812770366668701\n",
      "epoch: 22 step: 307, loss is 1.248023509979248\n",
      "epoch: 22 step: 308, loss is 1.2769455909729004\n",
      "epoch: 22 step: 309, loss is 1.181641936302185\n",
      "epoch: 22 step: 310, loss is 1.1849638223648071\n",
      "epoch: 22 step: 311, loss is 1.1497936248779297\n",
      "epoch: 22 step: 312, loss is 1.1355634927749634\n",
      "epoch: 22 step: 313, loss is 1.1059664487838745\n",
      "epoch: 22 step: 314, loss is 1.2516453266143799\n",
      "epoch: 22 step: 315, loss is 1.242316722869873\n",
      "epoch: 22 step: 316, loss is 1.2426908016204834\n",
      "epoch: 22 step: 317, loss is 1.148488998413086\n",
      "epoch: 22 step: 318, loss is 1.1706937551498413\n",
      "epoch: 22 step: 319, loss is 1.1254510879516602\n",
      "epoch: 22 step: 320, loss is 1.182765007019043\n",
      "epoch: 22 step: 321, loss is 1.1265766620635986\n",
      "epoch: 22 step: 322, loss is 1.200667381286621\n",
      "epoch: 22 step: 323, loss is 1.2639284133911133\n",
      "epoch: 22 step: 324, loss is 1.2962541580200195\n",
      "epoch: 22 step: 325, loss is 1.1465568542480469\n",
      "epoch: 22 step: 326, loss is 1.2098432779312134\n",
      "epoch: 22 step: 327, loss is 1.269136905670166\n",
      "epoch: 22 step: 328, loss is 1.2541229724884033\n",
      "epoch: 22 step: 329, loss is 1.1222660541534424\n",
      "epoch: 22 step: 330, loss is 1.1351430416107178\n",
      "epoch: 22 step: 331, loss is 1.1685782670974731\n",
      "epoch: 22 step: 332, loss is 1.2432950735092163\n",
      "epoch: 22 step: 333, loss is 1.3568546772003174\n",
      "epoch: 22 step: 334, loss is 1.0841772556304932\n",
      "epoch: 22 step: 335, loss is 1.185007095336914\n",
      "epoch: 22 step: 336, loss is 1.2181308269500732\n",
      "epoch: 22 step: 337, loss is 1.2019380331039429\n",
      "epoch: 22 step: 338, loss is 1.2412350177764893\n",
      "epoch: 22 step: 339, loss is 1.233494520187378\n",
      "epoch: 22 step: 340, loss is 1.2661609649658203\n",
      "epoch: 22 step: 341, loss is 1.325516700744629\n",
      "epoch: 22 step: 342, loss is 1.1888506412506104\n",
      "epoch: 22 step: 343, loss is 1.1553137302398682\n",
      "epoch: 22 step: 344, loss is 1.187461018562317\n",
      "epoch: 22 step: 345, loss is 1.3289114236831665\n",
      "epoch: 22 step: 346, loss is 1.1794421672821045\n",
      "epoch: 22 step: 347, loss is 1.2526683807373047\n",
      "epoch: 22 step: 348, loss is 1.1461491584777832\n",
      "epoch: 22 step: 349, loss is 1.305179476737976\n",
      "epoch: 22 step: 350, loss is 1.2122048139572144\n",
      "epoch: 22 step: 351, loss is 1.0858418941497803\n",
      "epoch: 22 step: 352, loss is 1.2379423379898071\n",
      "epoch: 22 step: 353, loss is 1.1926103830337524\n",
      "epoch: 22 step: 354, loss is 1.2311210632324219\n",
      "epoch: 22 step: 355, loss is 1.3486309051513672\n",
      "epoch: 22 step: 356, loss is 1.1159846782684326\n",
      "epoch: 22 step: 357, loss is 1.3061619997024536\n",
      "epoch: 22 step: 358, loss is 1.2630411386489868\n",
      "epoch: 22 step: 359, loss is 1.1324396133422852\n",
      "epoch: 22 step: 360, loss is 1.1681171655654907\n",
      "epoch: 22 step: 361, loss is 1.192954182624817\n",
      "epoch: 22 step: 362, loss is 1.1360507011413574\n",
      "epoch: 22 step: 363, loss is 1.2010486125946045\n",
      "epoch: 22 step: 364, loss is 1.2593261003494263\n",
      "epoch: 22 step: 365, loss is 1.1954832077026367\n",
      "epoch: 22 step: 366, loss is 1.2348005771636963\n",
      "epoch: 22 step: 367, loss is 1.1676204204559326\n",
      "epoch: 22 step: 368, loss is 1.2508575916290283\n",
      "epoch: 22 step: 369, loss is 1.1837660074234009\n",
      "epoch: 22 step: 370, loss is 1.1701221466064453\n",
      "epoch: 22 step: 371, loss is 1.1736763715744019\n",
      "epoch: 22 step: 372, loss is 1.1831786632537842\n",
      "epoch: 22 step: 373, loss is 1.235607385635376\n",
      "epoch: 22 step: 374, loss is 1.2005977630615234\n",
      "epoch: 22 step: 375, loss is 1.167458415031433\n",
      "epoch: 22 step: 376, loss is 1.1788302659988403\n",
      "epoch: 22 step: 377, loss is 1.355381727218628\n",
      "epoch: 22 step: 378, loss is 1.1852885484695435\n",
      "epoch: 22 step: 379, loss is 1.288002610206604\n",
      "epoch: 22 step: 380, loss is 1.1532782316207886\n",
      "epoch: 22 step: 381, loss is 1.185143232345581\n",
      "epoch: 22 step: 382, loss is 1.2336931228637695\n",
      "epoch: 22 step: 383, loss is 1.253523826599121\n",
      "epoch: 22 step: 384, loss is 1.1942658424377441\n",
      "epoch: 22 step: 385, loss is 1.087194561958313\n",
      "epoch: 22 step: 386, loss is 1.2848360538482666\n",
      "epoch: 22 step: 387, loss is 1.1788769960403442\n",
      "epoch: 22 step: 388, loss is 1.2657619714736938\n",
      "epoch: 22 step: 389, loss is 1.2094759941101074\n",
      "epoch: 22 step: 390, loss is 1.0387818813323975\n",
      "Train epoch time: 152553.829 ms, per step time: 391.164 ms\n",
      "epoch: 23 step: 1, loss is 1.1506909132003784\n",
      "epoch: 23 step: 2, loss is 1.1796233654022217\n",
      "epoch: 23 step: 3, loss is 1.1342135667800903\n",
      "epoch: 23 step: 4, loss is 1.2398890256881714\n",
      "epoch: 23 step: 5, loss is 1.1781587600708008\n",
      "epoch: 23 step: 6, loss is 1.1909464597702026\n",
      "epoch: 23 step: 7, loss is 1.2729068994522095\n",
      "epoch: 23 step: 8, loss is 1.2035998106002808\n",
      "epoch: 23 step: 9, loss is 1.2802610397338867\n",
      "epoch: 23 step: 10, loss is 1.251396656036377\n",
      "epoch: 23 step: 11, loss is 1.1665308475494385\n",
      "epoch: 23 step: 12, loss is 1.1339621543884277\n",
      "epoch: 23 step: 13, loss is 1.1197067499160767\n",
      "epoch: 23 step: 14, loss is 1.170196294784546\n",
      "epoch: 23 step: 15, loss is 1.217250943183899\n",
      "epoch: 23 step: 16, loss is 1.2329362630844116\n",
      "epoch: 23 step: 17, loss is 1.2313909530639648\n",
      "epoch: 23 step: 18, loss is 1.2699687480926514\n",
      "epoch: 23 step: 19, loss is 1.2334883213043213\n",
      "epoch: 23 step: 20, loss is 1.152655839920044\n",
      "epoch: 23 step: 21, loss is 1.2699453830718994\n",
      "epoch: 23 step: 22, loss is 1.115369200706482\n",
      "epoch: 23 step: 23, loss is 1.1298105716705322\n",
      "epoch: 23 step: 24, loss is 1.2322216033935547\n",
      "epoch: 23 step: 25, loss is 1.227689504623413\n",
      "epoch: 23 step: 26, loss is 1.168932557106018\n",
      "epoch: 23 step: 27, loss is 1.3296935558319092\n",
      "epoch: 23 step: 28, loss is 1.1249278783798218\n",
      "epoch: 23 step: 29, loss is 1.2087100744247437\n",
      "epoch: 23 step: 30, loss is 1.2444807291030884\n",
      "epoch: 23 step: 31, loss is 1.2740495204925537\n",
      "epoch: 23 step: 32, loss is 1.3124232292175293\n",
      "epoch: 23 step: 33, loss is 1.3864020109176636\n",
      "epoch: 23 step: 34, loss is 1.1474236249923706\n",
      "epoch: 23 step: 35, loss is 1.3211309909820557\n",
      "epoch: 23 step: 36, loss is 1.2378666400909424\n",
      "epoch: 23 step: 37, loss is 1.2627047300338745\n",
      "epoch: 23 step: 38, loss is 1.1174145936965942\n",
      "epoch: 23 step: 39, loss is 1.288581371307373\n",
      "epoch: 23 step: 40, loss is 1.155656099319458\n",
      "epoch: 23 step: 41, loss is 1.2267650365829468\n",
      "epoch: 23 step: 42, loss is 1.271915078163147\n",
      "epoch: 23 step: 43, loss is 1.2643473148345947\n",
      "epoch: 23 step: 44, loss is 1.3349357843399048\n",
      "epoch: 23 step: 45, loss is 1.1953802108764648\n",
      "epoch: 23 step: 46, loss is 1.2583264112472534\n",
      "epoch: 23 step: 47, loss is 1.152103066444397\n",
      "epoch: 23 step: 48, loss is 1.3082984685897827\n",
      "epoch: 23 step: 49, loss is 1.1889777183532715\n",
      "epoch: 23 step: 50, loss is 1.3070895671844482\n",
      "epoch: 23 step: 51, loss is 1.2366001605987549\n",
      "epoch: 23 step: 52, loss is 1.2091152667999268\n",
      "epoch: 23 step: 53, loss is 1.2652745246887207\n",
      "epoch: 23 step: 54, loss is 1.2324799299240112\n",
      "epoch: 23 step: 55, loss is 1.155694603919983\n",
      "epoch: 23 step: 56, loss is 1.146004319190979\n",
      "epoch: 23 step: 57, loss is 1.2309329509735107\n",
      "epoch: 23 step: 58, loss is 1.2216533422470093\n",
      "epoch: 23 step: 59, loss is 1.1996583938598633\n",
      "epoch: 23 step: 60, loss is 1.2215495109558105\n",
      "epoch: 23 step: 61, loss is 1.2382895946502686\n",
      "epoch: 23 step: 62, loss is 1.2796499729156494\n",
      "epoch: 23 step: 63, loss is 1.1445553302764893\n",
      "epoch: 23 step: 64, loss is 1.3042116165161133\n",
      "epoch: 23 step: 65, loss is 1.2654982805252075\n",
      "epoch: 23 step: 66, loss is 1.230431079864502\n",
      "epoch: 23 step: 67, loss is 1.080177903175354\n",
      "epoch: 23 step: 68, loss is 1.2016366720199585\n",
      "epoch: 23 step: 69, loss is 1.157483696937561\n",
      "epoch: 23 step: 70, loss is 1.1481139659881592\n",
      "epoch: 23 step: 71, loss is 1.1666889190673828\n",
      "epoch: 23 step: 72, loss is 1.2301701307296753\n",
      "epoch: 23 step: 73, loss is 1.1816216707229614\n",
      "epoch: 23 step: 74, loss is 1.1142277717590332\n",
      "epoch: 23 step: 75, loss is 1.1871005296707153\n",
      "epoch: 23 step: 76, loss is 1.06178617477417\n",
      "epoch: 23 step: 77, loss is 1.1819713115692139\n",
      "epoch: 23 step: 78, loss is 1.1079254150390625\n",
      "epoch: 23 step: 79, loss is 1.1736812591552734\n",
      "epoch: 23 step: 80, loss is 1.1165510416030884\n",
      "epoch: 23 step: 81, loss is 1.2324028015136719\n",
      "epoch: 23 step: 82, loss is 1.142117977142334\n",
      "epoch: 23 step: 83, loss is 1.2354966402053833\n",
      "epoch: 23 step: 84, loss is 1.098087191581726\n",
      "epoch: 23 step: 85, loss is 1.1841059923171997\n",
      "epoch: 23 step: 86, loss is 1.2614191770553589\n",
      "epoch: 23 step: 87, loss is 1.1873278617858887\n",
      "epoch: 23 step: 88, loss is 1.1955246925354004\n",
      "epoch: 23 step: 89, loss is 1.0807862281799316\n",
      "epoch: 23 step: 90, loss is 1.0888466835021973\n",
      "epoch: 23 step: 91, loss is 1.0658974647521973\n",
      "epoch: 23 step: 92, loss is 1.166032314300537\n",
      "epoch: 23 step: 93, loss is 1.2757210731506348\n",
      "epoch: 23 step: 94, loss is 1.1784937381744385\n",
      "epoch: 23 step: 95, loss is 1.2060333490371704\n",
      "epoch: 23 step: 96, loss is 1.2307956218719482\n",
      "epoch: 23 step: 97, loss is 1.2791101932525635\n",
      "epoch: 23 step: 98, loss is 1.2546584606170654\n",
      "epoch: 23 step: 99, loss is 1.1717584133148193\n",
      "epoch: 23 step: 100, loss is 1.1818315982818604\n",
      "epoch: 23 step: 101, loss is 1.0670732259750366\n",
      "epoch: 23 step: 102, loss is 1.229431390762329\n",
      "epoch: 23 step: 103, loss is 1.3128643035888672\n",
      "epoch: 23 step: 104, loss is 1.1409677267074585\n",
      "epoch: 23 step: 105, loss is 1.2474679946899414\n",
      "epoch: 23 step: 106, loss is 1.131298542022705\n",
      "epoch: 23 step: 107, loss is 1.1957767009735107\n",
      "epoch: 23 step: 108, loss is 1.1526286602020264\n",
      "epoch: 23 step: 109, loss is 1.1243990659713745\n",
      "epoch: 23 step: 110, loss is 1.2806891202926636\n",
      "epoch: 23 step: 111, loss is 1.169062614440918\n",
      "epoch: 23 step: 112, loss is 1.1704113483428955\n",
      "epoch: 23 step: 113, loss is 1.2284870147705078\n",
      "epoch: 23 step: 114, loss is 1.2362138032913208\n",
      "epoch: 23 step: 115, loss is 1.2005270719528198\n",
      "epoch: 23 step: 116, loss is 1.2398595809936523\n",
      "epoch: 23 step: 117, loss is 1.2319018840789795\n",
      "epoch: 23 step: 118, loss is 1.2325377464294434\n",
      "epoch: 23 step: 119, loss is 1.2034302949905396\n",
      "epoch: 23 step: 120, loss is 1.1682156324386597\n",
      "epoch: 23 step: 121, loss is 1.1355149745941162\n",
      "epoch: 23 step: 122, loss is 1.1343709230422974\n",
      "epoch: 23 step: 123, loss is 1.2408535480499268\n",
      "epoch: 23 step: 124, loss is 1.2041089534759521\n",
      "epoch: 23 step: 125, loss is 1.157104253768921\n",
      "epoch: 23 step: 126, loss is 1.2544913291931152\n",
      "epoch: 23 step: 127, loss is 1.1332542896270752\n",
      "epoch: 23 step: 128, loss is 1.228111743927002\n",
      "epoch: 23 step: 129, loss is 1.289198637008667\n",
      "epoch: 23 step: 130, loss is 1.1711814403533936\n",
      "epoch: 23 step: 131, loss is 1.2345877885818481\n",
      "epoch: 23 step: 132, loss is 1.0816564559936523\n",
      "epoch: 23 step: 133, loss is 1.1854267120361328\n",
      "epoch: 23 step: 134, loss is 1.3659237623214722\n",
      "epoch: 23 step: 135, loss is 1.159263014793396\n",
      "epoch: 23 step: 136, loss is 1.2933980226516724\n",
      "epoch: 23 step: 137, loss is 1.3125704526901245\n",
      "epoch: 23 step: 138, loss is 1.1801985502243042\n",
      "epoch: 23 step: 139, loss is 1.1973923444747925\n",
      "epoch: 23 step: 140, loss is 1.1884596347808838\n",
      "epoch: 23 step: 141, loss is 1.3395742177963257\n",
      "epoch: 23 step: 142, loss is 1.40288507938385\n",
      "epoch: 23 step: 143, loss is 1.2566101551055908\n",
      "epoch: 23 step: 144, loss is 1.169974446296692\n",
      "epoch: 23 step: 145, loss is 1.2539894580841064\n",
      "epoch: 23 step: 146, loss is 1.200458288192749\n",
      "epoch: 23 step: 147, loss is 1.0851798057556152\n",
      "epoch: 23 step: 148, loss is 1.2843317985534668\n",
      "epoch: 23 step: 149, loss is 1.2645864486694336\n",
      "epoch: 23 step: 150, loss is 1.1450283527374268\n",
      "epoch: 23 step: 151, loss is 1.1424309015274048\n",
      "epoch: 23 step: 152, loss is 1.1687333583831787\n",
      "epoch: 23 step: 153, loss is 1.1952241659164429\n",
      "epoch: 23 step: 154, loss is 1.3449074029922485\n",
      "epoch: 23 step: 155, loss is 1.1909799575805664\n",
      "epoch: 23 step: 156, loss is 1.216642141342163\n",
      "epoch: 23 step: 157, loss is 1.232061505317688\n",
      "epoch: 23 step: 158, loss is 1.2791235446929932\n",
      "epoch: 23 step: 159, loss is 1.238114356994629\n",
      "epoch: 23 step: 160, loss is 1.1058229207992554\n",
      "epoch: 23 step: 161, loss is 1.1796799898147583\n",
      "epoch: 23 step: 162, loss is 1.1705994606018066\n",
      "epoch: 23 step: 163, loss is 1.1968791484832764\n",
      "epoch: 23 step: 164, loss is 1.1899802684783936\n",
      "epoch: 23 step: 165, loss is 1.234274983406067\n",
      "epoch: 23 step: 166, loss is 1.0359315872192383\n",
      "epoch: 23 step: 167, loss is 1.0967657566070557\n",
      "epoch: 23 step: 168, loss is 1.1461575031280518\n",
      "epoch: 23 step: 169, loss is 1.2694520950317383\n",
      "epoch: 23 step: 170, loss is 1.2110958099365234\n",
      "epoch: 23 step: 171, loss is 1.1701444387435913\n",
      "epoch: 23 step: 172, loss is 1.19199538230896\n",
      "epoch: 23 step: 173, loss is 1.2644644975662231\n",
      "epoch: 23 step: 174, loss is 1.0798674821853638\n",
      "epoch: 23 step: 175, loss is 1.1191637516021729\n",
      "epoch: 23 step: 176, loss is 1.2921602725982666\n",
      "epoch: 23 step: 177, loss is 1.2399892807006836\n",
      "epoch: 23 step: 178, loss is 1.2003377676010132\n",
      "epoch: 23 step: 179, loss is 1.1035985946655273\n",
      "epoch: 23 step: 180, loss is 1.1321299076080322\n",
      "epoch: 23 step: 181, loss is 1.224807620048523\n",
      "epoch: 23 step: 182, loss is 1.1365997791290283\n",
      "epoch: 23 step: 183, loss is 1.0914188623428345\n",
      "epoch: 23 step: 184, loss is 1.166756510734558\n",
      "epoch: 23 step: 185, loss is 1.235710859298706\n",
      "epoch: 23 step: 186, loss is 1.2109144926071167\n",
      "epoch: 23 step: 187, loss is 1.1023311614990234\n",
      "epoch: 23 step: 188, loss is 1.1315455436706543\n",
      "epoch: 23 step: 189, loss is 1.180267095565796\n",
      "epoch: 23 step: 190, loss is 1.3210997581481934\n",
      "epoch: 23 step: 191, loss is 1.1789530515670776\n",
      "epoch: 23 step: 192, loss is 1.2304606437683105\n",
      "epoch: 23 step: 193, loss is 1.0863208770751953\n",
      "epoch: 23 step: 194, loss is 1.105311632156372\n",
      "epoch: 23 step: 195, loss is 1.1624606847763062\n",
      "epoch: 23 step: 196, loss is 1.1631126403808594\n",
      "epoch: 23 step: 197, loss is 1.2036209106445312\n",
      "epoch: 23 step: 198, loss is 1.097044587135315\n",
      "epoch: 23 step: 199, loss is 1.0805765390396118\n",
      "epoch: 23 step: 200, loss is 1.2213624715805054\n",
      "epoch: 23 step: 201, loss is 1.2062052488327026\n",
      "epoch: 23 step: 202, loss is 1.0820274353027344\n",
      "epoch: 23 step: 203, loss is 1.237709403038025\n",
      "epoch: 23 step: 204, loss is 1.270348072052002\n",
      "epoch: 23 step: 205, loss is 1.2909722328186035\n",
      "epoch: 23 step: 206, loss is 1.2359769344329834\n",
      "epoch: 23 step: 207, loss is 1.017501950263977\n",
      "epoch: 23 step: 208, loss is 1.1648902893066406\n",
      "epoch: 23 step: 209, loss is 1.145302653312683\n",
      "epoch: 23 step: 210, loss is 1.339317798614502\n",
      "epoch: 23 step: 211, loss is 1.0869224071502686\n",
      "epoch: 23 step: 212, loss is 1.1952012777328491\n",
      "epoch: 23 step: 213, loss is 1.167462706565857\n",
      "epoch: 23 step: 214, loss is 1.2026698589324951\n",
      "epoch: 23 step: 215, loss is 1.1755214929580688\n",
      "epoch: 23 step: 216, loss is 1.2778494358062744\n",
      "epoch: 23 step: 217, loss is 1.186779499053955\n",
      "epoch: 23 step: 218, loss is 1.1571121215820312\n",
      "epoch: 23 step: 219, loss is 1.2813297510147095\n",
      "epoch: 23 step: 220, loss is 1.0289076566696167\n",
      "epoch: 23 step: 221, loss is 1.1847870349884033\n",
      "epoch: 23 step: 222, loss is 1.182787537574768\n",
      "epoch: 23 step: 223, loss is 1.1436491012573242\n",
      "epoch: 23 step: 224, loss is 1.2318685054779053\n",
      "epoch: 23 step: 225, loss is 1.2497119903564453\n",
      "epoch: 23 step: 226, loss is 1.2796828746795654\n",
      "epoch: 23 step: 227, loss is 1.2800089120864868\n",
      "epoch: 23 step: 228, loss is 1.1733015775680542\n",
      "epoch: 23 step: 229, loss is 1.271632432937622\n",
      "epoch: 23 step: 230, loss is 1.22666335105896\n",
      "epoch: 23 step: 231, loss is 1.1141180992126465\n",
      "epoch: 23 step: 232, loss is 1.2293334007263184\n",
      "epoch: 23 step: 233, loss is 1.1850665807724\n",
      "epoch: 23 step: 234, loss is 1.185778021812439\n",
      "epoch: 23 step: 235, loss is 1.1487817764282227\n",
      "epoch: 23 step: 236, loss is 1.0849146842956543\n",
      "epoch: 23 step: 237, loss is 1.1435959339141846\n",
      "epoch: 23 step: 238, loss is 1.2286992073059082\n",
      "epoch: 23 step: 239, loss is 1.2049224376678467\n",
      "epoch: 23 step: 240, loss is 1.2276592254638672\n",
      "epoch: 23 step: 241, loss is 1.2369686365127563\n",
      "epoch: 23 step: 242, loss is 1.0862852334976196\n",
      "epoch: 23 step: 243, loss is 1.2123979330062866\n",
      "epoch: 23 step: 244, loss is 1.060422658920288\n",
      "epoch: 23 step: 245, loss is 1.146162986755371\n",
      "epoch: 23 step: 246, loss is 1.1528465747833252\n",
      "epoch: 23 step: 247, loss is 1.3329468965530396\n",
      "epoch: 23 step: 248, loss is 1.2375719547271729\n",
      "epoch: 23 step: 249, loss is 1.184285044670105\n",
      "epoch: 23 step: 250, loss is 1.2624074220657349\n",
      "epoch: 23 step: 251, loss is 1.1923141479492188\n",
      "epoch: 23 step: 252, loss is 1.091062068939209\n",
      "epoch: 23 step: 253, loss is 1.2050549983978271\n",
      "epoch: 23 step: 254, loss is 1.0800867080688477\n",
      "epoch: 23 step: 255, loss is 1.1277929544448853\n",
      "epoch: 23 step: 256, loss is 1.2253375053405762\n",
      "epoch: 23 step: 257, loss is 1.159860372543335\n",
      "epoch: 23 step: 258, loss is 1.1542164087295532\n",
      "epoch: 23 step: 259, loss is 1.2324951887130737\n",
      "epoch: 23 step: 260, loss is 1.2456315755844116\n",
      "epoch: 23 step: 261, loss is 1.0934561491012573\n",
      "epoch: 23 step: 262, loss is 1.169030785560608\n",
      "epoch: 23 step: 263, loss is 1.1732097864151\n",
      "epoch: 23 step: 264, loss is 1.1744581460952759\n",
      "epoch: 23 step: 265, loss is 1.1676535606384277\n",
      "epoch: 23 step: 266, loss is 1.1416672468185425\n",
      "epoch: 23 step: 267, loss is 1.2706881761550903\n",
      "epoch: 23 step: 268, loss is 1.3523926734924316\n",
      "epoch: 23 step: 269, loss is 1.082204818725586\n",
      "epoch: 23 step: 270, loss is 1.256915807723999\n",
      "epoch: 23 step: 271, loss is 1.241209626197815\n",
      "epoch: 23 step: 272, loss is 1.2027331590652466\n",
      "epoch: 23 step: 273, loss is 1.2252037525177002\n",
      "epoch: 23 step: 274, loss is 1.1626657247543335\n",
      "epoch: 23 step: 275, loss is 1.2332834005355835\n",
      "epoch: 23 step: 276, loss is 1.1875512599945068\n",
      "epoch: 23 step: 277, loss is 1.1206085681915283\n",
      "epoch: 23 step: 278, loss is 1.112855315208435\n",
      "epoch: 23 step: 279, loss is 1.0527353286743164\n",
      "epoch: 23 step: 280, loss is 1.2234132289886475\n",
      "epoch: 23 step: 281, loss is 1.2218973636627197\n",
      "epoch: 23 step: 282, loss is 1.3412038087844849\n",
      "epoch: 23 step: 283, loss is 1.2163584232330322\n",
      "epoch: 23 step: 284, loss is 1.1557420492172241\n",
      "epoch: 23 step: 285, loss is 1.3365979194641113\n",
      "epoch: 23 step: 286, loss is 1.2772165536880493\n",
      "epoch: 23 step: 287, loss is 1.274871826171875\n",
      "epoch: 23 step: 288, loss is 1.1171386241912842\n",
      "epoch: 23 step: 289, loss is 1.1633124351501465\n",
      "epoch: 23 step: 290, loss is 1.1691123247146606\n",
      "epoch: 23 step: 291, loss is 1.3571369647979736\n",
      "epoch: 23 step: 292, loss is 1.1305135488510132\n",
      "epoch: 23 step: 293, loss is 1.271094560623169\n",
      "epoch: 23 step: 294, loss is 1.133091688156128\n",
      "epoch: 23 step: 295, loss is 1.260998010635376\n",
      "epoch: 23 step: 296, loss is 1.3299530744552612\n",
      "epoch: 23 step: 297, loss is 1.2028201818466187\n",
      "epoch: 23 step: 298, loss is 1.233528971672058\n",
      "epoch: 23 step: 299, loss is 1.1144499778747559\n",
      "epoch: 23 step: 300, loss is 1.146116852760315\n",
      "epoch: 23 step: 301, loss is 1.2050490379333496\n",
      "epoch: 23 step: 302, loss is 1.1476457118988037\n",
      "epoch: 23 step: 303, loss is 1.1090400218963623\n",
      "epoch: 23 step: 304, loss is 1.0804389715194702\n",
      "epoch: 23 step: 305, loss is 1.1772236824035645\n",
      "epoch: 23 step: 306, loss is 1.1625394821166992\n",
      "epoch: 23 step: 307, loss is 1.2262566089630127\n",
      "epoch: 23 step: 308, loss is 1.4197430610656738\n",
      "epoch: 23 step: 309, loss is 1.2578660249710083\n",
      "epoch: 23 step: 310, loss is 1.2822701930999756\n",
      "epoch: 23 step: 311, loss is 1.1486870050430298\n",
      "epoch: 23 step: 312, loss is 1.0325615406036377\n",
      "epoch: 23 step: 313, loss is 1.3000446557998657\n",
      "epoch: 23 step: 314, loss is 1.190058708190918\n",
      "epoch: 23 step: 315, loss is 1.2989630699157715\n",
      "epoch: 23 step: 316, loss is 1.105870246887207\n",
      "epoch: 23 step: 317, loss is 1.0473204851150513\n",
      "epoch: 23 step: 318, loss is 1.180042028427124\n",
      "epoch: 23 step: 319, loss is 1.2809809446334839\n",
      "epoch: 23 step: 320, loss is 1.1769434213638306\n",
      "epoch: 23 step: 321, loss is 1.2988522052764893\n",
      "epoch: 23 step: 322, loss is 1.2311041355133057\n",
      "epoch: 23 step: 323, loss is 1.213314414024353\n",
      "epoch: 23 step: 324, loss is 1.2250845432281494\n",
      "epoch: 23 step: 325, loss is 1.1123833656311035\n",
      "epoch: 23 step: 326, loss is 1.1342666149139404\n",
      "epoch: 23 step: 327, loss is 1.214759349822998\n",
      "epoch: 23 step: 328, loss is 1.279332160949707\n",
      "epoch: 23 step: 329, loss is 1.1022945642471313\n",
      "epoch: 23 step: 330, loss is 1.2671618461608887\n",
      "epoch: 23 step: 331, loss is 1.2181921005249023\n",
      "epoch: 23 step: 332, loss is 1.1191123723983765\n",
      "epoch: 23 step: 333, loss is 1.1754621267318726\n",
      "epoch: 23 step: 334, loss is 1.151282787322998\n",
      "epoch: 23 step: 335, loss is 1.2586534023284912\n",
      "epoch: 23 step: 336, loss is 1.313630223274231\n",
      "epoch: 23 step: 337, loss is 1.1892855167388916\n",
      "epoch: 23 step: 338, loss is 1.2431682348251343\n",
      "epoch: 23 step: 339, loss is 1.190427303314209\n",
      "epoch: 23 step: 340, loss is 1.3257776498794556\n",
      "epoch: 23 step: 341, loss is 1.1494497060775757\n",
      "epoch: 23 step: 342, loss is 1.0230765342712402\n",
      "epoch: 23 step: 343, loss is 1.189726710319519\n",
      "epoch: 23 step: 344, loss is 1.163187026977539\n",
      "epoch: 23 step: 345, loss is 1.1820309162139893\n",
      "epoch: 23 step: 346, loss is 1.1322405338287354\n",
      "epoch: 23 step: 347, loss is 1.2059547901153564\n",
      "epoch: 23 step: 348, loss is 1.1346324682235718\n",
      "epoch: 23 step: 349, loss is 1.2016892433166504\n",
      "epoch: 23 step: 350, loss is 1.3000366687774658\n",
      "epoch: 23 step: 351, loss is 1.170964241027832\n",
      "epoch: 23 step: 352, loss is 1.1820034980773926\n",
      "epoch: 23 step: 353, loss is 1.3083666563034058\n",
      "epoch: 23 step: 354, loss is 1.2339074611663818\n",
      "epoch: 23 step: 355, loss is 1.0626435279846191\n",
      "epoch: 23 step: 356, loss is 1.115190863609314\n",
      "epoch: 23 step: 357, loss is 1.2763023376464844\n",
      "epoch: 23 step: 358, loss is 1.105370044708252\n",
      "epoch: 23 step: 359, loss is 1.1612187623977661\n",
      "epoch: 23 step: 360, loss is 1.2459688186645508\n",
      "epoch: 23 step: 361, loss is 1.321312427520752\n",
      "epoch: 23 step: 362, loss is 1.314872145652771\n",
      "epoch: 23 step: 363, loss is 1.1697468757629395\n",
      "epoch: 23 step: 364, loss is 1.249161720275879\n",
      "epoch: 23 step: 365, loss is 1.1694425344467163\n",
      "epoch: 23 step: 366, loss is 1.2679628133773804\n",
      "epoch: 23 step: 367, loss is 1.1148210763931274\n",
      "epoch: 23 step: 368, loss is 1.213441014289856\n",
      "epoch: 23 step: 369, loss is 1.1196178197860718\n",
      "epoch: 23 step: 370, loss is 1.2191267013549805\n",
      "epoch: 23 step: 371, loss is 1.2362018823623657\n",
      "epoch: 23 step: 372, loss is 1.1887574195861816\n",
      "epoch: 23 step: 373, loss is 1.0706509351730347\n",
      "epoch: 23 step: 374, loss is 1.2205286026000977\n",
      "epoch: 23 step: 375, loss is 1.1685584783554077\n",
      "epoch: 23 step: 376, loss is 1.1961283683776855\n",
      "epoch: 23 step: 377, loss is 1.1998990774154663\n",
      "epoch: 23 step: 378, loss is 1.1559453010559082\n",
      "epoch: 23 step: 379, loss is 1.1941373348236084\n",
      "epoch: 23 step: 380, loss is 1.2702107429504395\n",
      "epoch: 23 step: 381, loss is 1.1375564336776733\n",
      "epoch: 23 step: 382, loss is 1.1853729486465454\n",
      "epoch: 23 step: 383, loss is 1.432767391204834\n",
      "epoch: 23 step: 384, loss is 1.2811036109924316\n",
      "epoch: 23 step: 385, loss is 1.17099928855896\n",
      "epoch: 23 step: 386, loss is 1.0936377048492432\n",
      "epoch: 23 step: 387, loss is 1.163310170173645\n",
      "epoch: 23 step: 388, loss is 1.1618083715438843\n",
      "epoch: 23 step: 389, loss is 1.0740165710449219\n",
      "epoch: 23 step: 390, loss is 1.202309489250183\n",
      "Train epoch time: 171900.150 ms, per step time: 440.770 ms\n",
      "epoch: 24 step: 1, loss is 1.1478352546691895\n",
      "epoch: 24 step: 2, loss is 1.2225863933563232\n",
      "epoch: 24 step: 3, loss is 1.1664278507232666\n",
      "epoch: 24 step: 4, loss is 1.2633079290390015\n",
      "epoch: 24 step: 5, loss is 1.1840764284133911\n",
      "epoch: 24 step: 6, loss is 1.0433493852615356\n",
      "epoch: 24 step: 7, loss is 1.137898564338684\n",
      "epoch: 24 step: 8, loss is 1.2162573337554932\n",
      "epoch: 24 step: 9, loss is 1.2049504518508911\n",
      "epoch: 24 step: 10, loss is 1.184836983680725\n",
      "epoch: 24 step: 11, loss is 1.1122139692306519\n",
      "epoch: 24 step: 12, loss is 1.1311023235321045\n",
      "epoch: 24 step: 13, loss is 1.0899269580841064\n",
      "epoch: 24 step: 14, loss is 1.2153187990188599\n",
      "epoch: 24 step: 15, loss is 1.2257933616638184\n",
      "epoch: 24 step: 16, loss is 1.0758967399597168\n",
      "epoch: 24 step: 17, loss is 1.07575261592865\n",
      "epoch: 24 step: 18, loss is 1.1878188848495483\n",
      "epoch: 24 step: 19, loss is 1.1996712684631348\n",
      "epoch: 24 step: 20, loss is 1.195703387260437\n",
      "epoch: 24 step: 21, loss is 1.169721245765686\n",
      "epoch: 24 step: 22, loss is 1.188747525215149\n",
      "epoch: 24 step: 23, loss is 1.2669246196746826\n",
      "epoch: 24 step: 24, loss is 1.056952953338623\n",
      "epoch: 24 step: 25, loss is 1.1165657043457031\n",
      "epoch: 24 step: 26, loss is 1.2144107818603516\n",
      "epoch: 24 step: 27, loss is 1.212458848953247\n",
      "epoch: 24 step: 28, loss is 1.21708345413208\n",
      "epoch: 24 step: 29, loss is 1.1934208869934082\n",
      "epoch: 24 step: 30, loss is 1.0828850269317627\n",
      "epoch: 24 step: 31, loss is 1.2676966190338135\n",
      "epoch: 24 step: 32, loss is 1.1987121105194092\n",
      "epoch: 24 step: 33, loss is 1.0930918455123901\n",
      "epoch: 24 step: 34, loss is 1.155451774597168\n",
      "epoch: 24 step: 35, loss is 1.1352447271347046\n",
      "epoch: 24 step: 36, loss is 1.1488392353057861\n",
      "epoch: 24 step: 37, loss is 1.0649635791778564\n",
      "epoch: 24 step: 38, loss is 1.1580479145050049\n",
      "epoch: 24 step: 39, loss is 1.1143803596496582\n",
      "epoch: 24 step: 40, loss is 1.2702680826187134\n",
      "epoch: 24 step: 41, loss is 1.1274222135543823\n",
      "epoch: 24 step: 42, loss is 1.1874102354049683\n",
      "epoch: 24 step: 43, loss is 1.2334259748458862\n",
      "epoch: 24 step: 44, loss is 1.1726207733154297\n",
      "epoch: 24 step: 45, loss is 1.1706715822219849\n",
      "epoch: 24 step: 46, loss is 1.0644795894622803\n",
      "epoch: 24 step: 47, loss is 1.2458146810531616\n",
      "epoch: 24 step: 48, loss is 1.072768211364746\n",
      "epoch: 24 step: 49, loss is 1.2198008298873901\n",
      "epoch: 24 step: 50, loss is 1.200675368309021\n",
      "epoch: 24 step: 51, loss is 1.2815089225769043\n",
      "epoch: 24 step: 52, loss is 1.2759686708450317\n",
      "epoch: 24 step: 53, loss is 1.1815041303634644\n",
      "epoch: 24 step: 54, loss is 1.1004903316497803\n",
      "epoch: 24 step: 55, loss is 1.2608798742294312\n",
      "epoch: 24 step: 56, loss is 1.1199225187301636\n",
      "epoch: 24 step: 57, loss is 1.1940956115722656\n",
      "epoch: 24 step: 58, loss is 1.2983040809631348\n",
      "epoch: 24 step: 59, loss is 1.1563785076141357\n",
      "epoch: 24 step: 60, loss is 1.292327642440796\n",
      "epoch: 24 step: 61, loss is 1.194291353225708\n",
      "epoch: 24 step: 62, loss is 1.1578943729400635\n",
      "epoch: 24 step: 63, loss is 1.1733942031860352\n",
      "epoch: 24 step: 64, loss is 1.2754480838775635\n",
      "epoch: 24 step: 65, loss is 1.079789638519287\n",
      "epoch: 24 step: 66, loss is 1.252091407775879\n",
      "epoch: 24 step: 67, loss is 1.2059917449951172\n",
      "epoch: 24 step: 68, loss is 1.2102793455123901\n",
      "epoch: 24 step: 69, loss is 1.2226077318191528\n",
      "epoch: 24 step: 70, loss is 1.115647554397583\n",
      "epoch: 24 step: 71, loss is 1.1759573221206665\n",
      "epoch: 24 step: 72, loss is 1.2300124168395996\n",
      "epoch: 24 step: 73, loss is 1.2995009422302246\n",
      "epoch: 24 step: 74, loss is 1.2475388050079346\n",
      "epoch: 24 step: 75, loss is 1.1306447982788086\n",
      "epoch: 24 step: 76, loss is 1.178666353225708\n",
      "epoch: 24 step: 77, loss is 1.1083507537841797\n",
      "epoch: 24 step: 78, loss is 1.1838818788528442\n",
      "epoch: 24 step: 79, loss is 1.1814155578613281\n",
      "epoch: 24 step: 80, loss is 1.2734973430633545\n",
      "epoch: 24 step: 81, loss is 1.2934620380401611\n",
      "epoch: 24 step: 82, loss is 1.1244993209838867\n",
      "epoch: 24 step: 83, loss is 1.1586984395980835\n",
      "epoch: 24 step: 84, loss is 1.3285831212997437\n",
      "epoch: 24 step: 85, loss is 1.3501591682434082\n",
      "epoch: 24 step: 86, loss is 1.2383743524551392\n",
      "epoch: 24 step: 87, loss is 1.2372127771377563\n",
      "epoch: 24 step: 88, loss is 1.0950433015823364\n",
      "epoch: 24 step: 89, loss is 1.187055230140686\n",
      "epoch: 24 step: 90, loss is 1.1735076904296875\n",
      "epoch: 24 step: 91, loss is 1.245962381362915\n",
      "epoch: 24 step: 92, loss is 1.1316672563552856\n",
      "epoch: 24 step: 93, loss is 1.1376891136169434\n",
      "epoch: 24 step: 94, loss is 1.1890569925308228\n",
      "epoch: 24 step: 95, loss is 1.0891960859298706\n",
      "epoch: 24 step: 96, loss is 1.1653203964233398\n",
      "epoch: 24 step: 97, loss is 1.2504475116729736\n",
      "epoch: 24 step: 98, loss is 1.2206461429595947\n",
      "epoch: 24 step: 99, loss is 1.1712536811828613\n",
      "epoch: 24 step: 100, loss is 1.2372233867645264\n",
      "epoch: 24 step: 101, loss is 1.3266555070877075\n",
      "epoch: 24 step: 102, loss is 1.1692572832107544\n",
      "epoch: 24 step: 103, loss is 1.160603642463684\n",
      "epoch: 24 step: 104, loss is 1.1467875242233276\n",
      "epoch: 24 step: 105, loss is 1.0964279174804688\n",
      "epoch: 24 step: 106, loss is 1.2427382469177246\n",
      "epoch: 24 step: 107, loss is 1.1873096227645874\n",
      "epoch: 24 step: 108, loss is 1.0822848081588745\n",
      "epoch: 24 step: 109, loss is 1.1585601568222046\n",
      "epoch: 24 step: 110, loss is 1.1998169422149658\n",
      "epoch: 24 step: 111, loss is 1.1219414472579956\n",
      "epoch: 24 step: 112, loss is 1.2469427585601807\n",
      "epoch: 24 step: 113, loss is 1.1685212850570679\n",
      "epoch: 24 step: 114, loss is 1.125192642211914\n",
      "epoch: 24 step: 115, loss is 1.1723600625991821\n",
      "epoch: 24 step: 116, loss is 1.1701607704162598\n",
      "epoch: 24 step: 117, loss is 1.2552733421325684\n",
      "epoch: 24 step: 118, loss is 1.2145367860794067\n",
      "epoch: 24 step: 119, loss is 1.1804909706115723\n",
      "epoch: 24 step: 120, loss is 1.157945990562439\n",
      "epoch: 24 step: 121, loss is 1.11224365234375\n",
      "epoch: 24 step: 122, loss is 1.2175582647323608\n",
      "epoch: 24 step: 123, loss is 1.1477147340774536\n",
      "epoch: 24 step: 124, loss is 1.1693086624145508\n",
      "epoch: 24 step: 125, loss is 1.3596588373184204\n",
      "epoch: 24 step: 126, loss is 1.1775500774383545\n",
      "epoch: 24 step: 127, loss is 1.1045690774917603\n",
      "epoch: 24 step: 128, loss is 1.3043005466461182\n",
      "epoch: 24 step: 129, loss is 1.2158381938934326\n",
      "epoch: 24 step: 130, loss is 1.2461422681808472\n",
      "epoch: 24 step: 131, loss is 1.1644487380981445\n",
      "epoch: 24 step: 132, loss is 1.2318944931030273\n",
      "epoch: 24 step: 133, loss is 1.1681495904922485\n",
      "epoch: 24 step: 134, loss is 1.0686103105545044\n",
      "epoch: 24 step: 135, loss is 1.1775168180465698\n",
      "epoch: 24 step: 136, loss is 1.2766809463500977\n",
      "epoch: 24 step: 137, loss is 1.1105252504348755\n",
      "epoch: 24 step: 138, loss is 1.0784205198287964\n",
      "epoch: 24 step: 139, loss is 1.0864582061767578\n",
      "epoch: 24 step: 140, loss is 1.1490449905395508\n",
      "epoch: 24 step: 141, loss is 1.0977658033370972\n",
      "epoch: 24 step: 142, loss is 1.137967824935913\n",
      "epoch: 24 step: 143, loss is 1.209519624710083\n",
      "epoch: 24 step: 144, loss is 1.2240365743637085\n",
      "epoch: 24 step: 145, loss is 1.1512469053268433\n",
      "epoch: 24 step: 146, loss is 1.1973069906234741\n",
      "epoch: 24 step: 147, loss is 1.1186318397521973\n",
      "epoch: 24 step: 148, loss is 1.0706894397735596\n",
      "epoch: 24 step: 149, loss is 1.1953518390655518\n",
      "epoch: 24 step: 150, loss is 1.1511695384979248\n",
      "epoch: 24 step: 151, loss is 1.2425190210342407\n",
      "epoch: 24 step: 152, loss is 1.1789835691452026\n",
      "epoch: 24 step: 153, loss is 1.1633570194244385\n",
      "epoch: 24 step: 154, loss is 1.1108660697937012\n",
      "epoch: 24 step: 155, loss is 1.2038267850875854\n",
      "epoch: 24 step: 156, loss is 1.1631817817687988\n",
      "epoch: 24 step: 157, loss is 1.271405577659607\n",
      "epoch: 24 step: 158, loss is 1.1061631441116333\n",
      "epoch: 24 step: 159, loss is 1.1811408996582031\n",
      "epoch: 24 step: 160, loss is 1.3374063968658447\n",
      "epoch: 24 step: 161, loss is 1.0221803188323975\n",
      "epoch: 24 step: 162, loss is 1.0937914848327637\n",
      "epoch: 24 step: 163, loss is 1.1873676776885986\n",
      "epoch: 24 step: 164, loss is 1.163464903831482\n",
      "epoch: 24 step: 165, loss is 1.256090760231018\n",
      "epoch: 24 step: 166, loss is 1.191239833831787\n",
      "epoch: 24 step: 167, loss is 1.2756801843643188\n",
      "epoch: 24 step: 168, loss is 1.1864347457885742\n",
      "epoch: 24 step: 169, loss is 1.0614813566207886\n",
      "epoch: 24 step: 170, loss is 1.1354100704193115\n",
      "epoch: 24 step: 171, loss is 1.188250184059143\n",
      "epoch: 24 step: 172, loss is 1.2062715291976929\n",
      "epoch: 24 step: 173, loss is 1.1884644031524658\n",
      "epoch: 24 step: 174, loss is 1.2260159254074097\n",
      "epoch: 24 step: 175, loss is 1.115903377532959\n",
      "epoch: 24 step: 176, loss is 1.2345809936523438\n",
      "epoch: 24 step: 177, loss is 1.157949686050415\n",
      "epoch: 24 step: 178, loss is 1.309206247329712\n",
      "epoch: 24 step: 179, loss is 1.1108051538467407\n",
      "epoch: 24 step: 180, loss is 1.1797837018966675\n",
      "epoch: 24 step: 181, loss is 1.179606556892395\n",
      "epoch: 24 step: 182, loss is 1.2178664207458496\n",
      "epoch: 24 step: 183, loss is 1.1713268756866455\n",
      "epoch: 24 step: 184, loss is 1.2096879482269287\n",
      "epoch: 24 step: 185, loss is 1.0525246858596802\n",
      "epoch: 24 step: 186, loss is 1.056269645690918\n",
      "epoch: 24 step: 187, loss is 1.2094484567642212\n",
      "epoch: 24 step: 188, loss is 1.1160657405853271\n",
      "epoch: 24 step: 189, loss is 1.1174248456954956\n",
      "epoch: 24 step: 190, loss is 1.1827853918075562\n",
      "epoch: 24 step: 191, loss is 1.1122536659240723\n",
      "epoch: 24 step: 192, loss is 1.189181923866272\n",
      "epoch: 24 step: 193, loss is 1.1969103813171387\n",
      "epoch: 24 step: 194, loss is 1.2836161851882935\n",
      "epoch: 24 step: 195, loss is 1.1191718578338623\n",
      "epoch: 24 step: 196, loss is 1.2081859111785889\n",
      "epoch: 24 step: 197, loss is 1.155005931854248\n",
      "epoch: 24 step: 198, loss is 1.29423189163208\n",
      "epoch: 24 step: 199, loss is 1.1658028364181519\n",
      "epoch: 24 step: 200, loss is 1.0960230827331543\n",
      "epoch: 24 step: 201, loss is 1.256270170211792\n",
      "epoch: 24 step: 202, loss is 1.1291123628616333\n",
      "epoch: 24 step: 203, loss is 1.2996245622634888\n",
      "epoch: 24 step: 204, loss is 1.1609066724777222\n",
      "epoch: 24 step: 205, loss is 1.2707836627960205\n",
      "epoch: 24 step: 206, loss is 1.1690762042999268\n",
      "epoch: 24 step: 207, loss is 1.1410545110702515\n",
      "epoch: 24 step: 208, loss is 1.09125554561615\n",
      "epoch: 24 step: 209, loss is 1.1174685955047607\n",
      "epoch: 24 step: 210, loss is 1.2656104564666748\n",
      "epoch: 24 step: 211, loss is 1.2639787197113037\n",
      "epoch: 24 step: 212, loss is 1.2911101579666138\n",
      "epoch: 24 step: 213, loss is 1.1079000234603882\n",
      "epoch: 24 step: 214, loss is 1.1046421527862549\n",
      "epoch: 24 step: 215, loss is 1.1919878721237183\n",
      "epoch: 24 step: 216, loss is 1.1815028190612793\n",
      "epoch: 24 step: 217, loss is 1.281757116317749\n",
      "epoch: 24 step: 218, loss is 1.110102891921997\n",
      "epoch: 24 step: 219, loss is 1.1597607135772705\n",
      "epoch: 24 step: 220, loss is 1.208267331123352\n",
      "epoch: 24 step: 221, loss is 1.223457932472229\n",
      "epoch: 24 step: 222, loss is 1.250765085220337\n",
      "epoch: 24 step: 223, loss is 1.20215904712677\n",
      "epoch: 24 step: 224, loss is 1.1623387336730957\n",
      "epoch: 24 step: 225, loss is 1.2267826795578003\n",
      "epoch: 24 step: 226, loss is 1.2135207653045654\n",
      "epoch: 24 step: 227, loss is 1.0503588914871216\n",
      "epoch: 24 step: 228, loss is 1.14544677734375\n",
      "epoch: 24 step: 229, loss is 1.1693891286849976\n",
      "epoch: 24 step: 230, loss is 1.1834758520126343\n",
      "epoch: 24 step: 231, loss is 1.1576130390167236\n",
      "epoch: 24 step: 232, loss is 1.119384527206421\n",
      "epoch: 24 step: 233, loss is 1.066116213798523\n",
      "epoch: 24 step: 234, loss is 1.138038158416748\n",
      "epoch: 24 step: 235, loss is 1.2030866146087646\n",
      "epoch: 24 step: 236, loss is 1.1345388889312744\n",
      "epoch: 24 step: 237, loss is 1.1795871257781982\n",
      "epoch: 24 step: 238, loss is 1.2640854120254517\n",
      "epoch: 24 step: 239, loss is 1.0801780223846436\n",
      "epoch: 24 step: 240, loss is 1.172675371170044\n",
      "epoch: 24 step: 241, loss is 1.3094838857650757\n",
      "epoch: 24 step: 242, loss is 1.1128942966461182\n",
      "epoch: 24 step: 243, loss is 1.1997270584106445\n",
      "epoch: 24 step: 244, loss is 1.2964333295822144\n",
      "epoch: 24 step: 245, loss is 1.1578179597854614\n",
      "epoch: 24 step: 246, loss is 1.213099718093872\n",
      "epoch: 24 step: 247, loss is 1.1818140745162964\n",
      "epoch: 24 step: 248, loss is 1.3067774772644043\n",
      "epoch: 24 step: 249, loss is 1.224569320678711\n",
      "epoch: 24 step: 250, loss is 1.272308111190796\n",
      "epoch: 24 step: 251, loss is 1.185659646987915\n",
      "epoch: 24 step: 252, loss is 1.1239346265792847\n",
      "epoch: 24 step: 253, loss is 1.1912513971328735\n",
      "epoch: 24 step: 254, loss is 1.0777368545532227\n",
      "epoch: 24 step: 255, loss is 1.1351839303970337\n",
      "epoch: 24 step: 256, loss is 1.3041977882385254\n",
      "epoch: 24 step: 257, loss is 1.246925711631775\n",
      "epoch: 24 step: 258, loss is 1.2156462669372559\n",
      "epoch: 24 step: 259, loss is 1.284972906112671\n",
      "epoch: 24 step: 260, loss is 1.2190706729888916\n",
      "epoch: 24 step: 261, loss is 1.2270450592041016\n",
      "epoch: 24 step: 262, loss is 1.2106081247329712\n",
      "epoch: 24 step: 263, loss is 1.2216966152191162\n",
      "epoch: 24 step: 264, loss is 1.1580071449279785\n",
      "epoch: 24 step: 265, loss is 1.2102036476135254\n",
      "epoch: 24 step: 266, loss is 1.1747772693634033\n",
      "epoch: 24 step: 267, loss is 1.2330658435821533\n",
      "epoch: 24 step: 268, loss is 1.1685494184494019\n",
      "epoch: 24 step: 269, loss is 1.2596315145492554\n",
      "epoch: 24 step: 270, loss is 1.2069156169891357\n",
      "epoch: 24 step: 271, loss is 1.207099437713623\n",
      "epoch: 24 step: 272, loss is 1.170763373374939\n",
      "epoch: 24 step: 273, loss is 1.1725231409072876\n",
      "epoch: 24 step: 274, loss is 1.125248670578003\n",
      "epoch: 24 step: 275, loss is 1.1584081649780273\n",
      "epoch: 24 step: 276, loss is 1.240322232246399\n",
      "epoch: 24 step: 277, loss is 1.194503903388977\n",
      "epoch: 24 step: 278, loss is 1.2581424713134766\n",
      "epoch: 24 step: 279, loss is 1.2014353275299072\n",
      "epoch: 24 step: 280, loss is 1.2330236434936523\n",
      "epoch: 24 step: 281, loss is 1.1583011150360107\n",
      "epoch: 24 step: 282, loss is 1.207756519317627\n",
      "epoch: 24 step: 283, loss is 1.0939569473266602\n",
      "epoch: 24 step: 284, loss is 1.2531845569610596\n",
      "epoch: 24 step: 285, loss is 1.2606956958770752\n",
      "epoch: 24 step: 286, loss is 1.0602244138717651\n",
      "epoch: 24 step: 287, loss is 1.2671104669570923\n",
      "epoch: 24 step: 288, loss is 1.1099588871002197\n",
      "epoch: 24 step: 289, loss is 1.1017003059387207\n",
      "epoch: 24 step: 290, loss is 1.0994963645935059\n",
      "epoch: 24 step: 291, loss is 1.2903928756713867\n",
      "epoch: 24 step: 292, loss is 1.2128660678863525\n",
      "epoch: 24 step: 293, loss is 1.1716384887695312\n",
      "epoch: 24 step: 294, loss is 1.193953514099121\n",
      "epoch: 24 step: 295, loss is 1.2467628717422485\n",
      "epoch: 24 step: 296, loss is 1.1044930219650269\n",
      "epoch: 24 step: 297, loss is 1.1449427604675293\n",
      "epoch: 24 step: 298, loss is 1.2551063299179077\n",
      "epoch: 24 step: 299, loss is 1.1731685400009155\n",
      "epoch: 24 step: 300, loss is 1.2805664539337158\n",
      "epoch: 24 step: 301, loss is 1.232069730758667\n",
      "epoch: 24 step: 302, loss is 1.1900100708007812\n",
      "epoch: 24 step: 303, loss is 1.13457190990448\n",
      "epoch: 24 step: 304, loss is 1.1536486148834229\n",
      "epoch: 24 step: 305, loss is 1.1332576274871826\n",
      "epoch: 24 step: 306, loss is 1.2444369792938232\n",
      "epoch: 24 step: 307, loss is 1.280735969543457\n",
      "epoch: 24 step: 308, loss is 1.2360526323318481\n",
      "epoch: 24 step: 309, loss is 1.068451166152954\n",
      "epoch: 24 step: 310, loss is 1.1378700733184814\n",
      "epoch: 24 step: 311, loss is 1.319148063659668\n",
      "epoch: 24 step: 312, loss is 1.1570119857788086\n",
      "epoch: 24 step: 313, loss is 1.3479670286178589\n",
      "epoch: 24 step: 314, loss is 1.180387020111084\n",
      "epoch: 24 step: 315, loss is 1.1105083227157593\n",
      "epoch: 24 step: 316, loss is 1.2516345977783203\n",
      "epoch: 24 step: 317, loss is 1.1998337507247925\n",
      "epoch: 24 step: 318, loss is 1.1058228015899658\n",
      "epoch: 24 step: 319, loss is 1.1729483604431152\n",
      "epoch: 24 step: 320, loss is 1.0267353057861328\n",
      "epoch: 24 step: 321, loss is 1.179305911064148\n",
      "epoch: 24 step: 322, loss is 1.139150857925415\n",
      "epoch: 24 step: 323, loss is 1.1805832386016846\n",
      "epoch: 24 step: 324, loss is 1.2131755352020264\n",
      "epoch: 24 step: 325, loss is 1.1560590267181396\n",
      "epoch: 24 step: 326, loss is 1.2067208290100098\n",
      "epoch: 24 step: 327, loss is 1.0990970134735107\n",
      "epoch: 24 step: 328, loss is 1.207159399986267\n",
      "epoch: 24 step: 329, loss is 1.1815388202667236\n",
      "epoch: 24 step: 330, loss is 1.1787219047546387\n",
      "epoch: 24 step: 331, loss is 1.2359473705291748\n",
      "epoch: 24 step: 332, loss is 1.1234333515167236\n",
      "epoch: 24 step: 333, loss is 1.2348496913909912\n",
      "epoch: 24 step: 334, loss is 1.138744592666626\n",
      "epoch: 24 step: 335, loss is 1.240665078163147\n",
      "epoch: 24 step: 336, loss is 1.2182084321975708\n",
      "epoch: 24 step: 337, loss is 1.1485176086425781\n",
      "epoch: 24 step: 338, loss is 1.2241833209991455\n",
      "epoch: 24 step: 339, loss is 1.1393609046936035\n",
      "epoch: 24 step: 340, loss is 1.1812238693237305\n",
      "epoch: 24 step: 341, loss is 1.0884851217269897\n",
      "epoch: 24 step: 342, loss is 1.2182013988494873\n",
      "epoch: 24 step: 343, loss is 1.2327699661254883\n",
      "epoch: 24 step: 344, loss is 1.3870291709899902\n",
      "epoch: 24 step: 345, loss is 1.2331037521362305\n",
      "epoch: 24 step: 346, loss is 1.21524178981781\n",
      "epoch: 24 step: 347, loss is 1.2582799196243286\n",
      "epoch: 24 step: 348, loss is 1.164081335067749\n",
      "epoch: 24 step: 349, loss is 1.155687689781189\n",
      "epoch: 24 step: 350, loss is 1.278153896331787\n",
      "epoch: 24 step: 351, loss is 1.1790179014205933\n",
      "epoch: 24 step: 352, loss is 1.1158722639083862\n",
      "epoch: 24 step: 353, loss is 1.3110158443450928\n",
      "epoch: 24 step: 354, loss is 1.0917359590530396\n",
      "epoch: 24 step: 355, loss is 1.2569431066513062\n",
      "epoch: 24 step: 356, loss is 1.0666931867599487\n",
      "epoch: 24 step: 357, loss is 1.0324225425720215\n",
      "epoch: 24 step: 358, loss is 1.2909667491912842\n",
      "epoch: 24 step: 359, loss is 1.2484760284423828\n",
      "epoch: 24 step: 360, loss is 1.0781779289245605\n",
      "epoch: 24 step: 361, loss is 1.0561800003051758\n",
      "epoch: 24 step: 362, loss is 1.1309510469436646\n",
      "epoch: 24 step: 363, loss is 1.2489444017410278\n",
      "epoch: 24 step: 364, loss is 1.1641814708709717\n",
      "epoch: 24 step: 365, loss is 1.0520989894866943\n",
      "epoch: 24 step: 366, loss is 1.1475516557693481\n",
      "epoch: 24 step: 367, loss is 1.1391046047210693\n",
      "epoch: 24 step: 368, loss is 1.1103744506835938\n",
      "epoch: 24 step: 369, loss is 1.1733616590499878\n",
      "epoch: 24 step: 370, loss is 1.1471210718154907\n",
      "epoch: 24 step: 371, loss is 1.1420080661773682\n",
      "epoch: 24 step: 372, loss is 1.2092821598052979\n",
      "epoch: 24 step: 373, loss is 1.206122636795044\n",
      "epoch: 24 step: 374, loss is 1.2036840915679932\n",
      "epoch: 24 step: 375, loss is 1.0964815616607666\n",
      "epoch: 24 step: 376, loss is 1.2013144493103027\n",
      "epoch: 24 step: 377, loss is 1.384856104850769\n",
      "epoch: 24 step: 378, loss is 1.1659488677978516\n",
      "epoch: 24 step: 379, loss is 1.1933140754699707\n",
      "epoch: 24 step: 380, loss is 1.0978704690933228\n",
      "epoch: 24 step: 381, loss is 1.195791244506836\n",
      "epoch: 24 step: 382, loss is 1.0961260795593262\n",
      "epoch: 24 step: 383, loss is 1.1006596088409424\n",
      "epoch: 24 step: 384, loss is 1.213979959487915\n",
      "epoch: 24 step: 385, loss is 1.1896787881851196\n",
      "epoch: 24 step: 386, loss is 1.156815767288208\n",
      "epoch: 24 step: 387, loss is 1.3000757694244385\n",
      "epoch: 24 step: 388, loss is 1.19772207736969\n",
      "epoch: 24 step: 389, loss is 1.196083903312683\n",
      "epoch: 24 step: 390, loss is 1.225029468536377\n",
      "Train epoch time: 165508.665 ms, per step time: 424.381 ms\n",
      "epoch: 25 step: 1, loss is 1.2104874849319458\n",
      "epoch: 25 step: 2, loss is 1.227695345878601\n",
      "epoch: 25 step: 3, loss is 1.1822391748428345\n",
      "epoch: 25 step: 4, loss is 1.235782265663147\n",
      "epoch: 25 step: 5, loss is 1.1350911855697632\n",
      "epoch: 25 step: 6, loss is 1.1796836853027344\n",
      "epoch: 25 step: 7, loss is 1.2285624742507935\n",
      "epoch: 25 step: 8, loss is 1.0940525531768799\n",
      "epoch: 25 step: 9, loss is 1.1868854761123657\n",
      "epoch: 25 step: 10, loss is 1.153637409210205\n",
      "epoch: 25 step: 11, loss is 1.1704896688461304\n",
      "epoch: 25 step: 12, loss is 1.1200780868530273\n",
      "epoch: 25 step: 13, loss is 1.1731570959091187\n",
      "epoch: 25 step: 14, loss is 1.1795293092727661\n",
      "epoch: 25 step: 15, loss is 1.2302874326705933\n",
      "epoch: 25 step: 16, loss is 1.2387495040893555\n",
      "epoch: 25 step: 17, loss is 1.093299150466919\n",
      "epoch: 25 step: 18, loss is 1.2508457899093628\n",
      "epoch: 25 step: 19, loss is 1.1586711406707764\n",
      "epoch: 25 step: 20, loss is 1.1816456317901611\n",
      "epoch: 25 step: 21, loss is 1.1469731330871582\n",
      "epoch: 25 step: 22, loss is 1.2433035373687744\n",
      "epoch: 25 step: 23, loss is 1.195704698562622\n",
      "epoch: 25 step: 24, loss is 1.3069489002227783\n",
      "epoch: 25 step: 25, loss is 0.9364129304885864\n",
      "epoch: 25 step: 26, loss is 1.0823748111724854\n",
      "epoch: 25 step: 27, loss is 1.2281255722045898\n",
      "epoch: 25 step: 28, loss is 1.086897850036621\n",
      "epoch: 25 step: 29, loss is 1.0781503915786743\n",
      "epoch: 25 step: 30, loss is 1.2520934343338013\n",
      "epoch: 25 step: 31, loss is 1.1469632387161255\n",
      "epoch: 25 step: 32, loss is 1.1706106662750244\n",
      "epoch: 25 step: 33, loss is 1.1605263948440552\n",
      "epoch: 25 step: 34, loss is 1.135204792022705\n",
      "epoch: 25 step: 35, loss is 1.278445839881897\n",
      "epoch: 25 step: 36, loss is 1.117199420928955\n",
      "epoch: 25 step: 37, loss is 1.1762185096740723\n",
      "epoch: 25 step: 38, loss is 1.0656273365020752\n",
      "epoch: 25 step: 39, loss is 1.2188992500305176\n",
      "epoch: 25 step: 40, loss is 1.1118298768997192\n",
      "epoch: 25 step: 41, loss is 1.1617423295974731\n",
      "epoch: 25 step: 42, loss is 1.1207162141799927\n",
      "epoch: 25 step: 43, loss is 1.0783005952835083\n",
      "epoch: 25 step: 44, loss is 1.200313687324524\n",
      "epoch: 25 step: 45, loss is 1.241788625717163\n",
      "epoch: 25 step: 46, loss is 1.2645450830459595\n",
      "epoch: 25 step: 47, loss is 1.2280158996582031\n",
      "epoch: 25 step: 48, loss is 1.1555094718933105\n",
      "epoch: 25 step: 49, loss is 1.3147658109664917\n",
      "epoch: 25 step: 50, loss is 1.2218958139419556\n",
      "epoch: 25 step: 51, loss is 1.2071789503097534\n",
      "epoch: 25 step: 52, loss is 1.221683144569397\n",
      "epoch: 25 step: 53, loss is 1.3226739168167114\n",
      "epoch: 25 step: 54, loss is 1.2417097091674805\n",
      "epoch: 25 step: 55, loss is 1.2950642108917236\n",
      "epoch: 25 step: 56, loss is 1.2718719244003296\n",
      "epoch: 25 step: 57, loss is 1.113211750984192\n",
      "epoch: 25 step: 58, loss is 1.1882948875427246\n",
      "epoch: 25 step: 59, loss is 1.1824214458465576\n",
      "epoch: 25 step: 60, loss is 1.2577271461486816\n",
      "epoch: 25 step: 61, loss is 1.167454481124878\n",
      "epoch: 25 step: 62, loss is 1.261486291885376\n",
      "epoch: 25 step: 63, loss is 1.0976654291152954\n",
      "epoch: 25 step: 64, loss is 1.181080937385559\n",
      "epoch: 25 step: 65, loss is 1.2552266120910645\n",
      "epoch: 25 step: 66, loss is 1.061601161956787\n",
      "epoch: 25 step: 67, loss is 1.2274726629257202\n",
      "epoch: 25 step: 68, loss is 1.2308411598205566\n",
      "epoch: 25 step: 69, loss is 1.090429663658142\n",
      "epoch: 25 step: 70, loss is 1.0999257564544678\n",
      "epoch: 25 step: 71, loss is 1.1713227033615112\n",
      "epoch: 25 step: 72, loss is 1.097731590270996\n",
      "epoch: 25 step: 73, loss is 1.146206259727478\n",
      "epoch: 25 step: 74, loss is 1.2572864294052124\n",
      "epoch: 25 step: 75, loss is 1.0816872119903564\n",
      "epoch: 25 step: 76, loss is 1.107470989227295\n",
      "epoch: 25 step: 77, loss is 1.2118736505508423\n",
      "epoch: 25 step: 78, loss is 1.1037007570266724\n",
      "epoch: 25 step: 79, loss is 1.2051640748977661\n",
      "epoch: 25 step: 80, loss is 1.0880193710327148\n",
      "epoch: 25 step: 81, loss is 1.1682779788970947\n",
      "epoch: 25 step: 82, loss is 1.1564544439315796\n",
      "epoch: 25 step: 83, loss is 1.2349467277526855\n",
      "epoch: 25 step: 84, loss is 1.344640851020813\n",
      "epoch: 25 step: 85, loss is 1.0467976331710815\n",
      "epoch: 25 step: 86, loss is 1.059446096420288\n",
      "epoch: 25 step: 87, loss is 1.197453260421753\n",
      "epoch: 25 step: 88, loss is 1.2136778831481934\n",
      "epoch: 25 step: 89, loss is 1.1384116411209106\n",
      "epoch: 25 step: 90, loss is 1.2014434337615967\n",
      "epoch: 25 step: 91, loss is 1.2448205947875977\n",
      "epoch: 25 step: 92, loss is 1.1690642833709717\n",
      "epoch: 25 step: 93, loss is 1.1301037073135376\n",
      "epoch: 25 step: 94, loss is 1.2592270374298096\n",
      "epoch: 25 step: 95, loss is 1.1063685417175293\n",
      "epoch: 25 step: 96, loss is 1.2227810621261597\n",
      "epoch: 25 step: 97, loss is 1.1744248867034912\n",
      "epoch: 25 step: 98, loss is 1.2378714084625244\n",
      "epoch: 25 step: 99, loss is 1.1523809432983398\n",
      "epoch: 25 step: 100, loss is 1.0921285152435303\n",
      "epoch: 25 step: 101, loss is 1.1891683340072632\n",
      "epoch: 25 step: 102, loss is 1.2719500064849854\n",
      "epoch: 25 step: 103, loss is 1.0930408239364624\n",
      "epoch: 25 step: 104, loss is 1.1613960266113281\n",
      "epoch: 25 step: 105, loss is 1.1852154731750488\n",
      "epoch: 25 step: 106, loss is 1.1106597185134888\n",
      "epoch: 25 step: 107, loss is 1.1360605955123901\n",
      "epoch: 25 step: 108, loss is 1.1619611978530884\n",
      "epoch: 25 step: 109, loss is 1.0900194644927979\n",
      "epoch: 25 step: 110, loss is 1.1380510330200195\n",
      "epoch: 25 step: 111, loss is 1.305637240409851\n",
      "epoch: 25 step: 112, loss is 1.1737720966339111\n",
      "epoch: 25 step: 113, loss is 1.2194286584854126\n",
      "epoch: 25 step: 114, loss is 1.143774151802063\n",
      "epoch: 25 step: 115, loss is 1.354629397392273\n",
      "epoch: 25 step: 116, loss is 1.0271915197372437\n",
      "epoch: 25 step: 117, loss is 1.2099940776824951\n",
      "epoch: 25 step: 118, loss is 1.1996368169784546\n",
      "epoch: 25 step: 119, loss is 1.2684056758880615\n",
      "epoch: 25 step: 120, loss is 1.104841709136963\n",
      "epoch: 25 step: 121, loss is 1.073075294494629\n",
      "epoch: 25 step: 122, loss is 1.207556128501892\n",
      "epoch: 25 step: 123, loss is 1.1678268909454346\n",
      "epoch: 25 step: 124, loss is 1.1382429599761963\n",
      "epoch: 25 step: 125, loss is 1.1474735736846924\n",
      "epoch: 25 step: 126, loss is 1.1205291748046875\n",
      "epoch: 25 step: 127, loss is 1.103917121887207\n",
      "epoch: 25 step: 128, loss is 1.1678165197372437\n",
      "epoch: 25 step: 129, loss is 1.204624891281128\n",
      "epoch: 25 step: 130, loss is 1.1533219814300537\n",
      "epoch: 25 step: 131, loss is 1.165470838546753\n",
      "epoch: 25 step: 132, loss is 1.147660255432129\n",
      "epoch: 25 step: 133, loss is 1.1173713207244873\n",
      "epoch: 25 step: 134, loss is 1.0695929527282715\n",
      "epoch: 25 step: 135, loss is 1.2350718975067139\n",
      "epoch: 25 step: 136, loss is 1.1549166440963745\n",
      "epoch: 25 step: 137, loss is 1.110823392868042\n",
      "epoch: 25 step: 138, loss is 1.157725214958191\n",
      "epoch: 25 step: 139, loss is 1.1498188972473145\n",
      "epoch: 25 step: 140, loss is 1.2549501657485962\n",
      "epoch: 25 step: 141, loss is 1.123143196105957\n",
      "epoch: 25 step: 142, loss is 1.1860450506210327\n",
      "epoch: 25 step: 143, loss is 1.03868567943573\n",
      "epoch: 25 step: 144, loss is 1.1706933975219727\n",
      "epoch: 25 step: 145, loss is 1.1149682998657227\n",
      "epoch: 25 step: 146, loss is 1.07485032081604\n",
      "epoch: 25 step: 147, loss is 1.0824381113052368\n",
      "epoch: 25 step: 148, loss is 1.1609286069869995\n",
      "epoch: 25 step: 149, loss is 1.1531484127044678\n",
      "epoch: 25 step: 150, loss is 1.152282476425171\n",
      "epoch: 25 step: 151, loss is 1.1442633867263794\n",
      "epoch: 25 step: 152, loss is 1.2543123960494995\n",
      "epoch: 25 step: 153, loss is 1.141450047492981\n",
      "epoch: 25 step: 154, loss is 1.1404775381088257\n",
      "epoch: 25 step: 155, loss is 1.1897238492965698\n",
      "epoch: 25 step: 156, loss is 1.235953688621521\n",
      "epoch: 25 step: 157, loss is 1.1816707849502563\n",
      "epoch: 25 step: 158, loss is 1.1137943267822266\n",
      "epoch: 25 step: 159, loss is 1.150709867477417\n",
      "epoch: 25 step: 160, loss is 1.1804851293563843\n",
      "epoch: 25 step: 161, loss is 1.116445541381836\n",
      "epoch: 25 step: 162, loss is 1.2804203033447266\n",
      "epoch: 25 step: 163, loss is 1.183156967163086\n",
      "epoch: 25 step: 164, loss is 1.186344861984253\n",
      "epoch: 25 step: 165, loss is 1.1702319383621216\n",
      "epoch: 25 step: 166, loss is 1.0628976821899414\n",
      "epoch: 25 step: 167, loss is 1.0735716819763184\n",
      "epoch: 25 step: 168, loss is 1.2167659997940063\n",
      "epoch: 25 step: 169, loss is 1.1409114599227905\n",
      "epoch: 25 step: 170, loss is 1.0913150310516357\n",
      "epoch: 25 step: 171, loss is 1.1118957996368408\n",
      "epoch: 25 step: 172, loss is 1.2448734045028687\n",
      "epoch: 25 step: 173, loss is 1.1278260946273804\n",
      "epoch: 25 step: 174, loss is 1.2553913593292236\n",
      "epoch: 25 step: 175, loss is 1.0833337306976318\n",
      "epoch: 25 step: 176, loss is 1.076999545097351\n",
      "epoch: 25 step: 177, loss is 1.117651343345642\n",
      "epoch: 25 step: 178, loss is 1.130842924118042\n",
      "epoch: 25 step: 179, loss is 1.1840286254882812\n",
      "epoch: 25 step: 180, loss is 1.119195580482483\n",
      "epoch: 25 step: 181, loss is 1.2552158832550049\n",
      "epoch: 25 step: 182, loss is 1.3390796184539795\n",
      "epoch: 25 step: 183, loss is 1.1972923278808594\n",
      "epoch: 25 step: 184, loss is 1.148606538772583\n",
      "epoch: 25 step: 185, loss is 1.0369129180908203\n",
      "epoch: 25 step: 186, loss is 1.183483362197876\n",
      "epoch: 25 step: 187, loss is 1.329388976097107\n",
      "epoch: 25 step: 188, loss is 1.152287483215332\n",
      "epoch: 25 step: 189, loss is 1.2751543521881104\n",
      "epoch: 25 step: 190, loss is 1.1631015539169312\n",
      "epoch: 25 step: 191, loss is 1.0426875352859497\n",
      "epoch: 25 step: 192, loss is 1.2268667221069336\n",
      "epoch: 25 step: 193, loss is 1.2852082252502441\n",
      "epoch: 25 step: 194, loss is 1.1839985847473145\n",
      "epoch: 25 step: 195, loss is 1.1339762210845947\n",
      "epoch: 25 step: 196, loss is 1.2492432594299316\n",
      "epoch: 25 step: 197, loss is 1.1916006803512573\n",
      "epoch: 25 step: 198, loss is 1.016811490058899\n",
      "epoch: 25 step: 199, loss is 1.2639787197113037\n",
      "epoch: 25 step: 200, loss is 1.2518577575683594\n",
      "epoch: 25 step: 201, loss is 1.1394065618515015\n",
      "epoch: 25 step: 202, loss is 1.214804768562317\n",
      "epoch: 25 step: 203, loss is 1.3280407190322876\n",
      "epoch: 25 step: 204, loss is 1.0402027368545532\n",
      "epoch: 25 step: 205, loss is 1.201526165008545\n",
      "epoch: 25 step: 206, loss is 1.2497116327285767\n",
      "epoch: 25 step: 207, loss is 1.1006170511245728\n",
      "epoch: 25 step: 208, loss is 1.1550511121749878\n",
      "epoch: 25 step: 209, loss is 1.1690165996551514\n",
      "epoch: 25 step: 210, loss is 1.1618306636810303\n",
      "epoch: 25 step: 211, loss is 1.2867588996887207\n",
      "epoch: 25 step: 212, loss is 1.0870534181594849\n",
      "epoch: 25 step: 213, loss is 1.142740249633789\n",
      "epoch: 25 step: 214, loss is 1.1269546747207642\n",
      "epoch: 25 step: 215, loss is 1.1466357707977295\n",
      "epoch: 25 step: 216, loss is 1.202878713607788\n",
      "epoch: 25 step: 217, loss is 1.1168022155761719\n",
      "epoch: 25 step: 218, loss is 1.0927624702453613\n",
      "epoch: 25 step: 219, loss is 1.1148325204849243\n",
      "epoch: 25 step: 220, loss is 1.1062601804733276\n",
      "epoch: 25 step: 221, loss is 1.1585999727249146\n",
      "epoch: 25 step: 222, loss is 1.263300895690918\n",
      "epoch: 25 step: 223, loss is 1.2176859378814697\n",
      "epoch: 25 step: 224, loss is 1.1646320819854736\n",
      "epoch: 25 step: 225, loss is 1.226546287536621\n",
      "epoch: 25 step: 226, loss is 1.1881654262542725\n",
      "epoch: 25 step: 227, loss is 1.2472143173217773\n",
      "epoch: 25 step: 228, loss is 1.1235305070877075\n",
      "epoch: 25 step: 229, loss is 1.2211333513259888\n",
      "epoch: 25 step: 230, loss is 1.1489667892456055\n",
      "epoch: 25 step: 231, loss is 1.1828913688659668\n",
      "epoch: 25 step: 232, loss is 1.207751989364624\n",
      "epoch: 25 step: 233, loss is 1.054819107055664\n",
      "epoch: 25 step: 234, loss is 1.1858464479446411\n",
      "epoch: 25 step: 235, loss is 1.0740232467651367\n",
      "epoch: 25 step: 236, loss is 1.285731554031372\n",
      "epoch: 25 step: 237, loss is 1.258896827697754\n",
      "epoch: 25 step: 238, loss is 1.1649001836776733\n",
      "epoch: 25 step: 239, loss is 1.2594646215438843\n",
      "epoch: 25 step: 240, loss is 1.0879091024398804\n",
      "epoch: 25 step: 241, loss is 1.1150643825531006\n",
      "epoch: 25 step: 242, loss is 1.1535866260528564\n",
      "epoch: 25 step: 243, loss is 1.1255078315734863\n",
      "epoch: 25 step: 244, loss is 1.2612965106964111\n",
      "epoch: 25 step: 245, loss is 1.1710853576660156\n",
      "epoch: 25 step: 246, loss is 1.160266399383545\n",
      "epoch: 25 step: 247, loss is 1.2247360944747925\n",
      "epoch: 25 step: 248, loss is 1.2449126243591309\n",
      "epoch: 25 step: 249, loss is 1.265373945236206\n",
      "epoch: 25 step: 250, loss is 1.0724437236785889\n",
      "epoch: 25 step: 251, loss is 1.3193942308425903\n",
      "epoch: 25 step: 252, loss is 1.1275897026062012\n",
      "epoch: 25 step: 253, loss is 1.1411789655685425\n",
      "epoch: 25 step: 254, loss is 1.2302342653274536\n",
      "epoch: 25 step: 255, loss is 1.1771880388259888\n",
      "epoch: 25 step: 256, loss is 1.2150665521621704\n",
      "epoch: 25 step: 257, loss is 1.1641054153442383\n",
      "epoch: 25 step: 258, loss is 1.091524600982666\n",
      "epoch: 25 step: 259, loss is 1.1258275508880615\n",
      "epoch: 25 step: 260, loss is 1.1480062007904053\n",
      "epoch: 25 step: 261, loss is 1.1026396751403809\n",
      "epoch: 25 step: 262, loss is 1.1205627918243408\n",
      "epoch: 25 step: 263, loss is 1.1281460523605347\n",
      "epoch: 25 step: 264, loss is 1.2301149368286133\n",
      "epoch: 25 step: 265, loss is 1.186091423034668\n",
      "epoch: 25 step: 266, loss is 1.21212899684906\n",
      "epoch: 25 step: 267, loss is 1.1404105424880981\n",
      "epoch: 25 step: 268, loss is 1.08747398853302\n",
      "epoch: 25 step: 269, loss is 1.102626085281372\n",
      "epoch: 25 step: 270, loss is 1.105927586555481\n",
      "epoch: 25 step: 271, loss is 1.1825459003448486\n",
      "epoch: 25 step: 272, loss is 1.2537567615509033\n",
      "epoch: 25 step: 273, loss is 1.1774389743804932\n",
      "epoch: 25 step: 274, loss is 1.1853649616241455\n",
      "epoch: 25 step: 275, loss is 1.1713509559631348\n",
      "epoch: 25 step: 276, loss is 1.3207824230194092\n",
      "epoch: 25 step: 277, loss is 1.276906967163086\n",
      "epoch: 25 step: 278, loss is 1.096489667892456\n",
      "epoch: 25 step: 279, loss is 1.1927261352539062\n",
      "epoch: 25 step: 280, loss is 1.1663908958435059\n",
      "epoch: 25 step: 281, loss is 1.0848757028579712\n",
      "epoch: 25 step: 282, loss is 1.026917815208435\n",
      "epoch: 25 step: 283, loss is 1.124131441116333\n",
      "epoch: 25 step: 284, loss is 1.1972732543945312\n",
      "epoch: 25 step: 285, loss is 1.1517966985702515\n",
      "epoch: 25 step: 286, loss is 1.2592096328735352\n",
      "epoch: 25 step: 287, loss is 1.0377261638641357\n",
      "epoch: 25 step: 288, loss is 1.1108072996139526\n",
      "epoch: 25 step: 289, loss is 1.1046278476715088\n",
      "epoch: 25 step: 290, loss is 1.234944462776184\n",
      "epoch: 25 step: 291, loss is 1.059530258178711\n",
      "epoch: 25 step: 292, loss is 1.2147563695907593\n",
      "epoch: 25 step: 293, loss is 1.160872220993042\n",
      "epoch: 25 step: 294, loss is 1.1239962577819824\n",
      "epoch: 25 step: 295, loss is 1.145331621170044\n",
      "epoch: 25 step: 296, loss is 1.1054069995880127\n",
      "epoch: 25 step: 297, loss is 1.1437098979949951\n",
      "epoch: 25 step: 298, loss is 1.1848093271255493\n",
      "epoch: 25 step: 299, loss is 1.3461451530456543\n",
      "epoch: 25 step: 300, loss is 1.0999391078948975\n",
      "epoch: 25 step: 301, loss is 1.2276618480682373\n",
      "epoch: 25 step: 302, loss is 1.2737553119659424\n",
      "epoch: 25 step: 303, loss is 1.1754307746887207\n",
      "epoch: 25 step: 304, loss is 1.1313526630401611\n",
      "epoch: 25 step: 305, loss is 1.2011399269104004\n",
      "epoch: 25 step: 306, loss is 1.206420660018921\n",
      "epoch: 25 step: 307, loss is 1.154988408088684\n",
      "epoch: 25 step: 308, loss is 1.2060859203338623\n",
      "epoch: 25 step: 309, loss is 1.1997408866882324\n",
      "epoch: 25 step: 310, loss is 1.2072100639343262\n",
      "epoch: 25 step: 311, loss is 1.22575044631958\n",
      "epoch: 25 step: 312, loss is 1.1801403760910034\n",
      "epoch: 25 step: 313, loss is 1.3276631832122803\n",
      "epoch: 25 step: 314, loss is 1.2906866073608398\n",
      "epoch: 25 step: 315, loss is 1.142507791519165\n",
      "epoch: 25 step: 316, loss is 1.2117385864257812\n",
      "epoch: 25 step: 317, loss is 1.2091771364212036\n",
      "epoch: 25 step: 318, loss is 1.2527453899383545\n",
      "epoch: 25 step: 319, loss is 1.1482315063476562\n",
      "epoch: 25 step: 320, loss is 1.1954712867736816\n",
      "epoch: 25 step: 321, loss is 1.1535035371780396\n",
      "epoch: 25 step: 322, loss is 1.1544806957244873\n",
      "epoch: 25 step: 323, loss is 1.1382536888122559\n",
      "epoch: 25 step: 324, loss is 1.1284152269363403\n",
      "epoch: 25 step: 325, loss is 1.2094781398773193\n",
      "epoch: 25 step: 326, loss is 1.2101938724517822\n",
      "epoch: 25 step: 327, loss is 1.215648889541626\n",
      "epoch: 25 step: 328, loss is 1.2053197622299194\n",
      "epoch: 25 step: 329, loss is 1.1613924503326416\n",
      "epoch: 25 step: 330, loss is 1.196122646331787\n",
      "epoch: 25 step: 331, loss is 1.2276058197021484\n",
      "epoch: 25 step: 332, loss is 1.2294275760650635\n",
      "epoch: 25 step: 333, loss is 1.1824917793273926\n",
      "epoch: 25 step: 334, loss is 1.1917672157287598\n",
      "epoch: 25 step: 335, loss is 1.25473153591156\n",
      "epoch: 25 step: 336, loss is 1.2318496704101562\n",
      "epoch: 25 step: 337, loss is 1.1788688898086548\n",
      "epoch: 25 step: 338, loss is 1.1896088123321533\n",
      "epoch: 25 step: 339, loss is 1.072365164756775\n",
      "epoch: 25 step: 340, loss is 1.1150623559951782\n",
      "epoch: 25 step: 341, loss is 1.2204099893569946\n",
      "epoch: 25 step: 342, loss is 1.0996737480163574\n",
      "epoch: 25 step: 343, loss is 1.1927213668823242\n",
      "epoch: 25 step: 344, loss is 1.1639494895935059\n",
      "epoch: 25 step: 345, loss is 1.2074707746505737\n",
      "epoch: 25 step: 346, loss is 1.1559257507324219\n",
      "epoch: 25 step: 347, loss is 1.196368932723999\n",
      "epoch: 25 step: 348, loss is 1.0098321437835693\n",
      "epoch: 25 step: 349, loss is 1.1960277557373047\n",
      "epoch: 25 step: 350, loss is 1.1966880559921265\n",
      "epoch: 25 step: 351, loss is 1.135958194732666\n",
      "epoch: 25 step: 352, loss is 1.168227195739746\n",
      "epoch: 25 step: 353, loss is 1.2314879894256592\n",
      "epoch: 25 step: 354, loss is 1.125396966934204\n",
      "epoch: 25 step: 355, loss is 1.1068646907806396\n",
      "epoch: 25 step: 356, loss is 1.1604089736938477\n",
      "epoch: 25 step: 357, loss is 1.1453875303268433\n",
      "epoch: 25 step: 358, loss is 1.2199777364730835\n",
      "epoch: 25 step: 359, loss is 1.1289317607879639\n",
      "epoch: 25 step: 360, loss is 1.1490230560302734\n",
      "epoch: 25 step: 361, loss is 1.2942171096801758\n",
      "epoch: 25 step: 362, loss is 1.0463370084762573\n",
      "epoch: 25 step: 363, loss is 1.179635763168335\n",
      "epoch: 25 step: 364, loss is 1.13595449924469\n",
      "epoch: 25 step: 365, loss is 1.129272222518921\n",
      "epoch: 25 step: 366, loss is 1.1700079441070557\n",
      "epoch: 25 step: 367, loss is 1.263275384902954\n",
      "epoch: 25 step: 368, loss is 1.10732901096344\n",
      "epoch: 25 step: 369, loss is 1.0833449363708496\n",
      "epoch: 25 step: 370, loss is 0.9874228239059448\n",
      "epoch: 25 step: 371, loss is 1.2623034715652466\n",
      "epoch: 25 step: 372, loss is 1.1470131874084473\n",
      "epoch: 25 step: 373, loss is 1.1499677896499634\n",
      "epoch: 25 step: 374, loss is 1.2042837142944336\n",
      "epoch: 25 step: 375, loss is 1.1082357168197632\n",
      "epoch: 25 step: 376, loss is 1.2940857410430908\n",
      "epoch: 25 step: 377, loss is 1.1403917074203491\n",
      "epoch: 25 step: 378, loss is 1.2197024822235107\n",
      "epoch: 25 step: 379, loss is 1.2245198488235474\n",
      "epoch: 25 step: 380, loss is 1.227939486503601\n",
      "epoch: 25 step: 381, loss is 1.1779203414916992\n",
      "epoch: 25 step: 382, loss is 1.0650999546051025\n",
      "epoch: 25 step: 383, loss is 1.1832213401794434\n",
      "epoch: 25 step: 384, loss is 1.1645221710205078\n",
      "epoch: 25 step: 385, loss is 1.2115195989608765\n",
      "epoch: 25 step: 386, loss is 1.290004014968872\n",
      "epoch: 25 step: 387, loss is 1.1166354417800903\n",
      "epoch: 25 step: 388, loss is 1.2204997539520264\n",
      "epoch: 25 step: 389, loss is 1.1165874004364014\n",
      "epoch: 25 step: 390, loss is 1.3171132802963257\n",
      "Train epoch time: 162464.272 ms, per step time: 416.575 ms\n",
      "total time:1h 10m 11s\n",
      "============== Train Success ==============\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import mindspore\n",
    "import numpy as np\n",
    "from mindspore import Tensor, nn\n",
    "from mindspore.train import ModelCheckpoint, CheckpointConfig, TimeMonitor, LossMonitor, Model, Top1CategoricalAccuracy, Top5CategoricalAccuracy\n",
    "\n",
    "def train():\n",
    "    mindspore.set_context(mode=mindspore.PYNATIVE_MODE, device_target=\"Ascend\")\n",
    "    net = ShuffleNetV1(model_size=\"2.0x\", n_class=10)\n",
    "    loss = nn.CrossEntropyLoss(weight=None, reduction='mean', label_smoothing=0.1)\n",
    "    min_lr = 0.0005\n",
    "    base_lr = 0.05\n",
    "    lr_scheduler = mindspore.nn.cosine_decay_lr(min_lr,\n",
    "                                                base_lr,\n",
    "                                                batches_per_epoch*250,\n",
    "                                                batches_per_epoch,\n",
    "                                                decay_epoch=250)\n",
    "    lr = Tensor(lr_scheduler[-1])\n",
    "    optimizer = nn.Momentum(params=net.trainable_params(), learning_rate=lr, momentum=0.9, weight_decay=0.00004, loss_scale=1024)\n",
    "    loss_scale_manager = ms.amp.FixedLossScaleManager(1024, drop_overflow_update=False)\n",
    "    model = Model(net, loss_fn=loss, optimizer=optimizer, amp_level=\"O3\", loss_scale_manager=loss_scale_manager)\n",
    "    callback = [TimeMonitor(), LossMonitor()]\n",
    "    save_ckpt_path = \"./\"\n",
    "    config_ckpt = CheckpointConfig(save_checkpoint_steps=batches_per_epoch, keep_checkpoint_max=5)\n",
    "    ckpt_callback = ModelCheckpoint(\"shufflenetv1\", directory=save_ckpt_path, config=config_ckpt)\n",
    "    callback += [ckpt_callback]\n",
    "\n",
    "    print(\"============== Starting Training ==============\")\n",
    "    start_time = time.time()\n",
    "    model.train(25, dataset, callbacks=callback)\n",
    "    use_time = time.time() - start_time\n",
    "    hour = str(int(use_time // 60 // 60))\n",
    "    minute = str(int(use_time // 60 % 60))\n",
    "    second = str(int(use_time % 60))\n",
    "    print(\"total time:\" + hour + \"h \" + minute + \"m \" + second + \"s\")\n",
    "    print(\"============== Train Success ==============\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练好的模型保存在当前目录的`shufflenetv1-250_391.ckpt`中，用作评估。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型评估\n",
    "\n",
    "在CIFAR-10的测试集上对模型进行评估。\n",
    "\n",
    "设置好评估模型的路径后加载数据集，并设置Top 1, Top 5的评估标准，最后用`model.eval()`接口对模型进行评估。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(11825:281473636511760,MainProcess):2024-11-22-22:20:48.644.918 [mindspore/run_check/_check_version.py:396] Can not find the tbe operator implementation(need by mindspore-ascend). Please check whether the Environment Variable PYTHONPATH is set. For details, refer to the installation guidelines: https://www.mindspore.cn/install\n",
      "[WARNING] ME(11825:281473636511760,MainProcess):2024-11-22-22:20:48.647.655 [mindspore/run_check/_check_version.py:396] Can not find the tbe operator implementation(need by mindspore-ascend). Please check whether the Environment Variable PYTHONPATH is set. For details, refer to the installation guidelines: https://www.mindspore.cn/install\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size is  2.0x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.173.667 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.173.718 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.173.827 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.173.839 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.174.080 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.174.092 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.174.357 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.174.369 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.174.575 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.174.587 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.174.738 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.174.749 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.174.945 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.174.956 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.175.075 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.175.086 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.175.215 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.175.226 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.175.356 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.175.367 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.175.541 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.175.552 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.175.693 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.175.704 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.175.881 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.175.892 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.176.152 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.176.163 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.176.222 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.176.232 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.176.604 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.176.616 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.176.675 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.176.685 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.176.832 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.176.843 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.177.097 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.177.109 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.177.167 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.177.177 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.177.388 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.177.399 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.177.504 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.177.514 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.177.716 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.177.726 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.177.848 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.177.859 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.178.133 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.178.145 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.178.253 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.178.263 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.178.402 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.178.413 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.178.735 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.178.746 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.178.920 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.178.931 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.179.023 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.179.034 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.179.169 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.179.180 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.179.356 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.179.366 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.262.275 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.262.298 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.262.433 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.262.445 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.262.500 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.262.510 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.262.546 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.262.556 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.262.652 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.262.662 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.262.698 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.262.708 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.262.776 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.262.787 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.262.901 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.262.912 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.262.946 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.262.956 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.263.000 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.263.011 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.263.117 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.263.127 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.263.177 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.263.187 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.263.221 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.263.231 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.263.328 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.263.338 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.263.388 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.263.399 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.263.431 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.263.441 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.263.496 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.263.507 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.263.598 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.263.609 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.263.660 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.263.670 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.263.705 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.263.715 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.263.770 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.263.780 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.263.831 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.263.842 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.263.876 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.263.886 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.263.971 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.263.981 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.264.032 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.264.042 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.264.078 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.264.088 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.264.159 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.264.170 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.264.218 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.264.228 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.264.263 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.264.273 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.264.354 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.264.365 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.264.415 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.264.425 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.264.460 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.264.470 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.264.550 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.264.560 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.264.595 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.264.605 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.264.679 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.264.690 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.264.737 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.264.748 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.264.796 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.264.806 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.264.840 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.264.850 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.264.973 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.264.983 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.265.035 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.265.047 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.265.106 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.265.116 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.265.150 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.265.160 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.265.216 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.265.225 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.265.271 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.265.281 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.265.375 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.265.384 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.265.435 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.265.445 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.265.480 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.265.490 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.265.564 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.265.575 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.265.609 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.265.619 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.265.686 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.265.697 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.265.733 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.265.743 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.265.793 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.265.803 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.265.838 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.265.848 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.265.957 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.265.967 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.266.015 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.266.025 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.266.059 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.266.069 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.266.108 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.266.118 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.266.168 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.266.178 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.266.212 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.266.222 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.266.326 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.266.337 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.266.385 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.266.395 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.266.427 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.266.437 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.266.491 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.266.501 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.266.549 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.266.559 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.266.592 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.266.601 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.266.708 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.266.718 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.266.753 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.266.763 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.266.816 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.266.826 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.266.876 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.266.886 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.266.918 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.266.927 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.266.971 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.266.980 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.267.078 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.267.089 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.267.138 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.267.148 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.267.181 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.267.190 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.267.247 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.267.257 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.267.291 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.267.300 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.267.386 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.267.396 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.267.429 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.267.438 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.267.503 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.267.513 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.267.603 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.267.614 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.267.648 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.267.658 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.267.696 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.267.706 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.267.769 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.267.779 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.267.830 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.267.840 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.267.872 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.267.881 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.267.943 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.267.953 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.268.001 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.268.011 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.268.044 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.268.054 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.268.118 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.268.128 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.268.162 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.268.172 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.268.201 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:20:57.268.211 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result:{'Loss': 805.6793789364016, 'Top_1_Acc': 0.09304887820512821, 'Top_5_Acc': 0.5071113782051282}, ckpt:'./shufflenetv1-25_390.ckpt', time: 0h 1m 0s\n"
     ]
    }
   ],
   "source": [
    "from mindspore import load_checkpoint, load_param_into_net\n",
    "\n",
    "def test():\n",
    "    mindspore.set_context(mode=mindspore.GRAPH_MODE, device_target=\"Ascend\")\n",
    "    dataset = get_dataset(\"./dataset/cifar-10-batches-bin\", 128, \"test\")\n",
    "    net = ShuffleNetV1(model_size=\"2.0x\", n_class=10)\n",
    "    param_dict = load_checkpoint(\"shufflenetv1-25_390.ckpt\")\n",
    "    load_param_into_net(net, param_dict)\n",
    "    net.set_train(False)\n",
    "    loss = nn.CrossEntropyLoss(weight=None, reduction='mean', label_smoothing=0.1)\n",
    "    eval_metrics = {'Loss': nn.Loss(), 'Top_1_Acc': Top1CategoricalAccuracy(),\n",
    "                    'Top_5_Acc': Top5CategoricalAccuracy()}\n",
    "    model = Model(net, loss_fn=loss, metrics=eval_metrics)\n",
    "    start_time = time.time()\n",
    "    res = model.eval(dataset, dataset_sink_mode=False)\n",
    "    use_time = time.time() - start_time\n",
    "    hour = str(int(use_time // 60 // 60))\n",
    "    minute = str(int(use_time // 60 % 60))\n",
    "    second = str(int(use_time % 60))\n",
    "    log = \"result:\" + str(res) + \", ckpt:'\" + \"./shufflenetv1-25_390.ckpt\" \\\n",
    "        + \"', time: \" + hour + \"h \" + minute + \"m \" + second + \"s\"\n",
    "    print(log)\n",
    "    filename = './eval_log.txt'\n",
    "    with open(filename, 'a') as file_object:\n",
    "        file_object.write(log + '\\n')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型预测\n",
    "\n",
    "在CIFAR-10的测试集上对模型进行预测，并将预测结果可视化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size is  2.0x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.833.802 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/1681751341.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.833.854 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/1681751341.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.836.294 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.836.310 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.836.393 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.836.405 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.836.578 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/778396864.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.836.590 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/778396864.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.836.670 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.836.681 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.836.953 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.836.965 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.837.197 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.837.209 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.837.362 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.837.373 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.837.573 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.837.585 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.837.709 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.837.720 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.837.853 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.837.863 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.837.992 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.838.003 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.838.189 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.838.200 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.838.348 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.838.359 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.838.538 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.838.549 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.838.747 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/778396864.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.838.758 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/778396864.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.838.844 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.838.854 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.838.912 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.838.922 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.839.306 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.839.318 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.839.379 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.839.390 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.839.539 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.839.550 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.839.795 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.839.806 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.839.864 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.839.874 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.840.092 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.840.103 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.840.210 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.840.220 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.840.312 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/778396864.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.840.323 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/778396864.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.840.457 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.840.467 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.840.589 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.840.600 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.840.885 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.840.896 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.841.011 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.841.038 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.841.184 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.841.195 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.841.532 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.841.543 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.841.720 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.841.731 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.841.824 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.841.834 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.841.976 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.841.986 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "[ERROR] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.842.167 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11825/3162391481.py]\n",
      "[WARNING] CORE(11825,ffffb01e3010,python):2024-11-22-22:22:13.842.177 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_11825/3162391481.py' may not exists.\n",
      "/usr/local/Ascend/ascend-toolkit/8.0.RC3.alpha001/opp/built-in/op_impl/ai_core/tbe/impl/util/util_conv2d_dynamic.py:130: UserWarning: conv2d fmap ori_range changed from [[16, 31], [48, 48], [16, 63], [16, 63]] to [[16, 31], [48, 48], [16, 63], (16, 63)].\n",
      "  warnings.warn(to_print)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Ascend/ascend-toolkit/8.0.RC3.alpha001/opp/built-in/op_impl/ai_core/tbe/impl/util/util_conv2d_dynamic.py:130: UserWarning: conv2d fmap ori_range changed from [[16, 31], [480, 480], [16, 63], [16, 63]] to [[16, 31], [480, 480], [16, 63], (16, 63)].\n",
      "  warnings.warn(to_print)\n",
      "/usr/local/Ascend/ascend-toolkit/8.0.RC3.alpha001/opp/built-in/op_impl/ai_core/tbe/impl/util/util_conv2d_dynamic.py:130: UserWarning: conv2d fmap ori_range changed from [[16, 31], [960, 960], [4, 15], [4, 15]] to [[16, 31], [960, 960], [4, 15], (4, 15)].\n",
      "  warnings.warn(to_print)\n",
      "/usr/local/Ascend/ascend-toolkit/8.0.RC3.alpha001/opp/built-in/op_impl/ai_core/tbe/impl/util/util_conv2d_dynamic.py:130: UserWarning: conv2d fmap ori_range changed from [[16, 31], [1920, 1920], [7, 15], [7, 15]] to [[16, 31], [1920, 1920], [7, 15], (7, 15)].\n",
      "  warnings.warn(to_print)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x500 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import mindspore\n",
    "import matplotlib.pyplot as plt\n",
    "import mindspore.dataset as ds\n",
    "\n",
    "net = ShuffleNetV1(model_size=\"2.0x\", n_class=10)\n",
    "show_lst = []\n",
    "param_dict = load_checkpoint(\"shufflenetv1-25_390.ckpt\")\n",
    "load_param_into_net(net, param_dict)\n",
    "model = Model(net)\n",
    "dataset_predict = ds.Cifar10Dataset(dataset_dir=\"./dataset/cifar-10-batches-bin\", shuffle=False, usage=\"train\")\n",
    "dataset_show = ds.Cifar10Dataset(dataset_dir=\"./dataset/cifar-10-batches-bin\", shuffle=False, usage=\"train\")\n",
    "dataset_show = dataset_show.batch(16)\n",
    "show_images_lst = next(dataset_show.create_dict_iterator())[\"image\"].asnumpy()\n",
    "image_trans = [\n",
    "    vision.RandomCrop((32, 32), (4, 4, 4, 4)),\n",
    "    vision.RandomHorizontalFlip(prob=0.5),\n",
    "    vision.Resize((224, 224)),\n",
    "    vision.Rescale(1.0 / 255.0, 0.0),\n",
    "    vision.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010]),\n",
    "    vision.HWC2CHW()\n",
    "        ]\n",
    "dataset_predict = dataset_predict.map(image_trans, 'image')\n",
    "dataset_predict = dataset_predict.batch(16)\n",
    "class_dict = {0:\"airplane\", 1:\"automobile\", 2:\"bird\", 3:\"cat\", 4:\"deer\", 5:\"dog\", 6:\"frog\", 7:\"horse\", 8:\"ship\", 9:\"truck\"}\n",
    "# 推理效果展示(上方为预测的结果，下方为推理效果图片)\n",
    "plt.figure(figsize=(16, 5))\n",
    "predict_data = next(dataset_predict.create_dict_iterator())\n",
    "output = model.predict(ms.Tensor(predict_data['image']))\n",
    "pred = np.argmax(output.asnumpy(), axis=1)\n",
    "index = 0\n",
    "for image in show_images_lst:\n",
    "    plt.subplot(2, 8, index+1)\n",
    "    plt.title('{}'.format(class_dict[pred[index]]))\n",
    "    index += 1\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "5d834606fceea8447e1f2d2b8feecce2028f1eecd4b0eabaa8daf1aeed30752e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
